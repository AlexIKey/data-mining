<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Классификация, регрессия и другие алгоритмы Data Mining с использованием R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Реализация алгоритмов Data Mining с использованием R">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Классификация, регрессия и другие алгоритмы Data Mining с использованием R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://ranalytics.github.io/data-mining/" />
  
  <meta property="og:description" content="Реализация алгоритмов Data Mining с использованием R" />
  <meta name="github-repo" content="ranalytics/data-mining" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Классификация, регрессия и другие алгоритмы Data Mining с использованием R" />
  
  <meta name="twitter:description" content="Реализация алгоритмов Data Mining с использованием R" />
  

<meta name="author" content="Шитиков В. К., Мастицкий С. Э.">


<meta name="date" content="2017-04-06">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="103-Clustering-Quality.html">
<link rel="next" href="105-Cohonen-Maps.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Аннотация</a></li>
<li class="chapter" data-level="1" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html"><i class="fa fa-check"></i><b>1</b> Реализация моделей Data Mining в среде R (вместо предисловия)</a><ul>
<li class="chapter" data-level="1.1" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#section_1_1"><i class="fa fa-check"></i><b>1.1</b> Data Mining как направление анализа данных</a><ul>
<li class="chapter" data-level="1.1.1" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_1"><i class="fa fa-check"></i><b>1.1.1</b> От статистического анализа разового эксперимента к Data Mining</a></li>
<li class="chapter" data-level="1.1.2" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_2"><i class="fa fa-check"></i><b>1.1.2</b> Принципиальная множественность моделей окружающего мира</a></li>
<li class="chapter" data-level="1.1.3" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_3"><i class="fa fa-check"></i><b>1.1.3</b> Нарастающая множественность алгоритмов построения моделей</a></li>
<li class="chapter" data-level="1.1.4" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_4"><i class="fa fa-check"></i><b>1.1.4</b> Типы и характеристики групп моделей Data Mining</a></li>
<li class="chapter" data-level="1.1.5" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_5"><i class="fa fa-check"></i><b>1.1.5</b> Природа многомерного отклика и его моделирование</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="012-R-Intro.html"><a href="012-R-Intro.html"><i class="fa fa-check"></i><b>1.2</b> Статистическая среда R и ее использование в Data Mining</a></li>
<li class="chapter" data-level="1.3" data-path="013-What-This-Book-Is-About.html"><a href="013-What-This-Book-Is-About.html"><i class="fa fa-check"></i><b>1.3</b> О чем эта книга и чего в ней нет</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="021-Model-Quality-Criteria.html"><a href="021-Model-Quality-Criteria.html"><i class="fa fa-check"></i><b>2</b> Статистические модели: критерии и методы оценивания их качества</a><ul>
<li class="chapter" data-level="2.1" data-path="021-Model-Quality-Criteria.html"><a href="021-Model-Quality-Criteria.html#sec_2_1"><i class="fa fa-check"></i><b>2.1</b> Основные шаги построения и верификации моделей</a></li>
<li class="chapter" data-level="2.2" data-path="022-Resampling-Techniques.html"><a href="022-Resampling-Techniques.html"><i class="fa fa-check"></i><b>2.2</b> Использование алгоритмов ресэмплинга для тестирования моделей и оптимизации их параметров</a></li>
<li class="chapter" data-level="2.3" data-path="023-Models-for-Class-Prediction.html"><a href="023-Models-for-Class-Prediction.html"><i class="fa fa-check"></i><b>2.3</b> Модели для предсказания класса объектов</a></li>
<li class="chapter" data-level="2.4" data-path="024-Projecting-Data-onto-a-Plane.html"><a href="024-Projecting-Data-onto-a-Plane.html"><i class="fa fa-check"></i><b>2.4</b> Проецирование многомерных данных на плоскости</a></li>
<li class="chapter" data-level="2.5" data-path="025-MV-analysis.html"><a href="025-MV-analysis.html"><i class="fa fa-check"></i><b>2.5</b> Многомерный статистический анализ данных</a></li>
<li class="chapter" data-level="2.6" data-path="026-Clustering-Methods.html"><a href="026-Clustering-Methods.html"><i class="fa fa-check"></i><b>2.6</b> Методы кластеризации</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="031-Intro-to-Caret.html"><a href="031-Intro-to-Caret.html"><i class="fa fa-check"></i><b>3</b> Пакет <code>caret</code> - инструмент построения статистических моделей в R</a><ul>
<li class="chapter" data-level="3.1" data-path="031-Intro-to-Caret.html"><a href="031-Intro-to-Caret.html#---------caret"><i class="fa fa-check"></i><b>3.1</b> Универсальный интерфейс доступа к функциям машинного обучения в пакете <code id="sec_3_1">caret</code></a></li>
<li class="chapter" data-level="3.2" data-path="032-Removing-Predictors.html"><a href="032-Removing-Predictors.html"><i class="fa fa-check"></i><b>3.2</b> Обнаружение и удаление “ненужных” предикторов</a></li>
<li class="chapter" data-level="3.3" data-path="033-Preprocessing.html"><a href="033-Preprocessing.html"><i class="fa fa-check"></i><b>3.3</b> Предварительная обработка: преобразование и групповая трансформация переменных</a></li>
<li class="chapter" data-level="3.4" data-path="034-Handling-Missing-Values.html"><a href="034-Handling-Missing-Values.html"><i class="fa fa-check"></i><b>3.4</b> Заполнение пропущенных значений в данных</a></li>
<li class="chapter" data-level="3.5" data-path="035-The-train-Functions.html"><a href="035-The-train-Functions.html"><i class="fa fa-check"></i><b>3.5</b> Функция <code>train()</code> из пакета <code id="sec_3_5">caret</code></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html"><i class="fa fa-check"></i><b>4</b> Построение регрессионных моделей различного типа</a><ul>
<li class="chapter" data-level="4.1" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1"><i class="fa fa-check"></i><b>4.1</b> Селекция оптимального набора предикторов линейной модели</a><ul>
<li class="chapter" data-level="4.1.1" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_1"><i class="fa fa-check"></i><b>4.1.1</b> Полная регрессионная модель и пошаговая процедура</a></li>
<li class="chapter" data-level="4.1.2" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_2"><i class="fa fa-check"></i><b>4.1.2</b> Рекурсивное исключение переменных</a></li>
<li class="chapter" data-level="4.1.3" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_3"><i class="fa fa-check"></i><b>4.1.3</b> Генетический алгоритм</a></li>
<li class="chapter" data-level="4.1.4" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_4"><i class="fa fa-check"></i><b>4.1.4</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="042-Regularization.html"><a href="042-Regularization.html"><i class="fa fa-check"></i><b>4.2</b> Регуляризация, частные наименьшие квадраты и kNN-регрессия</a><ul>
<li class="chapter" data-level="4.2.1" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_1"><i class="fa fa-check"></i><b>4.2.1</b> Регрессия по методу “лассо”</a></li>
<li class="chapter" data-level="4.2.2" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_2"><i class="fa fa-check"></i><b>4.2.2</b> Метод частных наименьших квадратов (PLS)</a></li>
<li class="chapter" data-level="4.2.3" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_3"><i class="fa fa-check"></i><b>4.2.3</b> Регрессия по методу <em>k</em> ближайших соседей</a></li>
<li class="chapter" data-level="4.2.4" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_4"><i class="fa fa-check"></i><b>4.2.4</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html"><i class="fa fa-check"></i><b>4.3</b> Построение деревьев регрессии</a><ul>
<li class="chapter" data-level="4.3.1" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html#sec_4_3_1"><i class="fa fa-check"></i><b>4.3.1</b> Построение деревьев на основе рекурсивного разбиения</a></li>
<li class="chapter" data-level="4.3.2" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html#sec_4_3_2"><i class="fa fa-check"></i><b>4.3.2</b> Построение деревьев с использованием алгортма условного вывода</a></li>
<li class="chapter" data-level="4.3.3" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html#sec_4_3_3"><i class="fa fa-check"></i><b>4.3.3</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="044-Ensembles.html"><a href="044-Ensembles.html"><i class="fa fa-check"></i><b>4.4</b> Ансамбли моделей: бэггинг, случайные леса, бустинг</a><ul>
<li class="chapter" data-level="4.4.1" data-path="044-Ensembles.html"><a href="044-Ensembles.html#sec_4_4_1"><i class="fa fa-check"></i><b>4.4.1</b> Бэггинг и случайные леса</a></li>
<li class="chapter" data-level="4.4.2" data-path="044-Ensembles.html"><a href="044-Ensembles.html#sec_4_4_2"><i class="fa fa-check"></i><b>4.4.2</b> Бустинг</a></li>
<li class="chapter" data-level="4.4.3" data-path="044-Ensembles.html"><a href="044-Ensembles.html#sec_4_4_3"><i class="fa fa-check"></i><b>4.4.3</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="045-Comparing-Trees.html"><a href="045-Comparing-Trees.html"><i class="fa fa-check"></i><b>4.5</b> Сравнение построенных моделей и оценка информативности предикторов</a></li>
<li class="chapter" data-level="4.6" data-path="046-MV-Trees.html"><a href="046-MV-Trees.html"><i class="fa fa-check"></i><b>4.6</b> Деревья регрессии с многомерным откликом</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="051-Association-Rules.html"><a href="051-Association-Rules.html"><i class="fa fa-check"></i><b>5</b> Бинарные матрицы и ассоциативные правила</a><ul>
<li class="chapter" data-level="5.1" data-path="051-Association-Rules.html"><a href="051-Association-Rules.html#sec_5_1"><i class="fa fa-check"></i><b>5.1</b> Классификация в бинарных пространствах с использованием классических моделей</a></li>
<li class="chapter" data-level="5.2" data-path="052-Binary-Decision-Trees.html"><a href="052-Binary-Decision-Trees.html"><i class="fa fa-check"></i><b>5.2</b> Бинарные деревья решений</a></li>
<li class="chapter" data-level="5.3" data-path="053-Logic-Rules.html"><a href="053-Logic-Rules.html"><i class="fa fa-check"></i><b>5.3</b> Поиск логических закономерностей в данных</a></li>
<li class="chapter" data-level="5.4" data-path="054-Association-Rules-Algos.html"><a href="054-Association-Rules-Algos.html"><i class="fa fa-check"></i><b>5.4</b> Алгоритмы выделения ассоциативных правил</a></li>
<li class="chapter" data-level="5.5" data-path="055-Traminer.html"><a href="055-Traminer.html"><i class="fa fa-check"></i><b>5.5</b> Анализ последовательностей знаков или событий</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="061-Binary-Classifiers.html"><a href="061-Binary-Classifiers.html"><i class="fa fa-check"></i><b>6</b> Бинарные классификаторы с различными разделяющими поверхностями</a><ul>
<li class="chapter" data-level="6.1" data-path="061-Binary-Classifiers.html"><a href="061-Binary-Classifiers.html#sec_6_1"><i class="fa fa-check"></i><b>6.1</b> Дискриминантный анализ</a></li>
<li class="chapter" data-level="6.2" data-path="062-SVM.html"><a href="062-SVM.html"><i class="fa fa-check"></i><b>6.2</b> Дискриминантный анализ</a></li>
<li class="chapter" data-level="6.3" data-path="063-Nonlinear-Borders.html"><a href="063-Nonlinear-Borders.html"><i class="fa fa-check"></i><b>6.3</b> Ядерные функции машины опорных векторов</a></li>
<li class="chapter" data-level="6.4" data-path="064-Classification-Trees.html"><a href="064-Classification-Trees.html"><i class="fa fa-check"></i><b>6.4</b> Деревья классификации, случайный лес и логистическая регрессия</a></li>
<li class="chapter" data-level="6.5" data-path="065-Comparing-Classifiers.html"><a href="065-Comparing-Classifiers.html"><i class="fa fa-check"></i><b>6.5</b> Процедуры сравнения эффективности моделей классификации</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="071-Multiclass-Classification.html"><a href="071-Multiclass-Classification.html"><i class="fa fa-check"></i><b>7</b> Модели классификации для нескольких классов</a><ul>
<li class="chapter" data-level="7.1" data-path="071-Multiclass-Classification.html"><a href="071-Multiclass-Classification.html#sec_7_1"><i class="fa fa-check"></i><b>7.1</b> Ирисы Фишера и метод <em>k</em> ближайших соседей</a></li>
<li class="chapter" data-level="7.2" data-path="072-NBC.html"><a href="072-NBC.html"><i class="fa fa-check"></i><b>7.2</b> Наивный байесовский классификатор</a></li>
<li class="chapter" data-level="7.3" data-path="073-In-Discriminant-Space.html"><a href="073-In-Discriminant-Space.html"><i class="fa fa-check"></i><b>7.3</b> Классификация в линейном дискриминантном пространстве</a></li>
<li class="chapter" data-level="7.4" data-path="074-Nonlinear-Classifiers.html"><a href="074-Nonlinear-Classifiers.html"><i class="fa fa-check"></i><b>7.4</b> Нелинейные классификаторы в R</a></li>
<li class="chapter" data-level="7.5" data-path="075-Multinomial-Logit.html"><a href="075-Multinomial-Logit.html"><i class="fa fa-check"></i><b>7.5</b> Модель мультиномиального логита</a></li>
<li class="chapter" data-level="7.6" data-path="076-NN.html"><a href="076-NN.html"><i class="fa fa-check"></i><b>7.6</b> Классификаторы на основе искусственных нейронных сетей</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="081-Logit-for-Count.html"><a href="081-Logit-for-Count.html"><i class="fa fa-check"></i><b>8</b> Моделирование порядковых и счетных переменных</a><ul>
<li class="chapter" data-level="8.1" data-path="081-Logit-for-Count.html"><a href="081-Logit-for-Count.html#sec_8_1"><i class="fa fa-check"></i><b>8.1</b> Модель логита для порядковой переменной</a></li>
<li class="chapter" data-level="8.2" data-path="082-NN-with-Caret.html"><a href="082-NN-with-Caret.html"><i class="fa fa-check"></i><b>8.2</b> Настройка параметров нейронных сетей средствами пакета <code id="sec_8_2">caret</code></a></li>
<li class="chapter" data-level="8.3" data-path="083-Model-Complexes.html"><a href="083-Model-Complexes.html"><i class="fa fa-check"></i><b>8.3</b> Методы комплексации модельных прогнозов</a></li>
<li class="chapter" data-level="8.4" data-path="084-GLM-for-Counts.html"><a href="084-GLM-for-Counts.html"><i class="fa fa-check"></i><b>8.4</b> Обобщенные линейные модели для счетных данных</a></li>
<li class="chapter" data-level="8.5" data-path="085-ZIP-for-Counts.html"><a href="085-ZIP-for-Counts.html"><i class="fa fa-check"></i><b>8.5</b> ZIP- и барьерные модели счетных данных</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="091-Data-Transformation.html"><a href="091-Data-Transformation.html"><i class="fa fa-check"></i><b>9</b> Методы многомерной ординации</a><ul>
<li class="chapter" data-level="9.1" data-path="091-Data-Transformation.html"><a href="091-Data-Transformation.html#sec_9_1"><i class="fa fa-check"></i><b>9.1</b> Преобразование данных и вычисление матрицы расстояний</a></li>
<li class="chapter" data-level="9.2" data-path="092-Distance-ANOVA.html"><a href="092-Distance-ANOVA.html"><i class="fa fa-check"></i><b>9.2</b> Непараметрический дисперсионный анализ матриц дистанций</a></li>
<li class="chapter" data-level="9.3" data-path="093-Comparing-Diagrams.html"><a href="093-Comparing-Diagrams.html"><i class="fa fa-check"></i><b>9.3</b> Методы ординации объектов и переменных: построение и сравнение диаграмм</a></li>
<li class="chapter" data-level="9.4" data-path="094-Ordination-Factors.html"><a href="094-Ordination-Factors.html"><i class="fa fa-check"></i><b>9.4</b> Оценка связи ординации с внешними факторами</a></li>
<li class="chapter" data-level="9.5" data-path="095-NMDS.html"><a href="095-NMDS.html"><i class="fa fa-check"></i><b>9.5</b> Неметрическое многомерное шкалирование и построение распределения чувствительности видов</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="101-Partitioning-Algos.html"><a href="101-Partitioning-Algos.html"><i class="fa fa-check"></i><b>10</b> Кластерный анализ</a><ul>
<li class="chapter" data-level="10.1" data-path="101-Partitioning-Algos.html"><a href="101-Partitioning-Algos.html#sec_10_1"><i class="fa fa-check"></i><b>10.1</b> Алгоритмы кластеризации, основанные на разделении</a></li>
<li class="chapter" data-level="10.2" data-path="102-H-Clustering.html"><a href="102-H-Clustering.html"><i class="fa fa-check"></i><b>10.2</b> Иерархическая кластеризация</a></li>
<li class="chapter" data-level="10.3" data-path="103-Clustering-Quality.html"><a href="103-Clustering-Quality.html"><i class="fa fa-check"></i><b>10.3</b> Оценка качества кластеризации</a></li>
<li class="chapter" data-level="10.4" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html"><i class="fa fa-check"></i><b>10.4</b> Другие алгоритмы кластеризации</a><ul>
<li class="chapter" data-level="10.4.1" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html#sec_10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Иерархическая кластеризация на главные компоненты</a></li>
<li class="chapter" data-level="10.4.2" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html#sec_10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Метод нечетких <em>k</em> средних (fuzzy analysis clustering)</a></li>
<li class="chapter" data-level="10.4.3" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html#sec_10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Статистическая модель кластеризации</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="105-Cohonen-Maps.html"><a href="105-Cohonen-Maps.html"><i class="fa fa-check"></i><b>10.5</b> Самоорганизующиеся карты Кохонена</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="111-Rattle-Intro.html"><a href="111-Rattle-Intro.html"><i class="fa fa-check"></i><b>11</b> <code>rattle</code>: графический интерфейс R для реализации алгоритмов Data Mining</a><ul>
<li class="chapter" data-level="11.1" data-path="111-Rattle-Intro.html"><a href="111-Rattle-Intro.html#----rattle"><i class="fa fa-check"></i><b>11.1</b> Начало работы с пакетом <code id="sec_11_1">rattle</code></a></li>
<li class="chapter" data-level="11.2" data-path="112-Descriptive-Stats.html"><a href="112-Descriptive-Stats.html"><i class="fa fa-check"></i><b>11.2</b> Описательная статистика и визуализация данных</a></li>
<li class="chapter" data-level="11.3" data-path="113-Model-Building.html"><a href="113-Model-Building.html"><i class="fa fa-check"></i><b>11.3</b> Построение и тестирование моделей классификации</a></li>
<li class="chapter" data-level="11.4" data-path="114-Descriptive-Models.html"><a href="114-Descriptive-Models.html"><i class="fa fa-check"></i><b>11.4</b> Дескриптивные модели (обучение без учителя)</a><ul>
<li class="chapter" data-level="11.4.1" data-path="114-Descriptive-Models.html"><a href="114-Descriptive-Models.html#sec_11_4_1"><i class="fa fa-check"></i><b>11.4.1</b> Кластерный анализ</a></li>
<li class="chapter" data-level="11.4.2" data-path="114-Descriptive-Models.html"><a href="114-Descriptive-Models.html#sec_11_4_2"><i class="fa fa-check"></i><b>11.4.2</b> Ассоциативные правила</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="120-References.html"><a href="120-References.html"><i class="fa fa-check"></i><b>12</b> Список рекомендуемой литературы</a></li>
<li class="chapter" data-level="" data-path="130-Appendix.html"><a href="130-Appendix.html"><i class="fa fa-check"></i>Приложение: cправочная карта по Data Mining с использованием R</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Классификация, регрессия и другие алгоритмы Data Mining с использованием R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec_10_4" class="section level2">
<h2><span class="header-section-number">10.4</span> Другие алгоритмы кластеризации</h2>
<div id="sec_10_4_1" class="section level3">
<h3><span class="header-section-number">10.4.1</span> Иерархическая кластеризация на главные компоненты</h3>
<p>Использование главных компонент для иерархической кластеризации - один из возможных путей гибридизации алгоритмов, которая все чаще привлекает внимание специалистов. Предварительное сжатие пространства признаков - хороший метод сглаживания случайных флуктуаций в данных, что является предпосылкой построения более стабильных кластерных структур. Это особенно важно для матриц с большим количеством признаков, например, в генной инженерии.</p>
<p>В среде R кластеризация на главные компоненты реализована в пакете <code>FactoMineR</code> и состоит из двух шагов. На первом этапе функцией <code>РСА()</code> выполняется обычный анализ главных компонент и выбирается их число. Далее функция <code>HCPC()</code> использует эти результаты и строит иерархическую кластеризацию. При этом используется метод Уорда, который, как и анализ главных компонент, основан на анализе многомерной дисперсии (inertia).</p>
<p>Опять воспользуемся в качестве примера набором <code>Boston</code> из пакета <code>MASS</code>. Выполним снижение исходной размерности данных, определяемой 14 признаками привлекательности земельных участков Бостона, до 5 главных компонент (рис. <a href="104-Other-Clustering-Methods.html#fig:fig-10-13">10.13</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(FactoMineR)
<span class="kw">library</span>(factoextra)
<span class="kw">data</span>(Boston, <span class="dt">package =</span> <span class="st">&quot;MASS&quot;</span>)
df.scale &lt;-<span class="st"> </span><span class="kw">scale</span>(Boston)
res.pca &lt;-<span class="st"> </span><span class="kw">PCA</span>(df.scale, <span class="dt">ncp =</span> <span class="dv">5</span>, <span class="dt">graph =</span> <span class="ot">TRUE</span>)
<span class="kw">get_eig</span>(res.pca)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-10-13"></span>
<img src="figures/fig_10_13.png" alt="Разложение исходных признаков по осям двух главных компонент" width="640px" />
<p class="caption">
Рисунок 10.13: Разложение исходных признаков по осям двух главных компонент
</p>
</div>
<p>Напомним, что угол между двумя любыми осями (факторов и/или компонент) на рис. <a href="104-Other-Clustering-Methods.html#fig:fig-10-13">10.13</a> соответствует уровню корреляции между ними. Заинтересованный читатель может сравнить этот график с иерархической дендрограммой признаков на рис. <a href="103-Clustering-Quality.html#fig:fig-10-12">10.12</a>.</p>
<p>Выполним теперь иерархическую кластеризацию на главные компоненты. Построим две диаграммы - обычную дендрограмму, совмещенную с графиком “каменистой осыпи” дисперсии (<code>choice =&quot;tree&quot;</code>), и трехмерную дендрограмму, совмещенную с ординационной диаграммой (<code>choice = &quot;3D.map&quot;</code>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res.hcpc &lt;-<span class="st"> </span><span class="kw">HCPC</span>(res.pca, <span class="dt">graph =</span> <span class="ot">FALSE</span>)
<span class="kw">plot</span>(res.hcpc, <span class="dt">choice =</span> <span class="st">&quot;tree&quot;</span>)
<span class="kw">plot</span>(res.hcpc, <span class="dt">choice =</span> <span class="st">&quot;3D.map&quot;</span>, <span class="dt">ind.names =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="figures/fig_10_14a.png" width="640px" style="display: block; margin: auto;" /></p>
<div class="figure" style="text-align: center"><span id="fig:fig-10-14"></span>
<img src="figures/fig_10_14b.png" alt="Дендрограмма с разделением на кластеры (вверху) и гибрид дендрограммы с ординационной диаграммы (внизу)" width="640px" />
<p class="caption">
Рисунок 10.14: Дендрограмма с разделением на кластеры (вверху) и гибрид дендрограммы с ординационной диаграммы (внизу)
</p>
</div>
</div>
<div id="sec_10_4_2" class="section level3">
<h3><span class="header-section-number">10.4.2</span> Метод нечетких <em>k</em> средних (fuzzy analysis clustering)</h3>
<p>Традиционные принципы кластерного анализа предполагают, что выделяемые группы представляют собой детерминированные совокупности, т. е. каждый объект может принадлежать только к одному таксону. Ограниченность такого подхода часто приводит к аналитической неопределенности, и поэтому разумной альтернативой понятию абсолютной дискретности является интерпретация компонентов систем как нечетких объектов в составе гибко настраиваемых ординационных структур.</p>
<p>В конце 80-х годов, после того, как основные концепции нечетких множеств (fuzzy sets) были разработаны Лотфи Заде (Zadeh, 1965), началось бурное развитие технических устройств на базе нечетких контроллеров, а нечеткая логика (fuzzy logic) стала неотъемлемой составной частью современных систем искусственного интеллекта.</p>
<p>Множество <span class="math inline">\(\mathbf{C}\)</span> является нечетким, если существует функция принадлежности (membership function) <span class="math inline">\(\mu_C(x) = 0\)</span> означает полную несовместимость, т.е. <span class="math inline">\(x \notin \mathbf{C}\)</span>, а <span class="math inline">\(\mu_C(x) = 1\)</span> означает полную принадлежность, или <span class="math inline">\(x \in \mathbf{C}\)</span>. Применительно к кластеризации методом нечетких <span class="math inline">\(k\)</span> средних функция <span class="math inline">\(\mu_{ir}(\mathbf{x})\)</span> задает в масштабе от 0 до 1 степень принадлежности каждого объекта <span class="math inline">\(x_i\)</span> к каждому выделяемому кластеру <span class="math inline">\(r\)</span> (Bezdek, 1981).</p>
<p>Пусть <span class="math inline">\(D_{ir} = \sum_j (x_{ij} - \nu_{rj})^2\)</span> - расстояние между каждым <span class="math inline">\(i\)</span>-м объектом (<span class="math inline">\(i = 1, 2, \dots, n\)</span>), описанным набором признаков <span class="math inline">\(x_{ij}\)</span>, и центрами тяжести <span class="math inline">\(v_{rj}\)</span> каждого из <span class="math inline">\(k\)</span> кластера (<span class="math inline">\(r = 1, 2, \dots, k\)</span>). Тогда в общем случае кластеризацию объектов <span class="math inline">\(\mathbf{X}\)</span> можно сформулировать как задачу оптимизации, связанную с нахождением такой матрицы <span class="math inline">\(\mathbf{\mu}\)</span>, которая минимизировала бы критерий</p>
<p><span class="math display">\[F_{km}(\mathbf{\mu}) = \sum_{i=1}^n \sum_{r=1}^k \mu_{ir}^m D_{ir}.\]</span></p>
<p>Экспоненциальный вес (<span class="math inline">\(m\)</span>) в алгоритме нечетких <span class="math inline">\(k\)</span> средних задает уровень нечеткости получаемых кластеров: чем больше m, тем нечеткое разбиение более “размазано”. “Штатный” диапазон варьирования <span class="math inline">\(m\)</span> - от 1.2 до 2. Другим важным параметром является количество классов <span class="math inline">\(k\)</span>, которое принимается из описанных выше соображений.</p>
<p>Выполним построение “нечетких” кластеров для традиционного примера по криминогенной обстановке американских штатов с использованием функции <code>fanny()</code> из пакета <code>cluster</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(cluster) 
<span class="kw">data</span>(<span class="st">&quot;USArrests&quot;</span>)
<span class="kw">set.seed</span>(<span class="dv">123</span>)
res.fanny &lt;-<span class="st"> </span><span class="kw">fanny</span>(USArrests, <span class="dt">k =</span> <span class="dv">4</span>, <span class="dt">memb.exp =</span> <span class="fl">1.7</span>, 
                   <span class="dt">metric =</span> <span class="st">&quot;euclidean&quot;</span>, <span class="dt">stand =</span> <span class="ot">TRUE</span>, <span class="dt">maxit =</span> <span class="dv">500</span>)
<span class="kw">print</span>(<span class="kw">head</span>(res.fanny$membership),<span class="dv">3</span>)</code></pre></div>
<pre><code>##             [,1]  [,2]  [,3]   [,4]
## Alabama    0.639 0.183 0.113 0.0645
## Alaska     0.337 0.357 0.179 0.1280
## Arizona    0.213 0.595 0.131 0.0611
## Arkansas   0.371 0.170 0.264 0.1959
## California 0.224 0.529 0.160 0.0869
## Colorado   0.224 0.519 0.174 0.0830</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res.fanny$coeff</code></pre></div>
<pre><code>## dunn_coeff normalized 
##  0.3927126  0.1902834</code></pre>
<p>При <span class="math inline">\(k = 4\)</span> в результате выводится матрица коэффициентов принадлежности, максимальный из которых определяет назначаемый кластер. Так из приведенного фрагмента Алабама и Арканзас включены в кластер 1, а остальные четыре штата - в кластер 2.</p>
<p>Для оценки меры нечеткости полученной классификации используется коэффициент разделения Данна (Dunn):</p>
<p><span class="math display">\[F_k = \sum_{i=1}^n \sum_{r=1}^k \mu_{ir}^2 / k,\]</span></p>
<p>который принимает минимальное значение при полной нечеткости разбиения, когда расстояния от каждого объекта до центра тяжести любого кластера равновелики: <span class="math inline">\(\mu = 1/k\)</span>. Напротив, в случае четкой кластеризации (<span class="math inline">\(\mu = 1\)</span> или <span class="math inline">\(\mu = 0\)</span>) коэффициент Данна <span class="math inline">\(F_k\)</span> принимает значение 1. Для представленного примера <span class="math inline">\(F_k = 0.39\)</span>, а его нормированная версия, изменяющаяся от 0 до 1 и характеризующая степень нечеткости <span class="math inline">\(F_k&#39; = (kF_k - 1)/(k-1) = 0.19\)</span>.</p>
<p>Построим две диаграммы (рис. <a href="104-Other-Clustering-Methods.html#fig:fig-10-15">10.15</a>). На первой из них (слева) покажем матрицу <span class="math inline">\(\sum_{r=1}^k \mu_{ir}^2 / k\)</span>, отсортированную по убыванию (т.е. по степени нечеткости). На второй же (справа) – ординационную диаграмму в пространстве двух главных компонент с результатами кластеризации (аналогична рис. <a href="101-Partitioning-Algos.html#fig:fig-10-6">10.6</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Визуализация с использованием corrplot</span>
<span class="kw">library</span>(corrplot)
Dunn &lt;-<span class="st"> </span>res.fanny$membership^<span class="dv">2</span>
<span class="kw">corrplot</span>(Dunn[<span class="kw">rev</span>(<span class="kw">order</span>(<span class="kw">rowSums</span>(Dunn))), ], <span class="dt">is.corr =</span> <span class="ot">FALSE</span>)
<span class="co"># Ординационная диаграмма</span>
<span class="kw">library</span>(factoextra)
<span class="kw">fviz_cluster</span>(res.fanny, <span class="dt">frame.type =</span> <span class="st">&quot;norm&quot;</span>, <span class="dt">frame.level =</span> <span class="fl">0.7</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-10-15"></span>
<img src="figures/fig_10_15.png" alt="Матрица степени нечеткости (слева) и ординационная диаграмма (справа), полученные с применением метода нечеткой кластеризации" width="640px" />
<p class="caption">
Рисунок 10.15: Матрица степени нечеткости (слева) и ординационная диаграмма (справа), полученные с применением метода нечеткой кластеризации
</p>
</div>
</div>
<div id="sec_10_4_3" class="section level3">
<h3><span class="header-section-number">10.4.3</span> Статистическая модель кластеризации</h3>
<p>Важнейшими свойствами кластеров являются плотность, дисперсия, размер, форма и отделимость (Ким и др., 1989). Характер изменчивости облака экспериментальных точек относительно центроидов искомых кластеров может быть описан статистической моделью со смешанными параметрами (Banﬁeld, Raftery, 1993).</p>
<p>Предположим, что входное множество векторов наблюдений <span class="math inline">\(x_1, x_2, \dots, x_n\)</span> представляет собой случайные реализации из <span class="math inline">\(K\)</span> неизвестных распределений <span class="math inline">\(E_1, E_2, \dots, E_K\)</span>. Допустим, что плотность распределения <span class="math inline">\(x_i\)</span>, связанная с <span class="math inline">\(E_k\)</span>, задана функцией <span class="math inline">\(f_k(x_i, \theta)\)</span>, включающей некоторое неизвестное множество параметров <span class="math inline">\(\theta\)</span>. Если принять допущение, что классифицируемый объект принадлежит только одному распределению, то можно ввести переменную <span class="math inline">\(\gamma_I = k\)</span>, если xi принадлежит <span class="math inline">\(E_k, k = 1, 2, \dots, K\)</span>. Тогда цель оптимизации модели заключается в нахождении таких параметров <span class="math inline">\(\mathbf{\theta}\)</span> и <span class="math inline">\(\mathbf{\gamma}\)</span>, которые максимизируют совокупную вероятность:</p>
<p><span class="math display">\[L(\mathbf{\theta, \gamma}) = \prod_{i=1}^n f(y_i, x_i | \theta).\]</span></p>
<p>Поскольку вероятность <span class="math inline">\(L(\mathbf{\theta, \gamma})\)</span> основана на смеси <span class="math inline">\(K\)</span> распределений, то такая модель называется смешанной (mixture model). Если использовать в качестве распределений многомерные нормальные функции, то неизвестные параметры <span class="math inline">\(\mathbf{\theta}\)</span> определяются как вектор средних <span class="math inline">\(\mu_k\)</span> и матрица ковариации <span class="math inline">\(\Sigma_k\)</span> каждого <span class="math inline">\(k\)</span>-го распределения.</p>
<p>Если рассматривать изложенный формализм смешанных моделей применительно к кластеризации (model-based clustering), то каждой группе <span class="math inline">\(k\)</span> соответствует центроид <span class="math inline">\(\mu_k\)</span> с повышенной плотностью точек в его окрестностях. Геометрические особенности (форма, объем, ориентация) каждого кластера определены матрицей ковариации <span class="math inline">\(\Sigma_k\)</span>. Главным преимуществом этого подхода, по сравнению с другими методами кластеризации, является возможность автоматической оценки оптимального числа кластеров в процессе подгонки параметров модели.</p>
<p>Выборочные оценки параметров <span class="math inline">\(\theta\)</span> могут быть получены с использованием алгоритма <em>максимизации математических ожиданий</em> (EM, Expectation-Maximization), который оценивает максимумы вероятности <span class="math inline">\(L(\mathbf{\theta, \gamma})\)</span> серии моделей: для заданного диапазона значений <span class="math inline">\(k\)</span> и с различной параметризацией матрицы ковариации. Оптимальная модель обычно отбирается на основе максимума байесовского информационного критерия BIC.</p>
<p>Рассмотрим двумерный набор данных, который содержит время ожидания между извержениями (<code>waiting</code>) и продолжительностью извержения (<code>eruptions</code>) гейзера “Старый служака” в Йеллоустонском национальном парке. Для иллюстрации характера распределения наблюдаемых данных изобразим диаграмму рассеяния (рис. <a href="104-Other-Clustering-Methods.html#fig:fig-10-16">10.16</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(faithful)
<span class="kw">head</span>(faithful, <span class="dv">3</span>)</code></pre></div>
<pre><code>##   eruptions waiting
## 1     3.600      79
## 2     1.800      54
## 3     3.333      74</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">ggplot</span>(faithful, <span class="kw">aes</span>(<span class="dt">x =</span> eruptions, <span class="dt">y =</span> waiting)) +
<span class="st">    </span><span class="kw">geom_point</span>() +<span class="st"> </span><span class="kw">geom_density2d</span>()</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-10-16"></span>
<img src="104-Other-Clustering-Methods_files/figure-html/fig-10-16-1.png" alt="Диаграмма двумерной плотности распределения данных `faithful`" width="480" />
<p class="caption">
Рисунок 10.16: Диаграмма двумерной плотности распределения данных <code>faithful</code>
</p>
</div>
<p>Подогнать смешанную модель кластеризации по этим данным можно с использованием функции <code>Mclust()</code> из пакета <code>mclust</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mc &lt;-<span class="st"> </span>mclust::<span class="kw">Mclust</span>(faithful)
<span class="kw">summary</span>(mc)</code></pre></div>
<pre><code>## ----------------------------------------------------
## Gaussian finite mixture model fitted by EM algorithm 
## ----------------------------------------------------
## 
## Mclust EEE (ellipsoidal, equal volume, shape and orientation) model with 3 components:
## 
##  log.likelihood   n df       BIC       ICL
##       -1126.361 272 11 -2314.386 -2360.865
## 
## Clustering table:
##   1   2   3 
## 130  97  45</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(mc$z)</code></pre></div>
<pre><code>##           [,1]         [,2]         [,3]
## 1 2.181744e-02 1.130837e-08 9.781825e-01
## 2 2.475031e-21 1.000000e+00 3.320864e-13
## 3 2.521625e-03 2.051823e-05 9.974579e-01
## 4 6.553336e-14 9.999998e-01 1.664978e-07
## 5 9.838967e-01 7.642900e-20 1.610327e-02
## 6 2.104355e-07 9.975388e-01 2.461029e-03</code></pre>
<p>Мы получили разбиение исходных наблюдений на три кластера с оптимальным <code>BIC = -2314</code>. Каждому наблюдению назначается кластер с максимальной оцененной вероятностью <code>z</code>. Для визуализации полученных кластеров можно использовать различные варианты функции <code>plot()</code> или <code>fviz_cluster()</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
<span class="kw">plot</span>(mc, <span class="st">&quot;classification&quot;</span>)
<span class="kw">plot</span>(mc, <span class="st">&quot;uncertainty&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-10-17"></span>
<img src="104-Other-Clustering-Methods_files/figure-html/fig-10-17-1.png" alt="Распределение наблюдений по кластерам (на графике справа диаметр точек соответствует мере неопределенности - `uncertainty`)" width="768" />
<p class="caption">
Рисунок 10.17: Распределение наблюдений по кластерам (на графике справа диаметр точек соответствует мере неопределенности - <code>uncertainty</code>)
</p>
</div>
<p>Как следует из результатов <code>summary()</code>, ковариационные матрицы оптимизированы под вариант структурной организации кластеров <code>EEE</code> (эллипсоидальная с равным объемом, формой и ориентацией). Зависимость критерия BIC от числа кластеров для различных вариантов параметризации можно увидеть на следующем графике:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(mc, <span class="st">&quot;BIC&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-10-18"></span>
<img src="104-Other-Clustering-Methods_files/figure-html/fig-10-18-1.png" alt="Зависимость критерия BIC от числа кластеров для различных вариантов параметризации ковариационной матрицы" width="480" />
<p class="caption">
Рисунок 10.18: Зависимость критерия BIC от числа кластеров для различных вариантов параметризации ковариационной матрицы
</p>
</div>
<p>Каждая опция модели, представленная на рис. <a href="104-Other-Clustering-Methods.html#fig:fig-10-18">10.18</a>, описана идентификатором, первый символ которого относится к объему, второй - к форме, а третий - к ориентации. Символы могут принимать значения <code>&quot;E&quot;</code> - равный, <code>&quot;V&quot;</code> - переменный и <code>&quot;I&quot;</code> - расположение относительно осей координат. Так, <code>&quot;VEI&quot;</code> означает, что кластеры имеют разный объем, одинаковую форму и одинаково ориентированы по координатным осям. Подробно о составе и смысле опций можно узнать, выполнив команду <code>?mclustModelNames</code>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="103-Clustering-Quality.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="105-Cohonen-Maps.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["_main.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
