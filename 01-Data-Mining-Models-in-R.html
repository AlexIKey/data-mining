<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Классификация, регрессия и другие алгоритмы Data Mining с использованием R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Реализация алгоритмов Data Mining с использованием R">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Классификация, регрессия и другие алгоритмы Data Mining с использованием R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://ranalytics.github.io/data-mining/" />
  
  <meta property="og:description" content="Реализация алгоритмов Data Mining с использованием R" />
  <meta name="github-repo" content="ranalytics/data-mining" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Классификация, регрессия и другие алгоритмы Data Mining с использованием R" />
  
  <meta name="twitter:description" content="Реализация алгоритмов Data Mining с использованием R" />
  

<meta name="author" content="Шитиков В. К., Мастицкий С. Э.">


<meta name="date" content="2017-04-06">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="012-R-Intro.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Аннотация</a></li>
<li class="chapter" data-level="1" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html"><i class="fa fa-check"></i><b>1</b> Реализация моделей Data Mining в среде R (вместо предисловия)</a><ul>
<li class="chapter" data-level="1.1" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#section_1_1"><i class="fa fa-check"></i><b>1.1</b> Data Mining как направление анализа данных</a><ul>
<li class="chapter" data-level="1.1.1" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_1"><i class="fa fa-check"></i><b>1.1.1</b> От статистического анализа разового эксперимента к Data Mining</a></li>
<li class="chapter" data-level="1.1.2" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_2"><i class="fa fa-check"></i><b>1.1.2</b> Принципиальная множественность моделей окружающего мира</a></li>
<li class="chapter" data-level="1.1.3" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_3"><i class="fa fa-check"></i><b>1.1.3</b> Нарастающая множественность алгоритмов построения моделей</a></li>
<li class="chapter" data-level="1.1.4" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_4"><i class="fa fa-check"></i><b>1.1.4</b> Типы и характеристики групп моделей Data Mining</a></li>
<li class="chapter" data-level="1.1.5" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_5"><i class="fa fa-check"></i><b>1.1.5</b> Природа многомерного отклика и его моделирование</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="012-R-Intro.html"><a href="012-R-Intro.html"><i class="fa fa-check"></i><b>1.2</b> Статистическая среда R и ее использование в Data Mining</a></li>
<li class="chapter" data-level="1.3" data-path="013-What-This-Book-Is-About.html"><a href="013-What-This-Book-Is-About.html"><i class="fa fa-check"></i><b>1.3</b> О чем эта книга и чего в ней нет</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="021-Model-Quality-Criteria.html"><a href="021-Model-Quality-Criteria.html"><i class="fa fa-check"></i><b>2</b> Статистические модели: критерии и методы оценивания их качества</a><ul>
<li class="chapter" data-level="2.1" data-path="021-Model-Quality-Criteria.html"><a href="021-Model-Quality-Criteria.html#sec_2_1"><i class="fa fa-check"></i><b>2.1</b> Основные шаги построения и верификации моделей</a></li>
<li class="chapter" data-level="2.2" data-path="022-Resampling-Techniques.html"><a href="022-Resampling-Techniques.html"><i class="fa fa-check"></i><b>2.2</b> Использование алгоритмов ресэмплинга для тестирования моделей и оптимизации их параметров</a></li>
<li class="chapter" data-level="2.3" data-path="023-Models-for-Class-Prediction.html"><a href="023-Models-for-Class-Prediction.html"><i class="fa fa-check"></i><b>2.3</b> Модели для предсказания класса объектов</a></li>
<li class="chapter" data-level="2.4" data-path="024-Projecting-Data-onto-a-Plane.html"><a href="024-Projecting-Data-onto-a-Plane.html"><i class="fa fa-check"></i><b>2.4</b> Проецирование многомерных данных на плоскости</a></li>
<li class="chapter" data-level="2.5" data-path="025-MV-analysis.html"><a href="025-MV-analysis.html"><i class="fa fa-check"></i><b>2.5</b> Многомерный статистический анализ данных</a></li>
<li class="chapter" data-level="2.6" data-path="026-Clustering-Methods.html"><a href="026-Clustering-Methods.html"><i class="fa fa-check"></i><b>2.6</b> Методы кластеризации</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="031-Intro-to-Caret.html"><a href="031-Intro-to-Caret.html"><i class="fa fa-check"></i><b>3</b> Пакет <code>caret</code> - инструмент построения статистических моделей в R</a><ul>
<li class="chapter" data-level="3.1" data-path="031-Intro-to-Caret.html"><a href="031-Intro-to-Caret.html#---------caret"><i class="fa fa-check"></i><b>3.1</b> Универсальный интерфейс доступа к функциям машинного обучения в пакете <code id="sec_3_1">caret</code></a></li>
<li class="chapter" data-level="3.2" data-path="032-Removing-Predictors.html"><a href="032-Removing-Predictors.html"><i class="fa fa-check"></i><b>3.2</b> Обнаружение и удаление “ненужных” предикторов</a></li>
<li class="chapter" data-level="3.3" data-path="033-Preprocessing.html"><a href="033-Preprocessing.html"><i class="fa fa-check"></i><b>3.3</b> Предварительная обработка: преобразование и групповая трансформация переменных</a></li>
<li class="chapter" data-level="3.4" data-path="034-Handling-Missing-Values.html"><a href="034-Handling-Missing-Values.html"><i class="fa fa-check"></i><b>3.4</b> Заполнение пропущенных значений в данных</a></li>
<li class="chapter" data-level="3.5" data-path="035-The-train-Functions.html"><a href="035-The-train-Functions.html"><i class="fa fa-check"></i><b>3.5</b> Функция <code>train()</code> из пакета <code id="sec_3_5">caret</code></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html"><i class="fa fa-check"></i><b>4</b> Построение регрессионных моделей различного типа</a><ul>
<li class="chapter" data-level="4.1" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1"><i class="fa fa-check"></i><b>4.1</b> Селекция оптимального набора предикторов линейной модели</a><ul>
<li class="chapter" data-level="4.1.1" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_1"><i class="fa fa-check"></i><b>4.1.1</b> Полная регрессионная модель и пошаговая процедура</a></li>
<li class="chapter" data-level="4.1.2" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_2"><i class="fa fa-check"></i><b>4.1.2</b> Рекурсивное исключение переменных</a></li>
<li class="chapter" data-level="4.1.3" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_3"><i class="fa fa-check"></i><b>4.1.3</b> Генетический алгоритм</a></li>
<li class="chapter" data-level="4.1.4" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_4"><i class="fa fa-check"></i><b>4.1.4</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="042-Regularization.html"><a href="042-Regularization.html"><i class="fa fa-check"></i><b>4.2</b> Регуляризация, частные наименьшие квадраты и kNN-регрессия</a><ul>
<li class="chapter" data-level="4.2.1" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_1"><i class="fa fa-check"></i><b>4.2.1</b> Регрессия по методу “лассо”</a></li>
<li class="chapter" data-level="4.2.2" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_2"><i class="fa fa-check"></i><b>4.2.2</b> Метод частных наименьших квадратов (PLS)</a></li>
<li class="chapter" data-level="4.2.3" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_3"><i class="fa fa-check"></i><b>4.2.3</b> Регрессия по методу <em>k</em> ближайших соседей</a></li>
<li class="chapter" data-level="4.2.4" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_4"><i class="fa fa-check"></i><b>4.2.4</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html"><i class="fa fa-check"></i><b>4.3</b> Построение деревьев регрессии</a><ul>
<li class="chapter" data-level="4.3.1" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html#sec_4_3_1"><i class="fa fa-check"></i><b>4.3.1</b> Построение деревьев на основе рекурсивного разбиения</a></li>
<li class="chapter" data-level="4.3.2" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html#sec_4_3_2"><i class="fa fa-check"></i><b>4.3.2</b> Построение деревьев с использованием алгортма условного вывода</a></li>
<li class="chapter" data-level="4.3.3" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html#sec_4_3_3"><i class="fa fa-check"></i><b>4.3.3</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="044-Ensembles.html"><a href="044-Ensembles.html"><i class="fa fa-check"></i><b>4.4</b> Ансамбли моделей: бэггинг, случайные леса, бустинг</a><ul>
<li class="chapter" data-level="4.4.1" data-path="044-Ensembles.html"><a href="044-Ensembles.html#sec_4_4_1"><i class="fa fa-check"></i><b>4.4.1</b> Бэггинг и случайные леса</a></li>
<li class="chapter" data-level="4.4.2" data-path="044-Ensembles.html"><a href="044-Ensembles.html#sec_4_4_2"><i class="fa fa-check"></i><b>4.4.2</b> Бустинг</a></li>
<li class="chapter" data-level="4.4.3" data-path="044-Ensembles.html"><a href="044-Ensembles.html#sec_4_4_3"><i class="fa fa-check"></i><b>4.4.3</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="045-Comparing-Trees.html"><a href="045-Comparing-Trees.html"><i class="fa fa-check"></i><b>4.5</b> Сравнение построенных моделей и оценка информативности предикторов</a></li>
<li class="chapter" data-level="4.6" data-path="046-MV-Trees.html"><a href="046-MV-Trees.html"><i class="fa fa-check"></i><b>4.6</b> Деревья регрессии с многомерным откликом</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="051-Association-Rules.html"><a href="051-Association-Rules.html"><i class="fa fa-check"></i><b>5</b> Бинарные матрицы и ассоциативные правила</a><ul>
<li class="chapter" data-level="5.1" data-path="051-Association-Rules.html"><a href="051-Association-Rules.html#sec_5_1"><i class="fa fa-check"></i><b>5.1</b> Классификация в бинарных пространствах с использованием классических моделей</a></li>
<li class="chapter" data-level="5.2" data-path="052-Binary-Decision-Trees.html"><a href="052-Binary-Decision-Trees.html"><i class="fa fa-check"></i><b>5.2</b> Бинарные деревья решений</a></li>
<li class="chapter" data-level="5.3" data-path="053-Logic-Rules.html"><a href="053-Logic-Rules.html"><i class="fa fa-check"></i><b>5.3</b> Поиск логических закономерностей в данных</a></li>
<li class="chapter" data-level="5.4" data-path="054-Association-Rules-Algos.html"><a href="054-Association-Rules-Algos.html"><i class="fa fa-check"></i><b>5.4</b> Алгоритмы выделения ассоциативных правил</a></li>
<li class="chapter" data-level="5.5" data-path="055-Traminer.html"><a href="055-Traminer.html"><i class="fa fa-check"></i><b>5.5</b> Анализ последовательностей знаков или событий</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="061-Binary-Classifiers.html"><a href="061-Binary-Classifiers.html"><i class="fa fa-check"></i><b>6</b> Бинарные классификаторы с различными разделяющими поверхностями</a><ul>
<li class="chapter" data-level="6.1" data-path="061-Binary-Classifiers.html"><a href="061-Binary-Classifiers.html#sec_6_1"><i class="fa fa-check"></i><b>6.1</b> Дискриминантный анализ</a></li>
<li class="chapter" data-level="6.2" data-path="062-SVM.html"><a href="062-SVM.html"><i class="fa fa-check"></i><b>6.2</b> Дискриминантный анализ</a></li>
<li class="chapter" data-level="6.3" data-path="063-Nonlinear-Borders.html"><a href="063-Nonlinear-Borders.html"><i class="fa fa-check"></i><b>6.3</b> Ядерные функции машины опорных векторов</a></li>
<li class="chapter" data-level="6.4" data-path="064-Classification-Trees.html"><a href="064-Classification-Trees.html"><i class="fa fa-check"></i><b>6.4</b> Деревья классификации, случайный лес и логистическая регрессия</a></li>
<li class="chapter" data-level="6.5" data-path="065-Comparing-Classifiers.html"><a href="065-Comparing-Classifiers.html"><i class="fa fa-check"></i><b>6.5</b> Процедуры сравнения эффективности моделей классификации</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="071-Multiclass-Classification.html"><a href="071-Multiclass-Classification.html"><i class="fa fa-check"></i><b>7</b> Модели классификации для нескольких классов</a><ul>
<li class="chapter" data-level="7.1" data-path="071-Multiclass-Classification.html"><a href="071-Multiclass-Classification.html#sec_7_1"><i class="fa fa-check"></i><b>7.1</b> Ирисы Фишера и метод <em>k</em> ближайших соседей</a></li>
<li class="chapter" data-level="7.2" data-path="072-NBC.html"><a href="072-NBC.html"><i class="fa fa-check"></i><b>7.2</b> Наивный байесовский классификатор</a></li>
<li class="chapter" data-level="7.3" data-path="073-In-Discriminant-Space.html"><a href="073-In-Discriminant-Space.html"><i class="fa fa-check"></i><b>7.3</b> Классификация в линейном дискриминантном пространстве</a></li>
<li class="chapter" data-level="7.4" data-path="074-Nonlinear-Classifiers.html"><a href="074-Nonlinear-Classifiers.html"><i class="fa fa-check"></i><b>7.4</b> Нелинейные классификаторы в R</a></li>
<li class="chapter" data-level="7.5" data-path="075-Multinomial-Logit.html"><a href="075-Multinomial-Logit.html"><i class="fa fa-check"></i><b>7.5</b> Модель мультиномиального логита</a></li>
<li class="chapter" data-level="7.6" data-path="076-NN.html"><a href="076-NN.html"><i class="fa fa-check"></i><b>7.6</b> Классификаторы на основе искусственных нейронных сетей</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="081-Logit-for-Count.html"><a href="081-Logit-for-Count.html"><i class="fa fa-check"></i><b>8</b> Моделирование порядковых и счетных переменных</a><ul>
<li class="chapter" data-level="8.1" data-path="081-Logit-for-Count.html"><a href="081-Logit-for-Count.html#sec_8_1"><i class="fa fa-check"></i><b>8.1</b> Модель логита для порядковой переменной</a></li>
<li class="chapter" data-level="8.2" data-path="082-NN-with-Caret.html"><a href="082-NN-with-Caret.html"><i class="fa fa-check"></i><b>8.2</b> Настройка параметров нейронных сетей средствами пакета <code id="sec_8_2">caret</code></a></li>
<li class="chapter" data-level="8.3" data-path="083-Model-Complexes.html"><a href="083-Model-Complexes.html"><i class="fa fa-check"></i><b>8.3</b> Методы комплексации модельных прогнозов</a></li>
<li class="chapter" data-level="8.4" data-path="084-GLM-for-Counts.html"><a href="084-GLM-for-Counts.html"><i class="fa fa-check"></i><b>8.4</b> Обобщенные линейные модели для счетных данных</a></li>
<li class="chapter" data-level="8.5" data-path="085-ZIP-for-Counts.html"><a href="085-ZIP-for-Counts.html"><i class="fa fa-check"></i><b>8.5</b> ZIP- и барьерные модели счетных данных</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="091-Data-Transformation.html"><a href="091-Data-Transformation.html"><i class="fa fa-check"></i><b>9</b> Методы многомерной ординации</a><ul>
<li class="chapter" data-level="9.1" data-path="091-Data-Transformation.html"><a href="091-Data-Transformation.html#sec_9_1"><i class="fa fa-check"></i><b>9.1</b> Преобразование данных и вычисление матрицы расстояний</a></li>
<li class="chapter" data-level="9.2" data-path="092-Distance-ANOVA.html"><a href="092-Distance-ANOVA.html"><i class="fa fa-check"></i><b>9.2</b> Непараметрический дисперсионный анализ матриц дистанций</a></li>
<li class="chapter" data-level="9.3" data-path="093-Comparing-Diagrams.html"><a href="093-Comparing-Diagrams.html"><i class="fa fa-check"></i><b>9.3</b> Методы ординации объектов и переменных: построение и сравнение диаграмм</a></li>
<li class="chapter" data-level="9.4" data-path="094-Ordination-Factors.html"><a href="094-Ordination-Factors.html"><i class="fa fa-check"></i><b>9.4</b> Оценка связи ординации с внешними факторами</a></li>
<li class="chapter" data-level="9.5" data-path="095-NMDS.html"><a href="095-NMDS.html"><i class="fa fa-check"></i><b>9.5</b> Неметрическое многомерное шкалирование и построение распределения чувствительности видов</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="101-Partitioning-Algos.html"><a href="101-Partitioning-Algos.html"><i class="fa fa-check"></i><b>10</b> Кластерный анализ</a><ul>
<li class="chapter" data-level="10.1" data-path="101-Partitioning-Algos.html"><a href="101-Partitioning-Algos.html#sec_10_1"><i class="fa fa-check"></i><b>10.1</b> Алгоритмы кластеризации, основанные на разделении</a></li>
<li class="chapter" data-level="10.2" data-path="102-H-Clustering.html"><a href="102-H-Clustering.html"><i class="fa fa-check"></i><b>10.2</b> Иерархическая кластеризация</a></li>
<li class="chapter" data-level="10.3" data-path="103-Clustering-Quality.html"><a href="103-Clustering-Quality.html"><i class="fa fa-check"></i><b>10.3</b> Оценка качества кластеризации</a></li>
<li class="chapter" data-level="10.4" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html"><i class="fa fa-check"></i><b>10.4</b> Другие алгоритмы кластеризации</a><ul>
<li class="chapter" data-level="10.4.1" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html#sec_10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Иерархическая кластеризация на главные компоненты</a></li>
<li class="chapter" data-level="10.4.2" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html#sec_10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Метод нечетких <em>k</em> средних (fuzzy analysis clustering)</a></li>
<li class="chapter" data-level="10.4.3" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html#sec_10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Статистическая модель кластеризации</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="105-Cohonen-Maps.html"><a href="105-Cohonen-Maps.html"><i class="fa fa-check"></i><b>10.5</b> Самоорганизующиеся карты Кохонена</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="111-Rattle-Intro.html"><a href="111-Rattle-Intro.html"><i class="fa fa-check"></i><b>11</b> <code>rattle</code>: графический интерфейс R для реализации алгоритмов Data Mining</a><ul>
<li class="chapter" data-level="11.1" data-path="111-Rattle-Intro.html"><a href="111-Rattle-Intro.html#----rattle"><i class="fa fa-check"></i><b>11.1</b> Начало работы с пакетом <code id="sec_11_1">rattle</code></a></li>
<li class="chapter" data-level="11.2" data-path="112-Descriptive-Stats.html"><a href="112-Descriptive-Stats.html"><i class="fa fa-check"></i><b>11.2</b> Описательная статистика и визуализация данных</a></li>
<li class="chapter" data-level="11.3" data-path="113-Model-Building.html"><a href="113-Model-Building.html"><i class="fa fa-check"></i><b>11.3</b> Построение и тестирование моделей классификации</a></li>
<li class="chapter" data-level="11.4" data-path="114-Descriptive-Models.html"><a href="114-Descriptive-Models.html"><i class="fa fa-check"></i><b>11.4</b> Дескриптивные модели (обучение без учителя)</a><ul>
<li class="chapter" data-level="11.4.1" data-path="114-Descriptive-Models.html"><a href="114-Descriptive-Models.html#sec_11_4_1"><i class="fa fa-check"></i><b>11.4.1</b> Кластерный анализ</a></li>
<li class="chapter" data-level="11.4.2" data-path="114-Descriptive-Models.html"><a href="114-Descriptive-Models.html#sec_11_4_2"><i class="fa fa-check"></i><b>11.4.2</b> Ассоциативные правила</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="120-References.html"><a href="120-References.html"><i class="fa fa-check"></i><b>12</b> Список рекомендуемой литературы</a></li>
<li class="chapter" data-level="" data-path="130-Appendix.html"><a href="130-Appendix.html"><i class="fa fa-check"></i>Приложение: cправочная карта по Data Mining с использованием R</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Классификация, регрессия и другие алгоритмы Data Mining с использованием R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch_1" class="section level1">
<h1><span class="header-section-number">ГЛАВА 1</span> Реализация моделей Data Mining в среде R (вместо предисловия)</h1>
<div id="section_1_1" class="section level2">
<h2><span class="header-section-number">1.1</span> Data Mining как направление анализа данных<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></h2>
<blockquote>
<div align="right">
<em>“Наше видение природы претерпевает радикальные изменения в сторону множественности, темпоральности и сложности.”</em><br />
(Пригожин, Стенгерс, 2005, с. 11)
</div>
</blockquote>
<blockquote>
<div align="right">
<em>“Любое существо, наделенное интеллектом, решает в своей жизни, как правило, только три задачи: а) запоминание по ассоциациям; б) распознавание образов; в) задачи оптимизации и принятия решений.”</em><br />
(Хокинс, Блейксли, 2007)
</div>
</blockquote>
<div id="sec_1_1_1" class="section level3">
<h3><span class="header-section-number">1.1.1</span> От статистического анализа разового эксперимента к Data Mining</h3>
<p>Экспериментальные данные, представленные в компьютерном формате в виде взаимосвязанных таблиц, нуждаются в таких процедурах их обработки анализа и обработки, которые, во-первых, делают очевидными потенциально возможные закономерности и связи между отдельными компонентами и, во-вторых, дают возможность предсказать новые факты. В узком плане речь может идти об оценке значения целевого признака <span class="math inline">\(y\)</span> (отклика) для любого объекта <span class="math inline">\(a\)</span> по его описанию <span class="math inline">\(x\)</span> – набору независимых переменных (предикторов). Однако в более широком смысле затрагиваются традиционно ключевые вопросы многомерного анализа систем:</p>
<ul>
<li>Можно ли считать идентичными анализируемые объекты и за счет каких признаков можно объяснить их возможные отличия?</li>
<li>Как можно объединить отобранные объекты в группы?</li>
<li>Существует ли пространственная или временная изменчивость описанных объектов и каковы ее структурные особенности?</li>
<li>Изменение каких признаков приводит к систематическим причинно-следственным изменениям других?</li>
<li>Как можно осуществить прогноз состояния или поведения анализируемого объекта?</li>
</ul>
<p>До начала 90-х годов основной практикой научного исследования была оценка по Р. Фишеру отдельных “взаимодействий в разовом эксперименте” и, казалось, не было особой нужды кардинально менять ситуацию в этой области. Однако внедрение современных информационных технологий обрушило на людей колоссальные объемы разнородных данных в самых различных областях. Возник вопрос, что делать с этой информацией, поскольку ее осмысление без возможности ее эффективной обработки оказалось невозможным (Дюк, Самойленко, 2001; Барсегян и др., 2009).</p>
<p>Время “лоскутных исследований” стремительно проходит, уступая место комплексному (сейчас говорят “системному”) подходу к описанию процессов и явлений. Поэтому актуальной стратегией современной прикладной статистики является обработка массивов постоянно пополняемых и расширяемых данных с целью создания адекватных многофункциональных моделей изучаемых систем. Создались предпосылки для построения адаптируемых моделей, шаг за шагом улучшающихся по мере поступления новых экспериментальных данных или расширения “сферы влияния” модели.</p>
<p>Современные информационные технологии предполагают размещение таблиц в сконцентрированном виде в хранилищах данных (Барсегян и др., 2009). В этих системах разрозненная информация представляется в виде многомерного куба, которым можно легко манипулировать, извлекая срезами нужную информацию. Для проверки сложных гипотез и решения стратегических проблем используется аппарат извлечения знаний из обширных баз данных (knowledge discovery in databases), основой которого является интеллектуальный анализ данных – Data Mining.</p>
<p>Data Mining - это метафора от горной добычи: разработки пород, извлечение чего-то ценного, откапывание драгоценных крупиц ценных веществ в большом количестве сырого материала, но, соответственно, в применении к данным. По содержанию этот термин достаточно точно определяет Г. Пиатецкий-Шапиро (Piatetsky-Shapiro): <em>“это технология, которая предназначена для поиска в больших объемах данных неочевидных, объективных и полезных на практике закономерностей”</em>.</p>
<p>К настоящему времени термин Data Mining оформился как собирательное название целой совокупности методов при решении конкретных задач глубокого анализа данных с целью выявления скрытых закономерностей. Ряд направлений имеет достаточно специфический и иногда локальный характер: обработка текстов (Text Mining - Sakurai, 2012), анализ социальных сетей и функции работы с графами (Zafarani et al., 2015), решение проблем работы с большими массивами данных (Leskovec et al., 2014), выделение последовательностей термов (Wang, Yang, 2005) и т.д. В настоящей книге эти алгоритмы рассматриваться не будут.</p>
<p>Извлечение научных гипотез (data mining) и их последующий анализ (data analysis) - два комплексных неразрывно связанных процесса. Они протекают по стандартной схеме установления физических законов: сбор экспериментальных данных, организация их в виде таблиц и поиск такого способа обработки, который позволил бы обнаружить в исходных данных новые знания об анализируемом процессе. При этом должно быть ясное понимание того, что эти знания, как всегда для любого сложного явления, остаются в какой-то степени приближенными: чем глубже анализируется реальная сложная система, тем менее определенными становятся наши суждения о ее поведении.</p>
</div>
<div id="sec_1_1_2" class="section level3">
<h3><span class="header-section-number">1.1.2</span> Принципиальная множественность моделей окружающего мира</h3>
<p>Сложность системы и точность, с которой её можно анализировать, связаны обратной зависимостью: <em>“…исследователь постоянно находится между Сциллой усложненности и Харибдой недостоверности. С одной стороны, построенная им модель должна быть простой в математическом отношении, чтобы её можно было исследовать имеющимися средствами. С другой стороны, в результате всех упрощений она не должна утратить и”рациональное зерно“, существо проблемы”</em> (Самарский, 1979, с. 28).</p>
<p>Любая сложная система ведет себя контринтуитивно, т.е. она реагирует на воздействие совсем иным образом, чем это нами интуитивно ожидалось (Форрестер, 1977; Розенберг, 2013). При этом декларируемая нами точность любого математического описания системы - это не абсолютный вердикт, а только принятое нами соглашение о способе отождествления модели и реального мира.</p>
<p>В современных исследованиях обычно анализируются результаты пассивных наблюдений, поскольку поставить управляемый рандомизированный эксперимент часто попросту невозможно. Т.к. мы не точно не знаем, какие параметры определяют наш процесс, то для интерпретации данных крайне важно иметь по возможности полный список конкурирующих гипотез. Итогом статистического анализа является тогда уже не некая неоспоримая истина, а полезные (оптимальные в частном смысле) модели исследуемого явления, которые могут быть сопоставлены между собой, а в дальнейшем уточнены, дополнены или заменены на лучшие. <em>“Процедура проверки значимости нулевой гипотезы, основанная на значении Pval, - квинтэссенция традиционной (ортодоксальной) статистической практики и одновременно - ее величайшее недоразумение и заблуждение… Преодоление порогового (критического) уровня <span class="math inline">\(P_{val} &lt; 0.05\)</span> всего лишь в одной выборке часто необоснованно считается достаточным для вывода о статистической значимости наблюдаемого эффекта (или даже его”достоверности“)”</em> (Хромов-Борисов, 2011). Вероятно, при всей ее категоричности, эта точка зрения вполне заслуживает всяческого внимания.</p>
<p>Сразу стоит также подчеркнуть, что идеального метода, который одинаково хорошо покажет себя для любых целей и на любом наборе данных, попросту не существует. Поэтому нельзя игнорировать принцип <em>множественности моделей</em> В. В. Налимова (1971): для объяснения и предсказания структуры и (или) поведения сложной системы возможно построение нескольких моделей, имеющих одинаковое право на существование. Мультимодельный вывод (Anderson, 2008) предполагает также оценку параметров (их ошибок и доверительных интервалов) на основе не единственной модели, а их ряда. И здесь важны тщательная диагностика и широкая верификация построенных моделей, которые мы будем подробно обсуждать в главе <a href="021-Model-Quality-Criteria.html#ch_2">2</a>.</p>
<p>Наконец, следует помнить о том, что любой статистический метод будет хорош настолько, насколько качественными являются входные данные для обучения модели (англ. <em>“garbage in - garbage out”</em> или “хлам на входе - хлам на выходе”). Без затраты усилий на подготовку обучающей выборки (фильтрация, трансформация, удаление пропущенных значений, создание производных предикторов и т.д.) и понимания моделируемого процесса чудес не случается.</p>
</div>
<div id="sec_1_1_3" class="section level3">
<h3><span class="header-section-number">1.1.3</span> Нарастающая множественность алгоритмов построения моделей</h3>
<p>Одна из ключевых проблем, с которой исследователь сталкивается при разработке статистической модели изучаемого явления, заключается в выборе оптимального для конкретного случая алгоритма извлечения закономерностей. За несколько последних десятилетий было разработано огромное множество методов для решения задач классификации и регрессии, что, безусловно, существенно затрудняет этот выбор.</p>
<p>Попытки ранжирования статистических методов по их эффективности предпринимались неоднократно. Например, одна из недавно опубликованных статей (Fernandez-Delgado et al., 2014) так и называлась <em>“Do We Need Hundreds of Classifiers to Solve Real World Classification Problems?” (“Нужны ли нам сотни классификаторов для решения практических проблем классификации?”)</em>.</p>
<p>В этом обстоятельном исследовании была изучена эффективность работы 179 методов классификации из 17 “семейств” на 121 наборе данных. Читатель может рассматривать эту интересную статью даже просто как справочник по методам распознавания.</p>
<p>В проведенном исследовании для каждого метода классификации оценивалась общая верность предсказаний (overall accuracy) и другие показатели эффективности моделей. Авторы предложили ограничиться следующими четырьмя семействами методов, обладающими наибольшей точностью прогноза (перечислены в порядке убывания эффективности):</p>
<ol style="list-style-type: decimal">
<li>“Случайный лес” (Random Forest);</li>
<li>Машины опорных векторов (Support Vector Machines);</li>
<li>Искусственные нейронные сети (Artificial Neural Networks);</li>
<li>Бустинговые ансамбли моделей (Boosting Ensembles). Отметим, что перечисленные методы практически непригодны для интерпретации механизмов прогнозируемого явления, что вызвало ряд критических замечаний.</li>
</ol>
<p>Хотя приведенный выше список касается моделей-классификаторов, можно ожидать, что перечисленные методы будут также хорошо работать и для задач регрессии (т.е. для предсказания количественного отклика). Тем не менее, если количество предикторов невелико и в них отражается реально существующая закономерность, не следует забывать, что хорошие результаты можно получить и с использованием традиционных методов регрессии. Интересное обсуждение того, что простые методы (на примере классификаторов) при решении практических задач часто превосходят более сложные алгоритмы, можно найти также в широко цитируемой работе Д. Xэнда (Hand, 2006).</p>
<p>Часто выбор того или иного метода обусловлен предыдущим опытом и уровнем осведомленности исследователя. Так, в определенных областях может существовать своего рода “традиция” по использованию тех или иных методов для решения конкретного круга задач. В силу естественной ограниченности своей специализации исследовать может также просто не знать о существовании методов, которые являются более подходящими для его ситуации. Можно столкнуться и с такими случаями, когда некий разработчик программного обеспечения утверждает, что его новый алгоритм “не имеет аналогов”, превосходя все другие доступные решения. Поэтому важнейшей задачей аналитиков является репрезентативное тестирование и тщательная сравнительная диагностика широкого множества моделей-претендентов. Наличие подобной информации будет особенно полезным при работе над новыми проектами/данными, когда предыдущий опыт, который мог бы подсказать, с чего стоит начинать, отсутствует.</p>
</div>
<div id="sec_1_1_4" class="section level3">
<h3><span class="header-section-number">1.1.4</span> Типы и характеристики групп моделей Data Mining</h3>
<p>Предварительно, не вторгаясь в терминологические детали, отметим, что все три дисциплины – Data Mining, машинное обучение (Machine Learning) и статистический анализ - работают на одном и том же предметном поле и используют фактически одни и те же алгоритмы. Общие для них черты и темы гораздо обширней, чем различия, которые носят характер того или иного “уклона”: Data Mining включает в сферу своей компетенции практически необходимые приемы работы с большими массивами данных, сетями и проч., машинное обучение склонно к красивой фразеологии по поводу искусственного интеллекта, а статистический анализ ищет обоснование своих процедур в теоретико-вероятностном подходе. В рамках нашего изложения эти дисциплины рассматриваются как синонимы.</p>
<p>Представленное выше многообразие методов построения моделей порождает разнообразие подходов к их группировке.</p>
<p>По способам решения задачи разделяют на “обучение с учителем” и “обучение без учителя”. Формирование решающих правил без “учителя” объединяет алгоритмы, выявляющие скрытые закономерности без каких-либо предварительных знаний об анализируемых данных. В этом случае результаты наблюдений обычно геометрически интерпретируются как существенно “размытые” сгущения точек (объектов) в многомерном пространстве признаков.</p>
<p>Важнейшими методами поиска закономерностей без учителя являются кластеризация и ординация. Кластеризация исходит из принципа “дискретности” (или разделяемости) и пытается найти оптимальное разбиение исходной совокупности на отдельные группы (классы) однородных объектов таким образом, чтобы различия между группами были максимально возможными. Ординация основывается на принципе континуальности (непрерывности) и ищет упорядоченную последовательность проекций изучаемых объектов на главные оси пространства, с которыми потенциально может быть связана интерпретация научных гипотез.</p>
<p>Алгоритмы индуктивного распознавания с обучением (Ripley, 1996; Vapnik, 1995; Agresti, 2007; Hastie et al., 2007; Zaki M., Meira, 2014) предполагают наличие априори заданной выборки прецедентов, позволяющей построить модели статистической связи <span class="math inline">\(x \rightarrow y\)</span>, где</p>
<ul>
<li><span class="math inline">\(y \in Y\)</span>, <span class="math inline">\(\mathbf{Y}\)</span> - наблюдаемые реализации отклика (responses) или моделируемой случайной величины;</li>
<li><span class="math inline">\(x \in X\)</span>, <span class="math inline">\(\mathbf{X}\)</span> - множество переменных (predictors, independent variables, features), с помощью которых предполагается объяснить изменчивость переменной <span class="math inline">\(y\)</span>.</li>
</ul>
<p>Большинство моделей с учителем устроено таким образом, что их можно записать в виде <span class="math inline">\(y = f(\boldsymbol{x, \beta}) + \boldsymbol{\epsilon}\)</span>, где <span class="math inline">\(f\)</span> - математическая функция, выбранная из некоторого произвольного семейства, <span class="math inline">\(\boldsymbol{\beta}\)</span> - вектор параметров этой функции, а <span class="math inline">\(\boldsymbol{\epsilon}\)</span> - ошибки модели, которые обычно сгенерированы несмещенным, некоррелированным случайным процессом. В ходе построения модели по фиксированным выборочным значениям y минимизируют некоторую функцию остатков <span class="math inline">\(Q(y, \boldsymbol{\beta})\)</span> и находят <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span> - вектор с оптимальными оценками параметров модели.</p>
<p>Варьируя вид функций <span class="math inline">\(f()\)</span> и <span class="math inline">\(Q()\)</span>, можно получать разные модели, из которых предпочтение отдается наиболее эффективным – т.е. моделям, которые дают несмещенные, точные и надежные прогнозы отклика <span class="math inline">\(y\)</span>. Под <em>смещением</em> (bias) понимается различие между рассчитанным по модели прогнозом (например, среднее из оценок всех возможных выборок, которые могут быть взяты из совокупности) и истинным неизвестным значением моделируемой переменной. <em>Точность</em> (accuracy) - различие между оценками отклика, основанного на выборочных данных и истинным значением реализации <span class="math inline">\(y\)</span>. <em>Надежность</em> (precision) - различие между оценкой <span class="math inline">\(y\)</span> по разовой выборке и средним из прогнозов по всему множеству выборок, которые могут быть взяты из той же совокупности.</p>
<p>Выбор подходящего семейства моделей зависит от типа обрабатываемых данных (бинарные, категориальные, порядковые или метрические) и закона их распределения. В первую очередь это, разумеется, относится к форме представления отклика <span class="math inline">\(\mathbf{Y}\)</span>: если он представляет собой категориальную величину, то решается задача классификации, которая может иметь весьма специфические алгоритмы построения моделей и критерии их оптимизации. В случае метрических значений <span class="math inline">\(\mathbf{Y}\)</span> решается задача регрессии, а выбор способа моделирования порядковых или счетных данных часто оказывается неоднозначным. Тип переменных, составляющих матрицу предикторов, тоже в ряде случаев может оказать влияние на степень адекватности полученных результатов.</p>
<p>К типу задач обучения с учителем могут также относиться задачи выявления логических закономерностей и построения совокупности диагностических правил (“дистилляция эталонов”) для последующего распознавания.</p>
<p>Некоторую систематичность в типизацию моделей классификации и регрессии может внести их связь с тремя основными парадигмами машинного обучения: геометрической, вероятностной и логической (Дюк, Самойленко, 2001; Барсегян и др., 2009). Однако группировка по чисто внутренним математическим аспектам достижения оптимального решения, будь то применение переборных процессов, линейной алгебры или статистической теории, на наш взгляд, мало обоснована.</p>
<p>Наконец, традиционным является деление методов Data Mining на описательные и прогнозирующие. Описательные методы должны приводить к <em>объяснению</em> или улучшению <em>понимания</em> данных. Ключевой момент в таких моделях - легкость и прозрачность восприятия исследователем предполагаемой связи между наблюдаемыми переменными. Используемые при этом методы могут относиться как к чисто статистическим (дескриптивный анализ, корреляционный и регрессионный анализ, факторный анализ, дисперсионный анализ, компонентный анализ, дискриминантный анализ, анализ временных рядов), так и к “кибернетическим” (ассоциативные правила, нечеткая логика, деревья решений, сети Кохонена, генетические алгоритмы).</p>
<p>При <em>прогнозировании</em> не ставится цель вскрыть структуру внутренних взаимосвязей между предикторами или сделать сравнительную оценку силы их влияния на отклик. Основная задача - предсказать величину целевого признака y на основании наблюдаемой вариации значений признаков <span class="math inline">\(x_1, x_2, \dots, x_n\)</span>. Если моделируемый процесс имеет объективно сложный характер (например, временные ряды с нестационарным трендом), то такие модели могут иметь большое (до десятков тысяч) или неопределенное число параметров. Для их построения используются следующие принципы самоорганизации: а) определяется класс моделей, в рамках которого возможен последовательный переход от простейших вариантов к наиболее сложным (т.е. индуктивный процесс) и б) задается механизм эволюции моделей и критерии их отбора, благодаря которым каждая отдельная “мутация” оценивается с точки зрения полезности для улучшения качества конечного результата.</p>
<p>Обеспечивая точность предсказания, модели прогнозирования в большинстве случаев существенно теряют в интерпретируемости. Часто бывает затруднительным сделать предметный анализ нескольких десятков коэффициентов моделей МГУА или нейронных сетей. Очевидна стремительно набирающая темп современная тенденция на создание ансамблей из сотен моделей, осуществляющих коллективное предсказание (bagging, Random Forest, boosting), когда традиционные регрессионные методы, такие как анализ ковариаций, стандартных отклонений, доверительных интервалов коэффициентов и проч. вообще не имеют смысла.</p>
</div>
<div id="sec_1_1_5" class="section level3">
<h3><span class="header-section-number">1.1.5</span> Природа многомерного отклика и его моделирование</h3>
<p>При изучении современных информационных, биологических и социально-экономических систем характерна оценка взаимных зависимостей между комплексами многомерных переменных, и тогда задачей построения статистической модели является объяснение изменчивости многомерного отклика <span class="math inline">\(\mathbf{Y}\)</span>. Часто такой отклик может быть представлен в форме некоторой относительно замкнутой системы элементов <span class="math inline">\(N\)</span>, относящихся к множеству <span class="math inline">\(S = \{y\}\)</span> различных типов этих элементов (Арапов и др., 1975; Шитиков и др., 2012, глава <a href="051-Association-Rules.html#ch_5">5</a>). В разных предметных областях это может быть каталог продаваемых автомобилей, категории обеспеченности населения, словарь встретившихся словоформ, список экологических видов, обнаруженных в водоеме, перечень кандидатов, за которых голосовали на выборах и т.д. С помощью классификатора <span class="math inline">\(S\)</span> объекты разбиваются на подпопуляции (классы), т. е. каждой рубрике <span class="math inline">\(y \in S\)</span> соответствует подмножество <span class="math inline">\(N(y)\)</span> или частота всех вхождений объектов этого типа в <span class="math inline">\(N\)</span>. Закон <em>Ципфа–Парето</em>, описывающий характер таких распределений, играет практически ту же универсальную роль, что и закон Гаусса в обычных стохастических процессах с конечной дисперсией (Кудрин, 2002) - подробности см. в главе <a href="091-Data-Transformation.html#ch_9">9</a>.</p>
<p>Композиции объектов <span class="math inline">\(N(y)\)</span>, наблюдаемые в различных условиях (точках пространства, подразделениях, временных срезах), составляют матрицу <span class="math inline">\(\mathbf{Y}\)</span> случайных наблюдений многомерного отклика. Как всегда, задачей статистического моделирования является оценка зависимости <span class="math inline">\(\mathbf{Y}\)</span> от набора экзогенных независимых переменных <span class="math inline">\(\mathbf{X}\)</span>. Построить такую модель <span class="math inline">\(\mathbf{X} \rightarrow \mathbf{Y}\)</span> можно с применением двух подходов. Первый основан на свертке многомерного отклика к одномерному с помощью какой-нибудь функции (получив энтропию Шеннона, индекс типа Доу-Джонса, общую сумму стоимости товаров или нечто похожее), после чего используются обычные модели регрессии. Разумеется, в результате этого оказывается потерянной значительная часть важнейшей информации о структуре объекта. Другой вариант - использовать модели с многомерным откликом, которые обычно представляются в виде канонических матричных уравнений.</p>
<p>Если модель обычной регрессии оценивает условную вероятность математического ожидания <span class="math inline">\(y\)</span>, то в случае многомерного отклика <span class="math inline">\(\mathbf{Y}\)</span> анализируется влияние предикторов на структурную изменчивость корреляционной матрицы (или иной матрицы дистанций). К настоящему времени с использованием такого подхода разработаны методы непараметрического дисперсионного анализа (McArdle, Anderson, 2001), анализа избыточности и канонического корреспондентного анализа (Legendre, Legendre, 2012), построения деревьев с многомерным откликом (De’Ath, 2002) и др. В экологических исследованиях процедуры оптимального проецирования многомерных данных по осям главных компонент и одновременной оценки коэффициентов линейных канонических моделей объединяются под названием прямая ординация.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>В работах, претендующих на солидность, принято начинать с истории вопроса, базовых концепций и разъяснения терминов. Часто это напоминает нудный пересказ известных сюжетных линий перед началом балетного спектакля. Но мы все же не можем отказать себе в удовольствии поумствовать и привлечь мысли признанных авторитетов, чтобы прояснить природу множественности моделей. В конце концов, у читателя есть колесико мышки и он может легко пропустить этот раздел и сразу приступить к сути дела…<a href="01-Data-Mining-Models-in-R.html#fnref1">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="012-R-Intro.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["_main.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
