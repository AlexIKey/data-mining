<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Классификация, регрессия, алгоритмы Data Mining с использованием R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Реализация алгоритмов Data Mining с использованием R">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Классификация, регрессия, алгоритмы Data Mining с использованием R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Реализация алгоритмов Data Mining с использованием R" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Классификация, регрессия, алгоритмы Data Mining с использованием R" />
  
  <meta name="twitter:description" content="Реализация алгоритмов Data Mining с использованием R" />
  

<meta name="author" content="Шитиков В. К., Мастицкий С. Э.">


<meta name="date" content="2017-03-24">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="ch-4.html">
<link rel="next" href="ch-6.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Аннотация</a></li>
<li class="chapter" data-level="1" data-path="ch-1.html"><a href="ch-1.html"><i class="fa fa-check"></i><b>1</b> Реализация моделей Data Mining в среде R</a><ul>
<li class="chapter" data-level="1.1" data-path="ch-1.html"><a href="ch-1.html#section_1_1"><i class="fa fa-check"></i><b>1.1</b> Data Mining как направление анализа данных </a><ul>
<li><a href="ch-1.html#------data-mining"><em>От статистического анализа разового эксперимента к Data Mining</em></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-2.html"><a href="ch-2.html"><i class="fa fa-check"></i><b>2</b> Статистические модели: критерии и методы оценивания их качества</a><ul>
<li class="chapter" data-level="2.1" data-path="ch-2.html"><a href="ch-2.html#sec_2_1"><i class="fa fa-check"></i><b>2.1</b> Основные шаги построения и верификации моделей</a></li>
<li class="chapter" data-level="2.2" data-path="ch-2.html"><a href="ch-2.html#sec_2_2"><i class="fa fa-check"></i><b>2.2</b> Использование алгоритмов ресэмплинга для тестирования моделей и оптимизации их параметров</a></li>
<li class="chapter" data-level="2.3" data-path="ch-2.html"><a href="ch-2.html#sec_2_3"><i class="fa fa-check"></i><b>2.3</b> Модели для предсказания класса объектов</a></li>
<li class="chapter" data-level="2.4" data-path="ch-2.html"><a href="ch-2.html#sec_2_4"><i class="fa fa-check"></i><b>2.4</b> Проецирование многомерных данных на плоскости</a></li>
<li class="chapter" data-level="2.5" data-path="ch-2.html"><a href="ch-2.html#sec_2_5"><i class="fa fa-check"></i><b>2.5</b> Многомерный статистический анализ данных</a></li>
<li class="chapter" data-level="2.6" data-path="ch-2.html"><a href="ch-2.html#sec_2_6"><i class="fa fa-check"></i><b>2.6</b> Методы кластеризации</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-3.html"><a href="ch-3.html"><i class="fa fa-check"></i><b>3</b> Пакет <code>caret</code> - инструмент построения статистических моделей в R</a><ul>
<li class="chapter" data-level="3.1" data-path="ch-3.html"><a href="ch-3.html#---------caret"><i class="fa fa-check"></i><b>3.1</b> Универсальный интерфейс доступа к функциям машинного обучения в пакете <code id="sec_3_1">caret</code></a></li>
<li class="chapter" data-level="3.2" data-path="ch-3.html"><a href="ch-3.html#sec_3_2"><i class="fa fa-check"></i><b>3.2</b> Обнаружение и удаление “ненужных” предикторов</a></li>
<li class="chapter" data-level="3.3" data-path="ch-3.html"><a href="ch-3.html#sec_3_3"><i class="fa fa-check"></i><b>3.3</b> Предварительная обработка: преобразование и групповая трансформация переменных</a></li>
<li class="chapter" data-level="3.4" data-path="ch-3.html"><a href="ch-3.html#sec_3_4"><i class="fa fa-check"></i><b>3.4</b> Заполнение пропущенных значений в данных</a></li>
<li class="chapter" data-level="3.5" data-path="ch-3.html"><a href="ch-3.html#-train---caret"><i class="fa fa-check"></i><b>3.5</b> Функция <code>train()</code> из пакета <code id="sec_3_5">caret</code></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-4.html"><a href="ch-4.html"><i class="fa fa-check"></i><b>4</b> Построение регрессионных моделей различного типа</a><ul>
<li class="chapter" data-level="4.1" data-path="ch-4.html"><a href="ch-4.html#sec_4_1"><i class="fa fa-check"></i><b>4.1</b> Селекция оптимального набора предикторов линейной модели</a><ul>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#-----"><i class="fa fa-check"></i>Полная регрессионная модель и пошаговая процедура</a></li>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#--"><i class="fa fa-check"></i>Рекурсивное исключение переменных</a></li>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#-"><i class="fa fa-check"></i>Генетический алгоритм</a></li>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#------"><i class="fa fa-check"></i>Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-4.html"><a href="ch-4.html#sec_4_2"><i class="fa fa-check"></i><b>4.2</b> Регуляризация, частные наименьшие квадраты и kNN-регрессия</a><ul>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#---"><i class="fa fa-check"></i>Регрессия по методу “лассо”</a></li>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#----pls"><i class="fa fa-check"></i>Метод частных наименьших квадратов (PLS)</a></li>
<li><a href="ch-4.html#---k--">Регрессия по методу <em>k</em> ближайших соседей</a></li>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#-------1"><i class="fa fa-check"></i>Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-4.html"><a href="ch-4.html#sec_4_3"><i class="fa fa-check"></i><b>4.3</b> Построение деревьев регрессии</a><ul>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#-----"><i class="fa fa-check"></i>Построение деревьев на основе рекурсивного разбиения</a></li>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#------"><i class="fa fa-check"></i>Построение деревьев с использованием алгортма условного вывода</a></li>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#-------2"><i class="fa fa-check"></i>Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-4.html"><a href="ch-4.html#sec_4_4"><i class="fa fa-check"></i><b>4.4</b> Ансамбли моделей: бэггинг, случайные леса, бустинг</a><ul>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#---"><i class="fa fa-check"></i>Бэггинг и случайные леса</a></li>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#d091d183d181d182d0b8d0bdd0b3"><i class="fa fa-check"></i>Бустинг</a></li>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#-------3"><i class="fa fa-check"></i>Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="ch-4.html"><a href="ch-4.html#sec_4_5"><i class="fa fa-check"></i><b>4.5</b> Сравнение построенных моделей и оценка информативности предикторов</a></li>
<li class="chapter" data-level="4.6" data-path="ch-4.html"><a href="ch-4.html#sec_4_6"><i class="fa fa-check"></i><b>4.6</b> Деревья регрессии с многомерным откликом</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-5.html"><a href="ch-5.html"><i class="fa fa-check"></i><b>5</b> Бинарные матрицы и ассоциативные правила</a><ul>
<li class="chapter" data-level="5.1" data-path="ch-5.html"><a href="ch-5.html#sec_5_1"><i class="fa fa-check"></i><b>5.1</b> Классификация в бинарных пространствах с использованием классических моделей</a></li>
<li class="chapter" data-level="5.2" data-path="ch-5.html"><a href="ch-5.html#sec_5_2"><i class="fa fa-check"></i><b>5.2</b> Бинарные деревья решений</a></li>
<li class="chapter" data-level="5.3" data-path="ch-5.html"><a href="ch-5.html#sec_5_3"><i class="fa fa-check"></i><b>5.3</b> Поиск логических закономерностей в данных</a></li>
<li class="chapter" data-level="5.4" data-path="ch-5.html"><a href="ch-5.html#sec_5_4"><i class="fa fa-check"></i><b>5.4</b> Алгоритмы выделения ассоциативных правил</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-6.html"><a href="ch-6.html"><i class="fa fa-check"></i><b>6</b> Бинарные классфикаторы с различными разделяющими поверхностями</a><ul>
<li class="chapter" data-level="6.1" data-path="ch-6.html"><a href="ch-6.html#sec_6_1"><i class="fa fa-check"></i><b>6.1</b> Дискриминантный анализ</a></li>
<li class="chapter" data-level="6.2" data-path="ch-6.html"><a href="ch-6.html#sec_6_2"><i class="fa fa-check"></i><b>6.2</b> Дискриминантный анализ</a></li>
<li class="chapter" data-level="6.3" data-path="ch-6.html"><a href="ch-6.html#sec_6_3"><i class="fa fa-check"></i><b>6.3</b> Классификаторы с использованием нелинейных разделяющих поверхностей</a></li>
<li class="chapter" data-level="6.4" data-path="ch-6.html"><a href="ch-6.html#sec_6_4"><i class="fa fa-check"></i><b>6.4</b> Деревья классификации, случайный лес и логистическая регрессия</a></li>
<li class="chapter" data-level="6.5" data-path="ch-6.html"><a href="ch-6.html#sec_6_5"><i class="fa fa-check"></i><b>6.5</b> Процедуры сравнения эффективности моделей классификации</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Классификация, регрессия, алгоритмы Data Mining с использованием R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch_5" class="section level1">
<h1><span class="header-section-number">ГЛАВА 5</span> Бинарные матрицы и ассоциативные правила</h1>
<div id="sec_5_1" class="section level2">
<h2><span class="header-section-number">5.1</span> Классификация в бинарных пространствах с использованием классических моделей</h2>
<p>Большое количество задач связано с нахождением закономерностей в пространствах бинарных переменных: расшифровка геномного кода, анализ пиков на спектрофотометрах, обработка социологических анкет с ответами “Да/Нет” и т.д. Методы анализа таких выборок рассмотрим на не слишком серьезном примере, фигурирующем в книге Дука и Самойленко (2001).</p>
<p>Предположим, что некая избирательная комиссия озаботилась выделением характерных черт групп электората, голосующих за различные общественные платформы. На представленном рис. <a href="ch-5.html#fig:fig-5-1">5.1</a> схематично изображены лица людей, относящихся к двум классам (для определенности пусть это будут “Патриоты” и “Демократы”). Ставится задача найти комбинации признаков, характеризующие это разделение.</p>
<div class="figure" style="text-align: center"><span id="fig:fig-5-1"></span>
<img src="figures/faces.png" alt="Изображения лиц людей, относящихся к двум различным классам (Дук, Самойленко, 2001)" width="600px" />
<p class="caption">
Рисунок 5.1: Изображения лиц людей, относящихся к двум различным классам (Дук, Самойленко, 2001)
</p>
</div>
<p>Скорее всего, визуальный анализ, проведенный читателем, с большими трудностями позволит определить, чем лица разных классов отличаются друг от друга и что объединяет лица одного класса. Даже такую, на первый взгляд простую, задачу обнаружения скрытых закономерностей лучше поручить компьютерным методам анализа данных.</p>
<p>Прежде всего, выделим признаки, характеризующие изображенные лица. Это следующие характеристики:</p>
<p><span class="math inline">\(x_1\)</span> (голова) – круглая (1) или овальная (0);<br />
<span class="math inline">\(x_2\)</span> (уши) – оттопыренные (1) или прижатые (0);<br />
<span class="math inline">\(x_3\)</span> (нос) – круглый (1) или длинный (0);<br />
<span class="math inline">\(x_4\)</span> (глаза) – круглые (1) или узкие (0);<br />
<span class="math inline">\(x_5\)</span> (лоб) – с морщинами (1) или без морщин (0);<br />
<span class="math inline">\(x_6\)</span> (носогубная складка) – есть (1) или нет (0);<br />
<span class="math inline">\(x_7\)</span> (губы) – толстые (1) или тонкие (0);<br />
<span class="math inline">\(x_8\)</span> (волосы) – есть (1) или нет (0);<br />
<span class="math inline">\(x_9\)</span> (усы) – есть (1) или нет (0);<br />
<span class="math inline">\(x_{10}\)</span> (борода) – есть (1) или нет (0);<br />
<span class="math inline">\(x_{11}\)</span> (очки) – есть (1) или нет (0);<br />
<span class="math inline">\(x_{12}\)</span> (родинка на щеке) – есть (1) или нет (0);<br />
<span class="math inline">\(x_{13}\)</span> (бабочка) – есть (1) или нет (0);<br />
<span class="math inline">\(x_{14}\)</span> (брови) – подняты кверху (1) или опущены книзу (0);<br />
<span class="math inline">\(x_{15}\)</span> (серьга) – есть (1) или нет (0);<br />
<span class="math inline">\(x_{16}\)</span> (курительная трубка) – есть (1) или нет (0).</p>
<p>Загрузим исходную матрицу данных, соответствующую изображенным лицам, cтроки которой соответствуют объектам (<span class="math inline">\(n\)</span> = 16), а столбцы - выделенным бинарным признакам (<span class="math inline">\(m\)</span> = 16). Объекты с номерами 1-8 относятся к классу 1, а с номерами 9-16 - к классу 2.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">DFace &lt;-<span class="st"> </span><span class="kw">read.delim</span>(<span class="dt">file =</span> <span class="st">&quot;data/Faces.txt&quot;</span>, 
                    <span class="dt">header =</span> <span class="ot">TRUE</span>, <span class="dt">row.names =</span> <span class="dv">1</span>)
<span class="kw">head</span>(DFace, <span class="dv">8</span>)</code></pre></div>
<pre><code>##   x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 Class
## 1  0  1  0  0  1  1  0  0  1   1   1   0   1   1   0   1     1
## 2  1  0  1  1  0  0  1  1  0   1   1   1   0   0   1   0     1
## 3  0  0  0  1  1  1  0  1  1   0   1   1   1   0   0   1     1
## 4  0  1  1  0  0  1  1  0  0   1   1   0   0   1   1   1     1
## 5  1  1  0  1  0  1  0  1  0   1   0   1   0   1   1   0     1
## 6  0  0  1  0  1  1  1  0  1   0   1   0   1   0   1   1     1
## 7  1  1  0  1  0  0  0  0  1   1   0   0   1   1   1   1     1
## 8  0  0  1  1  0  1  1  0  1   1   1   0   1   0   1   0     1</code></pre>
<p>Попытаемся решить поставленную задачу с помощью классических статистических методов. Множественная логистическая регрессия - один из распространенных подходов к моделированию значений отклика, заданного альтернативной шкалой 0/1 (Мастицкий, Шитиков, 2015).</p>
<p>Если <span class="math inline">\(\boldsymbol{x}\)</span> - вектор значений <span class="math inline">\(m\)</span> независимых переменных, то вероятность <span class="math inline">\(P\)</span> того, что отклик y примет значение 1, может быть описана моделью</p>
<p><span class="math display">\[P(y = 1|\boldsymbol{x}) \equiv p(\boldsymbol{x}) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_m x_m,\]</span></p>
<p>которая после оценки коэффициентов регрессии <span class="math inline">\(\beta_i\)</span> будет возвращать искомую вероятность на интервале [0, 1]. Для того, чтобы обеспечить линейность функции относительно предикторов <span class="math inline">\(x_i\)</span> и робастность процедуры подбора коэффициентов, модель переписывают как функцию логита:</p>
<p><span class="math display">\[g(y) = \log \left( \frac{p(\boldsymbol{x})}{1 - p(\boldsymbol{x})} \right) =  \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_m x_m.\]</span></p>
<p>Неизвестные параметры модели <span class="math inline">\(\beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_m x_m\)</span> обычно находят с использованием функции максимального правдоподобия, а предсказанный результат <span class="math inline">\(g(y)\)</span> трансформируют обратно в вероятности <span class="math inline">\(p(\boldsymbol{x})\)</span> в диапазоне между 0 и 1.</p>
<p>Построенной модели регрессии при использовании представленных данных и полного комплекта переменных <span class="math inline">\(m\)</span> = 16 соответствует значение критерия Акаике AIC = 28. Компактную модель логита, состоящую только из 7 переменных, можно получить в ходе селекции набора информативных переменных на основе стандартной пошаговой процедуры <code>step()</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logit &lt;-<span class="st"> </span><span class="kw">glm</span>((Class -<span class="st"> </span><span class="dv">1</span>) ~<span class="st"> </span>., <span class="dt">data =</span> DFace, <span class="dt">family =</span> binomial)
logit.step &lt;-<span class="st"> </span><span class="kw">step</span>(logit)</code></pre></div>
<pre><code>## Start:  AIC=28
## (Class - 1) ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + 
##     x11 + x12 + x13 + x14 + x15 + x16
## 
## 
## Step:  AIC=28
## (Class - 1) ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + 
##     x11 + x12 + x13 + x14 + x15
## 
## 
## Step:  AIC=28
## (Class - 1) ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + 
##     x11 + x12 + x13 + x14
## 
## 
## Step:  AIC=28
## (Class - 1) ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x10 + x11 + 
##     x12 + x13 + x14
## 
##        Df Deviance    AIC
## - x1    1   0.0000 26.000
## - x6    1   0.0000 26.000
## - x8    1   0.0000 26.000
## - x10   1   0.0000 26.000
## - x11   1   0.0000 26.000
## - x12   1   0.0000 26.000
## - x14   1   0.0000 26.000
## - x3    1   0.0000 26.000
## - x7    1   0.0000 26.000
## - x2    1   0.0000 26.000
## - x5    1   0.0000 26.000
## - x4    1   0.0000 26.000
## &lt;none&gt;      0.0000 28.000
## - x13   1   2.7726 28.773
## 
## Step:  AIC=26
## (Class - 1) ~ x2 + x3 + x4 + x5 + x6 + x7 + x8 + x10 + x11 + 
##     x12 + x13 + x14
## 
##        Df Deviance    AIC
## - x6    1   0.0000 24.000
## - x8    1   0.0000 24.000
## - x10   1   0.0000 24.000
## - x11   1   0.0000 24.000
## - x12   1   0.0000 24.000
## - x3    1   0.0000 24.000
## - x7    1   0.0000 24.000
## - x2    1   0.0000 24.000
## - x14   1   0.0000 24.000
## - x5    1   0.0000 24.000
## - x4    1   0.0000 24.000
## &lt;none&gt;      0.0000 26.000
## - x13   1   2.7726 26.773
## 
## Step:  AIC=24
## (Class - 1) ~ x2 + x3 + x4 + x5 + x7 + x8 + x10 + x11 + x12 + 
##     x13 + x14
## 
##        Df Deviance    AIC
## - x8    1   0.0000 22.000
## - x10   1   0.0000 22.000
## - x11   1   0.0000 22.000
## - x12   1   0.0000 22.000
## - x3    1   0.0000 22.000
## - x2    1   0.0000 22.000
## - x5    1   0.0000 22.000
## - x14   1   0.0000 22.000
## - x7    1   0.0000 22.000
## - x4    1   0.0000 22.000
## &lt;none&gt;      0.0000 24.000
## - x13   1   2.7726 24.773
## 
## Step:  AIC=22
## (Class - 1) ~ x2 + x3 + x4 + x5 + x7 + x10 + x11 + x12 + x13 + 
##     x14
## 
##        Df Deviance    AIC
## - x12   1   0.0000 20.000
## - x10   1   0.0000 20.000
## - x11   1   0.0000 20.000
## - x14   1   0.0000 20.000
## - x2    1   0.0000 20.000
## - x3    1   0.0000 20.000
## - x5    1   0.0000 20.000
## - x4    1   0.0000 20.000
## &lt;none&gt;      0.0000 22.000
## - x13   1   2.7726 22.773
## - x7    1   2.7726 22.773
## 
## Step:  AIC=20
## (Class - 1) ~ x2 + x3 + x4 + x5 + x7 + x10 + x11 + x13 + x14
## 
##        Df Deviance    AIC
## - x10   1   0.0000 18.000
## - x11   1   0.0000 18.000
## - x5    1   0.0000 18.000
## - x4    1   0.0000 18.000
## - x2    1   0.0000 18.000
## &lt;none&gt;      0.0000 20.000
## - x13   1   2.7726 20.773
## - x7    1   2.7726 20.773
## - x14   1   5.5452 23.545
## - x3    1  10.0439 28.044
## 
## Step:  AIC=18
## (Class - 1) ~ x2 + x3 + x4 + x5 + x7 + x11 + x13 + x14
## 
##        Df Deviance    AIC
## - x11   1   0.0000 16.000
## &lt;none&gt;      0.0000 18.000
## - x7    1   2.7726 18.773
## - x2    1   2.7726 18.773
## - x13   1   2.7726 18.773
## - x14   1   5.5452 21.545
## - x5    1   5.5452 21.545
## - x4    1   6.4907 22.491
## - x3    1  10.4630 26.463
## 
## Step:  AIC=16
## (Class - 1) ~ x2 + x3 + x4 + x5 + x7 + x13 + x14
## 
##        Df Deviance    AIC
## &lt;none&gt;      0.0000 16.000
## - x2    1   2.7726 16.773
## - x7    1   2.7726 16.773
## - x13   1   2.7726 16.773
## - x3    1  13.5896 27.590
## - x14   1  13.5896 27.590
## - x4    1  14.9021 28.902
## - x5    1  15.1951 29.195</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(logit.step)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = (Class - 1) ~ x2 + x3 + x4 + x5 + x7 + x13 + x14, 
##     family = binomial, data = DFace)
## 
## Deviance Residuals: 
##        Min          1Q      Median          3Q         Max  
## -5.479e-06  -4.277e-06   0.000e+00   3.274e-06   6.239e-06  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)   -329.06  763966.35       0        1
## x2              50.59  190798.82       0        1
## x3             101.32  274757.62       0        1
## x4             102.33  279732.64       0        1
## x5             101.83  278492.43       0        1
## x7              50.59  190798.82       0        1
## x13             49.90  202466.80       0        1
## x14            101.32  274757.62       0        1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2.2181e+01  on 15  degrees of freedom
## Residual deviance: 2.2719e-10  on  8  degrees of freedom
## AIC: 16
## 
## Number of Fisher Scoring iterations: 25</code></pre>
<p>Значение информационного критерия значительно снизилось, остаточный девианс практически равен 0, но при этом все оцененные коэффициенты оказались статистически незначимыми. Вероятно, параметрический алгоритм оценки значимости коэффициентов не вполне работоспособен при небольшом числе измерений и/или особой конфигурации данных. Однако насколько эффективен построенный классификатор при предсказании? Рассчитаем число выполненных ошибок и таблицу неточностей:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mp &lt;-<span class="st"> </span><span class="kw">predict</span>(logit.step, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) 
pred =<span class="st"> </span><span class="kw">ifelse</span>(mp &gt;<span class="st"> </span><span class="fl">0.5</span>, <span class="dv">2</span>, <span class="dv">1</span>)
CTab &lt;-<span class="st"> </span><span class="kw">table</span>(Факт =<span class="st"> </span>DFace$Class, Прогноз =<span class="st"> </span>pred)
Acc =<span class="st"> </span><span class="kw">mean</span>(pred ==<span class="st"> </span>DFace$Class)
<span class="kw">paste</span>(<span class="st">&quot;Точность=&quot;</span>, <span class="kw">round</span>(<span class="dv">100</span>*Acc, <span class="dv">2</span>), <span class="st">&quot;%&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>) </code></pre></div>
<pre><code>## [1] &quot;Точность=100%&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">barplot</span>(mp -<span class="st"> </span><span class="fl">0.5</span>, <span class="dt">col =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Члены выборки&quot;</span>,
        <span class="dt">ylab =</span> <span class="st">&quot;Вероятности P - 0.5&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-5-2"></span>
<img src="_main_files/figure-html/fig-5-2-1.png" alt="Диаграмма вероятностей, предсказанных логит-регрессией" width="576" />
<p class="caption">
Рисунок 5.2: Диаграмма вероятностей, предсказанных логит-регрессией
</p>
</div>
<p>Все члены обучающей выборки с использованием полученной регрессионной модели были верно классифицированы и с максимальной вероятностью (см. рис. <a href="ch-5.html#fig:fig-5-2">5.2</a>). Перекрестную проверку построенной модели можно выполнить, например, с использованием функции <code>cv.glm()</code> из пакета <code>boot</code>. Если параметр <code>K</code>, определяющий число блоков разбиения, опущен, то выполняется скользящий контроль:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(boot)
<span class="kw">cv.glm</span>(DFace, logit.step)$delta</code></pre></div>
<pre><code>## [1] 2.564444e-17 2.404152e-17</code></pre>
<p>В результате получены среднеквадратичное отклонение по координате логита от проверочных наблюдений до гиперплоскости, найденной на обучающих выборках, а также его скорректированное значение (adjusted cross-validation estimate of prediction error). Поскольку ошибки перекрестной проверки ничтожно малы и все объекты в ее ходе были правильно опознаны, то в целом полученную модель можно считать высоко эффективной.</p>
<p>Мы убедились в том, что логистическая регрессия и функция <code>glm()</code> отлично работают с индикаторными переменными. Однако строгие аналитики (Дук, Самойленко, 2001) считают, что “вряд ли такое правило способно удовлетворить разработчика интеллектуальной системы. Оно формально, не дает нового знания, а попытка дать интерпретацию коэффициентам регрессии приводит к сомнительным выводам. Непонятно, например, почему вес формы носа (<code>х3</code>) в два раза превышает вес оттопыренности ушей (<code>х2</code>) и т.д.” Несмотря на высказанные сомнения, мы достигли желаемой цели - достоверное и эффективное правило классификации построено, а из анализа коэффициентов модели легко сделать вывод, что признаки <code>x3</code>, <code>x4</code>, <code>x5</code> и <code>x14</code> склонны иметь демократы, а показатели <code>x2</code>, <code>x7</code> и <code>x13</code> характеризуют патриотические устремления.</p>
<p>Рассмотрим теперь результаты стандартного линейного дискриминантного анализа, не вдаваясь в его исходные статистические предпосылки, смысл и технику вычислений (это будет подробно обсуждаться в следующих главах):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;MASS&quot;</span>) 
Fac.lda &lt;-<span class="st"> </span><span class="kw">lda</span>(DFace[,<span class="dv">1</span>:<span class="dv">16</span>], <span class="dt">grouping =</span> DFace$Class) 
pred &lt;-<span class="st"> </span><span class="kw">predict</span>(Fac.lda, DFace[, <span class="dv">1</span>:<span class="dv">16</span>])
<span class="kw">table</span>(Факт =<span class="st"> </span>DFace$Class, Прогноз =<span class="st"> </span>pred$class)</code></pre></div>
<pre><code>##     Прогноз
## Факт 1 2
##    1 6 2
##    2 2 6</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Acc =<span class="st"> </span><span class="kw">mean</span>(DFace$Class ==<span class="st"> </span>pred$class)
<span class="kw">paste</span>(<span class="st">&quot;Точность=&quot;</span>, <span class="kw">round</span>(<span class="dv">100</span>*Acc, <span class="dv">2</span>), <span class="st">&quot;%&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</code></pre></div>
<pre><code>## [1] &quot;Точность=75%&quot;</code></pre>
<p>Обратим внимание на неприятное сообщение о коллинеарности переменных и мало впечатляющие результаты прогноза.</p>
<p>Стохастические модели многомерных двоичных случайных величин основываются на распределении Бернулли <span class="math inline">\(\mathbf{X} = \{ x_1, \dots, x_m \} = \text{Be}_m(\boldsymbol{\mu}, \zeta)\)</span>, где <span class="math inline">\(\boldsymbol{\mu}\)</span> - вектор математических ожиданий, а <span class="math inline">\(\zeta\)</span> включает <span class="math inline">\((2^m - m - 1)\)</span> параметров, оценивающих взаимодействие между переменными <span class="math inline">\(x_j\)</span>. Как и в одномерном случае, дисперсии полностью определяются через средние <span class="math inline">\(\text{Var}(x_j) = \mu_j(1-\mu_j)\)</span>.</p>
<p>Во многих случаях, особенно для предсказания на основе выборок малого объема, но с большим числом предикторов, представляется разумным следовать “наивному” байесовскому подходу, т.е. игнорировать зависимости между индивидуальными переменными <span class="math inline">\(x_j\)</span>. Тогда количество параметров многомерной модели Бернулли существенно сокращается и функция объединенной плотности вероятности определяется произведением <span class="math inline">\(\mu_j\)</span> и диагональной ковариационной матрицей <span class="math inline">\(\text{Var}(x_j) = \text{diag}[\mu_j(1-\mu_j)]\)</span>.</p>
<p>С учетом этих положений разработан <em>метод бинарного дискриминантного анализа</em> (<code>binDA</code> - binary discriminant analysis; Gibb, Strimmer, 2015). Дискриминантная функция, полученная на основе теоремы Байеса, для каждого класса с меткой y имеет следующий вид:</p>
<p><span class="math display">\[d_y(x) = \log \pi_y + \log(P(\boldsymbol{x}|y)) + C,\]</span></p>
<p>где <span class="math inline">\(\pi_y\)</span> - априорная вероятность класса <span class="math inline">\(y\)</span>, <span class="math inline">\(C\)</span> - константа, а совокупная условная вероятность предикторов в зависимости от значения отклика вычисляется по формуле</p>
<p><span class="math display">\[
P(\boldsymbol{x}|y)=\prod_{i=1}^m
\begin{cases}
1-\mu_{yj}, \, \, \text{если} \, \, x_j = 0\\
\mu_{yj}, \, \, \text{если} \, \, x_j = 1
\end{cases}
\]</span></p>
<p>Тестируемый объект относится к тому классу <span class="math inline">\(y\)</span>, дискриминантная функция которого <span class="math inline">\(d_y(x)\)</span> максимальна. Параметром алгоритма является <span class="math inline">\(\lambda\)</span> - степень ослабления дисбаланса в частотах классов (<code>lambda.freqs</code> - shrinkage intensity for the frequencies). При <code>lambda.freqs = 0</code> используются эмпирические частоты классов <span class="math inline">\(\pi_y= n_y/n\)</span>, а при <code>lambda.freqs = 1</code> все вероятности принимаются равными.</p>
<p>Важным разделом алгоритма является устранение подмножества предикторов, влияние которых на значение метки y статистически незначимо. Разность между значениями логарифма функции правдоподобия полной модели и нуль-модели без параметров равна относительной энтропии, или дивергенции <span class="math inline">\(D\)</span> Кульбака-Лейблера (Kullback-Leibler). Относительные вклады <span class="math inline">\(S_j\)</span> каждого индивидуального предиктора в величину <span class="math inline">\(D\)</span> определяют меру значимости <span class="math inline">\(j\)</span>-й переменной в результате классификации. Отсюда легко найти матрицу <span class="math inline">\(t\)</span>-значений, соответствующих разностям между общим средним и средними для каждой переменной относительно каждого класса.</p>
<p>Алгоритм <code>binDA</code> реализован в пакете <code>binda</code> для R. Выполним построение дискриминантной модели и оценим точность прогнозирования на обучающей выборке:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(binda)
Xtrain &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(DFace[, <span class="dv">1</span>:<span class="dv">16</span>])
<span class="kw">is.binaryMatrix</span>(Xtrain) <span class="co"># Проверяем бинарность матрицы</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Class =<span class="st"> </span><span class="kw">as.factor</span>(DFace$Class)
binda.fit =<span class="st"> </span><span class="kw">binda</span>(Xtrain, Class, <span class="dt">lambda.freq =</span> <span class="dv">1</span>)</code></pre></div>
<pre><code>## Number of variables: 16 
## Number of observations: 16 
## Number of classes: 2 
## 
## Specified shrinkage intensity lambda.freq (frequencies): 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred &lt;-<span class="st"> </span><span class="kw">predict</span>(binda.fit, Xtrain)</code></pre></div>
<pre><code>## Prediction uses 16 features.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(Факт =<span class="st"> </span>DFace$Class, Прогноз =<span class="st"> </span>pred$class)</code></pre></div>
<pre><code>##     Прогноз
## Факт 1 2
##    1 7 1
##    2 1 7</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Acc =<span class="st"> </span><span class="kw">mean</span>(DFace$Class!=<span class="st"> </span>pred$class)
<span class="kw">paste</span>(<span class="st">&quot;Точность=&quot;</span>, <span class="kw">round</span>(<span class="dv">100</span>*Acc, <span class="dv">2</span>), <span class="st">&quot;%&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</code></pre></div>
<pre><code>## [1] &quot;Точность=12.5%&quot;</code></pre>
<p>При равных апостериорных вероятностях классов 0.5/0.5 объект №4 был ошибочно отнесен ко второму классу.</p>
<p>Выполним теперь оценку информативности предикторов по <span class="math inline">\(t\)</span>-критерию (рис. <a href="ch-3.html#fig:fig-3-3">3.3</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># ранжируем предикторы</span>
binda.x &lt;-<span class="st"> </span><span class="kw">binda.ranking</span>(Xtrain, Class)</code></pre></div>
<pre><code>## Number of variables: 16 
## Number of observations: 16 
## Number of classes: 2 
## 
## Estimating optimal shrinkage intensity lambda.freq (frequencies): 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(binda.x, <span class="dt">top =</span> <span class="dv">40</span>, <span class="dt">arrow.col =</span> <span class="st">&quot;blue&quot;</span>,
     <span class="dt">zeroaxis.col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Предикторы&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-5-3"></span>
<img src="_main_files/figure-html/fig-5-3-1.png" alt="Диаграмма вероятностей, предсказанных логит-регрессией" width="672" />
<p class="caption">
Рисунок 5.3: Диаграмма вероятностей, предсказанных логит-регрессией
</p>
</div>
<p>Все переменные разделились на две группы: значимые для классификации (<code>x2</code>, <code>x3</code>, <code>x6</code>, <code>x7</code>, <code>x10</code>, <code>x11</code>, <code>x14</code>, <code>x15</code>) со значением <span class="math inline">\(t\)</span> = Все переменные разделились на две группы: значимые для классификации (<code>x2</code>, <code>x3</code>, <code>x6</code>, <code>x7</code>, <code>x10</code>, <code>x11</code>, <code>x14</code>, <code>x15</code>) со значением <span class="math inline">\(t = \pm 1.032\)</span> и незначимые (<code>x1</code>, <code>x4</code>, <code>x5</code>, <code>x8</code>, <code>x9</code>, <code>x12</code>, <code>x13</code>, <code>x16</code>) с <span class="math inline">\(t = 0\)</span>. Интересно сравнить этот список со списком информативных предикторов, полученных пошаговой процедурой логистической регрессии.1.032 и незначи(x1, x4, x5, x8, x9, x12, x13, x16) с t = 0. Интересно сравнить этот список со списком информативных предикторов, полученных пошаговой процедурой логистической регрессии.</p>
<p>К сожалению, использование функции <code>train()</code> из пакета <code>caret</code> как с параметрами <code>method = &quot;glm&quot;</code>, <code>family = binomial</code>, так и <code>method = &quot;binda&quot;</code> привело к неудаче. Видимо при <span class="math inline">\(n\)</span> = 16 еще не достигнут порог репрезентативности исходных данных, при котором эта функция начинает стабильно выдавать адекватные результаты.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret) 
binda.train &lt;-<span class="st"> </span><span class="kw">train</span>(Xtrain, Class, <span class="dt">method =</span> <span class="st">&quot;binda&quot;</span>, <span class="dt">verbose =</span> <span class="ot">FALSE</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret) 
binda.train &lt;-<span class="st"> </span><span class="kw">train</span>(Xtrain, Class, <span class="dt">method =</span> <span class="st">&quot;binda&quot;</span>, <span class="dt">verbose =</span> <span class="ot">FALSE</span>)
binda.train</code></pre></div>
<p>Отрицательная величина <code>Kappa</code> практически означает, что тестируемый классификатор хуже случайного угадывания. Однако итоговая модель подобных неожиданностей не несет:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred.train &lt;-<span class="st"> </span><span class="kw">predict</span>(binda.train, Xtrain)</code></pre></div>
<pre><code>## Prediction uses 16 features.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(CTab &lt;-<span class="st"> </span><span class="kw">table</span>(Факт =<span class="st"> </span>DFace$Class, Прогноз =<span class="st"> </span>pred.train))</code></pre></div>
<pre><code>##     Прогноз
## Факт 1 2
##    1 7 1
##    2 0 8</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Acc =<span class="st"> </span><span class="kw">mean</span>(DFace$Class ==<span class="st"> </span>pred.train)
<span class="kw">paste</span>(<span class="st">&quot;Точность=&quot;</span>, <span class="kw">round</span>(<span class="dv">100</span>*Acc, <span class="dv">2</span>), <span class="st">&quot;%&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)</code></pre></div>
<pre><code>## [1] &quot;Точность=93.75%&quot;</code></pre>

</div>
<div id="sec_5_2" class="section level2">
<h2><span class="header-section-number">5.2</span> Бинарные деревья решений</h2>
<p>Как было отмечено в разделе <a href="ch-4.html#sec_4_3">4.3</a>, использование иерархических древовидных структур (decision trees) является одним из наиболее универсальных и эффективных методов классификации и регрессии (Hunt et al., 1966; Breiman at al., 1984; Loh and Shih, 1997). Классификационные модели деревьев рекурсивно делят набор данных на подмножества, являющиеся все более и более гомогенными относительно определенных признаков. Создается решающее правило классификации иерархического типа и формируется ассоциативный логический ключ, дающий возможность выполнять распознавание объектов из новых выборок.</p>
<p>Самым простым, частным случаем деревьев решений являются деревья бинарной классификации, в узлах которых ветвление может вестись только в двух направлениях, т.е. осуществляется выбор из двух альтернатив (“да” и “нет”). Создание такой дихотомической классификационной модели осуществляется с использованием алгоритма ID3 (Interactive Dichotomizer).</p>
<p>В основе алгоритма (Quinlan, 1986) лежит циклическое разбиение обучающей выборки на классы в соответствии с переменной, имеющей наибольшую “классифицирующую силу”. Каждое подмножество наблюдений (объектов), выделяемое такой переменной, вновь разбивается на классы с использованием следующей переменной с наибольшей классифицирующей способностью и т.д. Разбиение заканчивается, когда в итоговом подмножестве оказываются объекты лишь одного класса. Пути движения по полученному дереву решений с верхнего уровня на самые нижние определяются логическими правилами в виде цепочек конъюнкций.</p>
<p>При выборе критерия, оценивающего классифицирующую силу, обычно используют теоретико-информационный подход. Мера информационного выигрыша IG (Information Gain) для разбиения A определяется как разность энтропии Шеннона <span class="math inline">\(H(S)\)</span> для исходного набора данных и суммы энтропий всех фрагментов данных после разбиения <span class="math inline">\(Н(S_{\nu})\)</span> с соответствующим весом в зависимости от размера каждой части:</p>
<p><span class="math display">\[IG(S, A) = H(S) - \sum_{\nu \in V}\frac{S_{\nu}}{S}H(S_{\nu}),\]</span></p>
<p>где <span class="math inline">\(\nu\)</span> выбранное опорное значение переменной <span class="math inline">\(V\)</span>, использованное для разбиения <span class="math inline">\(A\)</span>. Если разделение на классы выполнено оптимально, то каждое подмножество <span class="math inline">\(S_{\nu}\)</span> будет иметь небольшую энтропию, и, следовательно, значение <span class="math inline">\(IG\)</span> будет максимальным. На языке R этот критерий может быть рассчитан следующим образом:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Entropy &lt;-<span class="st"> </span>function(vls) {
    res &lt;-<span class="st"> </span>vls/<span class="kw">sum</span>(vls)*<span class="kw">log2</span>(vls/<span class="kw">sum</span>(vls))
    res[vls ==<span class="st"> </span><span class="dv">0</span>] &lt;-<span class="st"> </span><span class="dv">0</span>
    -<span class="kw">sum</span>(res)
}
InformationGain &lt;-<span class="st"> </span>function(tble) {
    entropyBefore &lt;-<span class="st"> </span><span class="kw">Entropy</span>(<span class="kw">colSums</span>(tble))
    s &lt;-<span class="st"> </span><span class="kw">rowSums</span>(tble)
    entropyAfter &lt;-<span class="st"> </span><span class="kw">sum</span>(s/<span class="kw">sum</span>(s) *
<span class="st">                            </span><span class="kw">apply</span>(tble, <span class="dt">MARGIN =</span> <span class="dv">1</span>, <span class="dt">FUN =</span> Entropy ))
    informationGain &lt;-<span class="st"> </span>entropyBefore -<span class="st"> </span>entropyAfter
    <span class="kw">return</span> (informationGain)
}</code></pre></div>
<p>Для реализации алгоритма ID3 используем компоненты пакета data.tree: объект класса tree (дерево) и метод AddChild , добавляющий к выстраиваемому дереву очередной дочерний узел (см. <a href="http://ipub.com/wp-content/uploads/2016/02/applications.html">http://ipub.com/wp-content</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">IsPure &lt;-<span class="st"> </span>function(data) {
    <span class="kw">length</span>(<span class="kw">unique</span>(data[, <span class="kw">ncol</span>(data)])) ==<span class="st"> </span><span class="dv">1</span>
}

TrainID3 &lt;-<span class="st"> </span>function(node, data) {
    node$obsCount &lt;-<span class="st"> </span><span class="kw">nrow</span>(data)
    <span class="co">#  если текущий набор данных принадлежит к одному классу, то</span>
    if (<span class="kw">IsPure</span>(data)) {
        <span class="co">#создается лист дерева с экземплярами этого класса</span>
        child &lt;-<span class="st"> </span>node$<span class="kw">AddChild</span>(<span class="kw">unique</span>(data[, <span class="kw">ncol</span>(data)]))
        node$feature &lt;-<span class="st"> </span><span class="kw">tail</span>(<span class="kw">names</span>(data), <span class="dv">1</span>)
        child$obsCount &lt;-<span class="st"> </span><span class="kw">nrow</span>(data)
        child$feature &lt;-<span class="st"> &#39;&#39;</span>
    } else {
        <span class="co"># рассчитывается вектор информационных выигрышей IG</span>
        ig &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="kw">colnames</span>(data)[-<span class="kw">ncol</span>(data)], 
                     function(x) <span class="kw">InformationGain</span>(
                         <span class="kw">table</span>(data[, x], data[, <span class="kw">ncol</span>(data)])))
        <span class="co">#  выбирается значение признака с наибольшей величиной IG</span>
        feature &lt;-<span class="st"> </span><span class="kw">names</span>(<span class="kw">which.max</span>(ig))
        node$feature &lt;-<span class="st"> </span>feature
        <span class="co"># создается подмножества данных на основе этого значения </span>
        childObs &lt;-<span class="st"> </span><span class="kw">split</span>(data[, <span class="kw">names</span>(data) !=<span class="st"> </span>feature, 
                               <span class="dt">drop =</span> <span class="ot">FALSE</span>], data[, feature], <span class="dt">drop =</span> <span class="ot">TRUE</span>)
        <span class="co">#  создаются дочерние узлы дерева c именем признака</span>
        for (i in <span class="dv">1</span>:<span class="kw">length</span>(childObs)) {
            child &lt;-<span class="st"> </span>node$<span class="kw">AddChild</span>(<span class="kw">names</span>(childObs)[i])
            <span class="co"># осуществляется рекурсия алгоритма для дочернего узла</span>
            <span class="kw">TrainID3</span>(child, childObs[[i]])
        }
    }
}</code></pre></div>
<p>Обратим внимание, что наша функция TrainID3 является рекурсивной, т.е. для выполнения итераций вызов функции осуществляется из тела ее самой. Функция завершает работу, если все ветви дерева завершаются листьями с объектами одного класса.</p>
<p>Для иллюстрации работы алгоритма ID3 обратимся к нашему примеру по классификации лиц избирателей (см. рис. <a href="ch-5.html#fig:fig-5-1">5.1</a> в разделе <a href="ch-5.html#sec_5_1">5.1</a>). Для получения осмысленной визуализации дерева перейдем от численных обозначений к натуральным названиям признаков и их градаций. В результате получаем следующее дерево решений:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">DFace &lt;-<span class="st"> </span><span class="kw">read.delim</span>(<span class="dt">file =</span> <span class="st">&quot;data/Faces.txt&quot;</span>, <span class="dt">header =</span> <span class="ot">TRUE</span>, <span class="dt">row.names =</span> <span class="dv">1</span>)
DFaceN &lt;-<span class="st"> </span>DFace[, -<span class="dv">17</span>]; Class &lt;-<span class="st"> </span>DFace$Class 
DFaceN[DFaceN ==<span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> &quot;Да&quot;</span> ; DFaceN[DFaceN ==<span class="st"> </span><span class="dv">0</span>] &lt;-<span class="st"> &quot;Нет&quot;</span>
Class[Class ==<span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> &quot;Патриот&quot;</span>
Class[Class ==<span class="st"> </span><span class="dv">2</span>] &lt;-<span class="st"> &quot;Демократ&quot;</span>
DFaceN &lt;-<span class="st"> </span><span class="kw">cbind</span>(DFaceN,Class) 

<span class="kw">colnames</span>(DFaceN) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;голова_круглая&quot;</span>, <span class="st">&quot;уши_оттопырен&quot;</span>,
                      <span class="st">&quot;нос_круглый&quot;</span>, <span class="st">&quot;глаза_круглые&quot;</span>, <span class="st">&quot;лоб_морщины&quot;</span>,
                      <span class="st">&quot;носогубн_складка&quot;</span>, <span class="st">&quot;губы_толстые&quot;</span>, <span class="st">&quot;волосы&quot;</span>, <span class="st">&quot;усы&quot;</span>,<span class="st">&quot;борода&quot;</span>,
                      <span class="st">&quot;очки&quot;</span>,<span class="st">&quot;родинка_щеке&quot;</span>, <span class="st">&quot;бабочка&quot;</span>, <span class="st">&quot;брови_подняты&quot;</span>, <span class="st">&quot;серьга&quot;</span>,
                      <span class="st">&quot;курит_трубка&quot;</span>, <span class="st">&quot;Группа&quot;</span>)
<span class="kw">library</span>(data.tree)
tree &lt;-<span class="st"> </span>Node$<span class="kw">new</span>(<span class="st">&quot;DFaceN&quot;</span>)  <span class="co"># Создаем «пустое» дерево</span>
<span class="kw">TrainID3</span>(tree, DFaceN)
<span class="kw">print</span>(tree, <span class="st">&quot;feature&quot;</span>, <span class="st">&quot;obsCount&quot;</span>)</code></pre></div>
<pre><code>##                       levelName       feature obsCount
## 1  DFaceN                       уши_оттопырен       16
## 2   ¦--Да                              борода       10
## 3   ¦   ¦--Да                     нос_круглый        7
## 4   ¦   ¦   ¦--Да                курит_трубка        3
## 5   ¦   ¦   ¦   ¦--Да                  Группа        1
## 6   ¦   ¦   ¦   ¦   °--Патриот                       1
## 7   ¦   ¦   ¦   °--Нет                 Группа        2
## 8   ¦   ¦   ¦       °--Демократ                      2
## 9   ¦   ¦   °--Нет               губы_толстые        4
## 10  ¦   ¦       ¦--Да                  Группа        1
## 11  ¦   ¦       ¦   °--Демократ                      1
## 12  ¦   ¦       °--Нет                 Группа        3
## 13  ¦   ¦           °--Патриот                       3
## 14  ¦   °--Нет                         Группа        3
## 15  ¦       °--Демократ                              3
## 16  °--Нет                      брови_подняты        6
## 17      ¦--Да                          Группа        2
## 18      ¦   °--Демократ                              2
## 19      °--Нет                         Группа        4
## 20          °--Патриот                               4</code></pre>
<p>Выращивание дерева начинается с переменной, имеющей наибольшее значение критерия IG, каковой оказались оттопыренные уши: ее значению “Да” соответствует группа из 10 объектов, а значению “Нет” - из 6. Если в первой ветви для классификации последовательно привлекаются еще 4 признака (наличие бороды, круглого носа, курительной трубки и толстых губ), то во втором случае безошибочное разбиение сразу происходит при использовании поднятых бровей. Проверим теперь, насколько эффективен полученный классификатор на обучающей выборке:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Predict &lt;-<span class="st"> </span>function(tree, features) {
    if (tree$children[[<span class="dv">1</span>]]$isLeaf) 
        <span class="kw">return</span>(tree$children[[<span class="dv">1</span>]]$name)
    child &lt;-<span class="st"> </span>tree$children[[features[[tree$feature]]]]
    <span class="kw">return</span>( <span class="kw">Predict</span>(child, features))
}
pred &lt;-<span class="st"> </span><span class="kw">apply</span>(DFaceN[, -<span class="dv">17</span>], <span class="dv">1</span>, function(x) <span class="kw">Predict</span>(tree, x))
<span class="kw">table</span>(Факт =<span class="st"> </span>DFaceN$Группа, Прогноз =<span class="st"> </span>pred)</code></pre></div>
<pre><code>##           Прогноз
## Факт       Демократ Патриот
##   Демократ        8       0
##   Патриот         0       8</code></pre>
<p>Предсказание электоральной группы по всей обучающей выборке, как и в случае логистической регрессии, также оказалось безошибочным. Однако нет уверенности, что построенные деревья будут столь же успешны при практической реализации на “свежих” данных. Для оценки эффективности модели выполним перекрестную проверку:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Nerr &lt;-<span class="st"> </span><span class="dv">0</span>
for (i in <span class="dv">1</span>:<span class="kw">nrow</span>(DFaceN)) {
    tree &lt;-<span class="st"> </span>Node$<span class="kw">new</span>(<span class="st">&quot;DFaceN&quot;</span>)
    <span class="kw">TrainID3</span>(tree, DFaceN[-i, ])
    Nerr =<span class="st"> </span>Nerr +<span class="st"> </span>
<span class="st">        </span><span class="kw">sum</span>(DFaceN[i,<span class="dv">17</span>] !=<span class="st"> </span><span class="kw">Predict</span>(tree,DFaceN[i, -<span class="dv">17</span>]))  }
(Nerr/<span class="kw">nrow</span>(DFaceN))</code></pre></div>
<pre><code>## [1] 0.25</code></pre>
<p>При выполнении скользящего контроля четыре объекта из 16 оказались неверно предсказанными с помощью бинарного дерева, построенного после исключения тестируемого примера. Считается (Дук, Самойленко, 2001), что алгоритм ID3 в случае взаимозависимых признаков нередко направляет ход логического вывода по ложному пути и создает лишь иллюзию правильного рассуждения. Но для более обоснованных выводов следует рассматривать результаты тестирования на более серьезных примерах, чем наш.</p>

</div>
<div id="sec_5_3" class="section level2">
<h2><span class="header-section-number">5.3</span> Поиск логических закономерностей в данных</h2>
<p>Основное требование к математическому аппарату Data Minning заключается не только в эффективности классификаторов, но и в интерпретируемости результатов. Методы поиска логических закономерностей в бинарных матрицах наблюдений (как и в некоторых классических методах многомерного анализа) апеллируют к информации, заключенной не в отдельных признаках, а в различных сочетаниях их значений. Для признаков, измеренных в альтернативных шкалах, их значения <span class="math inline">\(x_i = 0\)</span> или <span class="math inline">\(x_i = 1\)</span> рассматриваются как элементарные события, что позволяет сформулировать решающие правила на простом и понятном человеку языке логических высказываний, таких как <code>ЕСЛИ {(событие 1) и (событие 2) и ... и (событие N)} ТО ...</code>.</p>
<p>Так, классификация лиц в рассмотренном примере рис. <a href="ch-5.html#fig:fig-5-1">5.1</a> может быть произведена, например, с помощью логических высказываний 1 и 2:</p>
<ol style="list-style-type: decimal">
<li><code>ЕСЛИ {(голова овальная) и (есть носогубная складка) и (есть очки) и (есть трубка)} ИЛИ {(глаза круглые) и (лоб без морщин) и (есть борода) и (есть серьга)} ТО (&quot;Патриот&quot;)</code></li>
<li><code>ЕСЛИ {(нос круглый) и (лысый) и (есть усы) и (брови подняты кверху)} ИЛИ {(оттопыренные уши) и (толстые губы) и (нет родинки на щеке) и (есть бабочка)} ТО (&quot;Демократ&quot;)</code>.</li>
</ol>
<p>Математическая запись этих правил выглядит следующим образом: <span class="math inline">\([(x_1=0) \wedge (x_6=1) \wedge (x_{11}=1) \wedge (x_{16}=1)] \vee [(x_{4}=1) \wedge (x_{5}=0) \wedge (x_{10}=1) \wedge (x_{15}=1)] \Rightarrow \omega_1\)</span> <span class="math inline">\([(х_3=1) \wedge (х_8=0) \wedge (х_9=1) \wedge (х_{14}=1)] \vee [(х_2=1) \wedge (x_7=1) \wedge (х_{12}=0) \wedge (х_{13}=1)] \Rightarrow \omega_2\)</span>. Здесь значки <span class="math inline">\(\vee\)</span> - конъюнкция (“и”), <span class="math inline">\(\wedge\)</span> - дизъюнкция (“или”), <span class="math inline">\(\Rightarrow\)</span> - импликация (“если, то”).</p>
<p>Будем использовать логический метод распознавания, известный под названием алгоритм “Кора”, который в свое время широко использовался в геологоразведочных работах и скрининге лекарственных препаратов (Бонгард, 1967; Вайнцвайг, 1973). В детерминистической версии алгоритма обучающая выборка <span class="math inline">\(\boldsymbol{x}\)</span>, предварительно разделенная на два класса (1 и 2), многократно просматривается и из всего множества высказываний выделяются так называемые непротиворечивые логические высказывания <span class="math inline">\(\mathbf{\Psi}(\boldsymbol{x}, \tau)\)</span>, покрывающие все множество примеров. Непротиворечивым высказыванием для каждого класса считается конъюнкция, которая встречается с вероятностью не менее <span class="math inline">\(\tau\)</span> только в одном классе и ни разу не встречается в другом.</p>
<p>Представленный ниже скрипт выполняет перебор всех возможных комбинаций столбцов <span class="math inline">\(x_i\)</span>, <span class="math inline">\(i = 1, \dots, m = 16\)</span>, по 2, 3 и <code>maxKSize = 4</code> с использованием функции <code>combn()</code>. Для каждой комбинации столбцов перебираются все возможные варианты событий (<code>x_i = 0</code> или <code>x_i = 1</code>). Для сокращения объема вычислений подмножества исходной матрицы трансформировались в векторы символьных бинарных переменных, а комбинации значений <span class="math inline">\(x_i\)</span> выражались наборами “битовых масок” (например, <code>&quot;000&quot;</code>, <code>&quot;001&quot;</code>, <code>&quot;010&quot;</code>, <code>&quot;011&quot;</code>, <code>&quot;100&quot;</code>, <code>&quot;101&quot;</code>, <code>&quot;110&quot;</code>, <code>&quot;111&quot;</code>).</p>
<p>В ходе перебора конъюнкции, отобранные по совокупности условий, будем сохранять в трех глобальных объектах класса <code>list</code>, для чего используем оператор глобального присваивания <code>&lt;&lt;-</code>. Определим предварительно несколько функций:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Преобразование числа в набор битов (5 -&gt; &quot;0101&quot;)</span>
number2binchar &lt;-<span class="st"> </span>function(number, nBits) {
    <span class="kw">paste</span>(<span class="kw">tail</span>(<span class="kw">rev</span>(<span class="kw">as.numeric</span>(<span class="kw">intToBits</span>(number))), nBits),
          <span class="dt">collapse =</span> <span class="st">&quot;&quot;</span>)
}

<span class="co">#  Поиск конъюнкций по набору битовых масок </span>
MaskCompare &lt;-<span class="st"> </span>function(Nclass, KSize, BitMask,
                        vec_pos, vec_neg, ColCom) {
    nK &lt;-<span class="st"> </span><span class="kw">sapply</span>(BitMask, function(x) {
        if (<span class="kw">sum</span>(x ==<span class="st"> </span>vec_neg) &gt;<span class="st"> </span><span class="dv">0</span>) <span class="kw">return</span> (<span class="dv">0</span>)
        if (minNum &gt;<span class="st"> </span>(<span class="dt">countK =</span> <span class="kw">sum</span>(x ==<span class="st"> </span>vec_pos))) <span class="kw">return</span>(<span class="dv">0</span>)
        <span class="co">#  Cохранение конъюнкции в трех объектах list</span>
        Value.list[[<span class="kw">length</span>(Value.list) +<span class="st"> </span><span class="dv">1</span>]] &lt;&lt;-
<span class="st">            </span><span class="kw">list</span>(<span class="dt">Nclass =</span> Nclass, <span class="dt">KSize =</span> KSize, 
                 <span class="dt">countK =</span> countK, <span class="dt">Bits =</span> x)
        ColCom.list[[<span class="kw">length</span>(ColCom.list) +<span class="st"> </span><span class="dv">1</span>]] &lt;&lt;-<span class="st"> </span><span class="kw">list</span>(ColCom)
        RowList.list[[<span class="kw">length</span>(RowList.list) +<span class="st"> </span><span class="dv">1</span>]] &lt;&lt;-
<span class="st">            </span><span class="kw">list</span>(<span class="kw">which</span>(vec_pos %in%<span class="st"> </span>x))
        <span class="kw">return</span>(countK) } )
}</code></pre></div>
<p>Зададим минимальную частоту встречаемости конъюнкций <code>minNum = 4</code> (т.е. <span class="math inline">\(\tau\)</span> = 50%) и выполним формирование всех логических правил для рассматриваемого примера:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">DFace &lt;-<span class="st"> </span><span class="kw">read.delim</span>(<span class="dt">file =</span> <span class="st">&quot;data/Faces.txt&quot;</span>,
                    <span class="dt">header =</span> <span class="ot">TRUE</span>, <span class="dt">row.names =</span> <span class="dv">1</span>)
maxKSize &lt;-<span class="st"> </span><span class="dv">4</span>
minNum &lt;-<span class="st"> </span><span class="dv">4</span>

<span class="co">#  Списки для хранения результатов</span>
Value.list &lt;-<span class="st"> </span><span class="kw">list</span>()   <span class="co"># Nclass, KSize, BitMask, countK</span>
ColCom.list &lt;-<span class="st"> </span><span class="kw">list</span>()  <span class="co"># Наименования переменных ColCom</span>
RowList.list &lt;-<span class="st"> </span><span class="kw">list</span>()  <span class="co"># Номера индексов строк RowList</span>

<span class="co">#  Перебор конъюнкций разной длины</span>
for (KSize in <span class="dv">2</span>:maxKSize) {
    BitMask &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">0</span>:(<span class="dv">2</span>^KSize -<span class="st"> </span><span class="dv">1</span>),
                      function(x) <span class="kw">number2binchar</span>(x, KSize))
    cols &lt;-<span class="st">  </span><span class="kw">combn</span>(<span class="kw">colnames</span>(DFace[, -<span class="dv">17</span>]), KSize)
    
    for (i in <span class="dv">1</span>:<span class="kw">ncol</span>(cols))  {
        SubArr &lt;-<span class="st"> </span>DFace[, (<span class="kw">names</span>(DFace) %in%<span class="st"> </span>cols[, i])]
        vec1 &lt;-<span class="st"> </span><span class="kw">apply</span>(SubArr[DFace$Class ==<span class="st"> </span><span class="dv">1</span>, ],<span class="dv">1</span>,
                      function(x) <span class="kw">paste</span>(x, <span class="dt">collapse =</span> <span class="st">&quot;&quot;</span>))
        vec2 &lt;-<span class="st"> </span><span class="kw">apply</span>(SubArr[DFace$Class ==<span class="st"> </span><span class="dv">2</span>,], <span class="dv">1</span>,
                      function(x) <span class="kw">paste</span>(x, <span class="dt">collapse =</span> <span class="st">&quot;&quot;</span>))
        <span class="kw">MaskCompare</span>(<span class="dv">1</span>, KSize, BitMask, vec1, vec2, cols[, i])
        <span class="kw">MaskCompare</span>(<span class="dv">2</span>, KSize, BitMask, vec2, vec1, cols[, i])
    }
}
<span class="co">#  Создание результирующей таблицы</span>
DFval =<span class="st"> </span><span class="kw">do.call</span>(rbind.data.frame, Value.list)
nrow =<span class="st"> </span><span class="kw">length</span>(Value.list)
DFvar &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">ncol =</span> maxKSize +<span class="st"> </span><span class="dv">1</span>, <span class="dt">nrow =</span> nrow,
                              <span class="dt">dimnames =</span> <span class="kw">list</span>(<span class="dv">1</span>:nrow, <span class="kw">c</span>(
                                  <span class="kw">paste</span>(<span class="st">&quot;L&quot;</span>, <span class="dv">1</span>:maxKSize, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>),
                                  <span class="st">&quot;Объекты:&quot;</span>))))
for (i in <span class="dv">1</span>:nrow) {
    Varl &lt;-<span class="st"> </span><span class="kw">unlist</span>(ColCom.list[[i]])
    DFvar[i, <span class="dv">1</span>:<span class="kw">length</span>( Varl)] &lt;-<span class="st"> </span>Varl
    Objl &lt;-<span class="st"> </span><span class="kw">unlist</span>(RowList.list[[i]])
    DFvar[i, maxKSize +<span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">paste</span>(Objl, <span class="dt">collapse =</span> <span class="st">&quot; &quot;</span>)
}

DFvar[<span class="kw">is.na</span>(DFvar)] &lt;-<span class="st"> &quot; &quot;</span>
DFout &lt;-<span class="st"> </span><span class="kw">cbind</span>(DFval, DFvar)

<span class="co">#  Вывод результатов</span>
<span class="kw">print</span>(<span class="st">&quot;Конъюнкции класса 1&quot;</span>) </code></pre></div>
<pre><code>## [1] &quot;Конъюнкции класса 1&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">DFout[DFout$Nclass ==<span class="st"> </span><span class="dv">1</span>, ]</code></pre></div>
<pre><code>##    Nclass KSize countK Bits L1  L2  L3  L4 Объекты:
## 1       1     2      4   00 x2 x14          2 3 6 8
## 2       1     2      4   00 x3  x7          1 3 5 7
## 7       1     3      4  010 x2 x11 x14      2 3 6 8
## 10      1     3      4  111 x4 x10 x15      2 5 7 8
## 12      1     3      4  111 x6  x9 x11      1 3 6 8
## 13      1     3      4  111 x6 x11 x16      1 3 4 6
## 14      1     3      4  111 x9 x11 x13      1 3 6 8
## 16      1     4      4 0111 x1  x6  x9 x11  1 3 6 8
## 17      1     4      4 0111 x1  x6 x11 x16  1 3 4 6
## 18      1     4      4 0111 x1  x9 x11 x13  1 3 6 8
## 23      1     4      4 1011 x4  x5 x10 x15  2 5 7 8
## 25      1     4      4 1111 x6  x9 x11 x13  1 3 6 8
## 26      1     4      4 0101 x8  x9 x12 x13  1 6 7 8</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="st">&quot;Конъюнкции класса 2&quot;</span>)</code></pre></div>
<pre><code>## [1] &quot;Конъюнкции класса 2&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">DFout[DFout$Nclass ==<span class="st"> </span><span class="dv">2</span>, ] </code></pre></div>
<pre><code>##    Nclass KSize countK Bits  L1  L2  L3  L4 Объекты:
## 3       2     2      4   00  x6 x10          4 5 6 8
## 4       2     2      4   00 x11 x15          1 3 5 7
## 5       2     3      4  111  x2  x4  x7      5 6 7 8
## 6       2     3      4  111  x2  x7 x13      2 5 7 8
## 8       2     3      4  111  x3  x9 x14      1 3 4 6
## 9       2     3      4  111  x4  x7 x16      5 6 7 8
## 11      2     3      4  010  x6  x7 x10      4 5 6 8
## 15      2     4      4 0101  x1  x4  x5 x16  1 6 7 8
## 19      2     4      4 1110  x2  x4  x7 x12  5 6 7 8
## 20      2     4      4 1111  x2  x4  x7 x16  5 6 7 8
## 21      2     4      4 1101  x2  x7 x12 x13  2 5 7 8
## 22      2     4      4 1011  x3  x8  x9 x14  1 3 4 6
## 24      2     4      4 1101  x4  x7 x12 x16  5 6 7 8</code></pre>
<p>Результат, содержащий логические высказывания для каждого класса (<code>Nclass</code>) будет выглядеть следующим образом, где <code>KSize</code> - длина коньюнкции, <code>Bits</code> - ее битовая маска, <code>L1</code>-<code>L4</code> - наименования исходных переменных, <code>countK</code> - встречаемость конъюнкции на объектах своего класса.</p>
<p>Дальнейшая работа с выделенными логическими высказываниями сводится к следующему (реализация алгоритма в R находится в стадии доработки). Из генерируемого списка необходимо исключить подчиненные (или дочерние) конъюнкции, включающие более короткие претенденты: например, для класса 1 высказывание №7 является дочерним по отношению к конъюнкции №1 и должно было быть удалено. После такой операции поглощения ликвидируется избыточность решающего правила, в котором остаются конъюнкции минимального ранга, содержащие выделенные закономерности в концентрированной форме.</p>
<p>Хорошим правилом было бы также исключить высказывания, которые по определенным критериям считаются “предрассудками”. К ним относятся конъюнкции, не связанные с объективным правилом классификации, но, в силу ограниченности выборки, получившие хорошие оценки на обучении. Для выделения признаков, склонных к предрассудкам, выполняют бутстреп-процедуру, в которой обучающая выборка многократно случайным образом разбивается на классы. Разбитая таким образом выборка является тестом, позволяющим присвоить каждому исходному признаку штрафные баллы за склонность к предрассудкам. Конъюнкции, набравшие сверхнормативное количество штрафных очков, из решающего правила исключаются.</p>
<p>Описанием каждого класса является логическая сумма (дизъюнкция) некоторого количества непротиворечивых и продуктивных конъюнкций, прошедших описанные выше этапы отбора. Комбинация этих логических высказываний представляет собой своеобразную мозаично-фрагментарную разделяющую поверхность специального типа (в отличии, например, от линейной плоскости логита). Более общая версия алгоритма “Кора” предполагает выделение логических закономерностей для <span class="math inline">\(K\)</span> классов, <span class="math inline">\(K &gt; 2\)</span>. Среди конъюнкций выделяются те, которые верны на обучающей выборке чаще, чем некоторый порог 1, для одного из классов и не характерны для любого другого (верны реже, чем в доле случаев <span class="math inline">\(\tau_2\)</span>).</p>
<p>Существует возможность использовать сгенерированные конъюнкции для экзамена тестируемых примеров по принципу голосования. Чтобы классифицировать новое наблюдение <span class="math inline">\(\boldsymbol{х}\)</span>, подсчитывается число отобранных конъюнкций <span class="math inline">\(L_k\)</span>, характерных для каждого <span class="math inline">\(k\)</span>-гo класса, которые верны для тестируемого бинарного вектора. Если <span class="math inline">\(L_k\)</span> является максимальным из всех, то принимается решение о принадлежности объекта <span class="math inline">\(k\)</span>-му классу.</p>
<p>Алгоритм “Кора”, как и другие логические методы распознавания образов, является достаточно трудоемким, поскольку при отборе конъюнкций необходим полный или частично направленный перебор. Здесь может быть полезно использование процедур эволюционного поиска: генетический алгоритм (genetic algorithm), случайный поиск с адаптацией (СПА - см. книгу Г. С. Лбова, 1981 на сайте <a href="http://math.nsc.ru/AP/oteks/Russian/links/SPA/index.html">math.nsc.ru</a>) и др.</p>

</div>
<div id="sec_5_4" class="section level2">
<h2><span class="header-section-number">5.4</span> Алгоритмы выделения ассоциативных правил</h2>
<p>Ассоциативные правила представляют собой механизм нахождения логических закономерностей между связанными элементами (событиями или объектами). Пусть имеется <span class="math inline">\(\mathbf{A} = \{a_1, a_2, a_3, \dots, a_n\}\)</span> - конечное множество уникальных элементов (list of items). Из этих компонентов может быть составлено множество наборов <span class="math inline">\(\mathbf{T}\)</span> (sets of items), т.е. <span class="math inline">\(\mathbf{T} \subseteq \mathbf{A}\)</span>.</p>
<p>Ассоциативные правила <span class="math inline">\(\mathcal{A} \rightarrow \mathcal{T}\)</span> имеют следующий вид: <code>если &lt;условие&gt; то &lt;результат&gt;</code>, где в отличие от деревьев классификации, <code>&lt;условие&gt;</code> - не логическое выражение, а набор объектов из множества <span class="math inline">\(\mathbf{A}\)</span>, с которыми связаны (ассоциированы) объекты того же множества, включенные в <code>&lt;результат&gt;</code> данного правила. Например, ассоциативное правило <code>если (смородина, тля) то (муравьи)</code> означает, что если на кусте смородины встретилась тля, то ищи поблизости и муравьев.</p>
<p>Понятие “вид элемента <span class="math inline">\(a_k\)</span>” легко может быть обобщено на ту или иную его категорию или вещественное значение, т.е. концепция ассоциативного анализа может быть применена для комбинаций любых переменных. Например, при прогнозировании погоды одно из ассоциативных правил может выглядеть так: <code>'направление ветра' = NNW -&gt; 'завтра будет дождь' = TRUE</code></p>
<p>Выделяют три вида правил:</p>
<ul>
<li><em>полезные правила</em>, содержащие действительную информацию, которая ранее была неизвестна, но имеет логическое объяснение;</li>
<li><em>тривиальные правила</em>, содержащие действительную и легко объяснимую информацию, отражающую известные законы в исследуемой области, и поэтому не приносящие какой-либо пользы;</li>
<li><em>непонятные правила</em>, содержащие информацию, которая не может быть объяснена (такие правила или получают на основе аномальных исходных данных, или они содержат глубоко скрытые закономерности, и поэтому для интерпретации непонятных правил нужен дополнительный анализ).</li>
</ul>
<p>Поиск ассоциативных правил обычно выполняют в два этапа:</p>
<ul>
<li>в пуле имеющихся признаков <span class="math inline">\(\mathbf{A}\)</span> находят наиболее часто встречающиеся комбинации элементов <span class="math inline">\(\mathbf{T}\)</span>;</li>
<li>из этих найденных наиболее часто встречающихся наборов формируют ассоциативные правила.</li>
</ul>
<p>Для оценки полезности и продуктивности перебираемых правил используются различные частотные критерии, анализирующие встречаемость кандидата в массиве экспериментальных данных. Важнейшими из них являются поддержка (support) и достоверность (confidence). Правило <span class="math inline">\(\mathcal{A} \rightarrow \mathcal{T}\)</span> имеет поддержку <span class="math inline">\(s\)</span>, если оно справедливо для <span class="math inline">\(s\%\)</span> взятых в анализ случаев:</p>
<p><span class="math display">\[\text{support}(\mathcal{A} \rightarrow \mathcal{T}) = P(\mathcal{A} \cup \mathcal{T})\]</span></p>
<p>Достоверность правила показывает, какова вероятность того, что из наличия в рассматриваемом случае условной части правила следует наличие заключительной его части (т.е. из <span class="math inline">\(\mathcal{A}\)</span> следует <span class="math inline">\(\mathcal{T}\)</span>):</p>
<p><span class="math display">\[\text{confidence}(\mathcal{A} \rightarrow \mathcal{T}) = P(\mathcal{A} \cup \mathcal{T})/P(\mathcal{A}) = \text{support}(\mathcal{A} \rightarrow \mathcal{T})/\text{support}(\mathcal{A}).\]</span></p>
<p>Алгоритмы поиска ассоциативных правил отбирают тех кандидатов, у которых поддержка и достоверность выше некоторых наперед заданных порогов: <code>minsupport</code> и <code>minconfidence</code>. Если поддержка имеет большое значение, то алгоритмы будут находить правила, хорошо известные аналитику или настолько очевидные, что нет никакого смысла проводить такой анализ. Большинство интересных правил находят именно при низком значении порога поддержки. С другой стороны, низкое значение <code>minsupport</code> ведет к генерации огромного количества вариантов, что требует существенных вычислительных ресурсов или ведет к генерации статистически необоснованных правил.</p>
<p>В пакете <code>arules</code> для R используются и другие показатели - подъемная сила, или лифт (lift), которая показывает, насколько повышается вероятность нахождения <span class="math inline">\(\mathcal{T}\)</span> в анализируемом случае, если в нем уже имеется <span class="math inline">\(\mathcal{A}\)</span>:</p>
<p><span class="math display">\[\text{lift}(\mathcal{A} \rightarrow \mathcal{T}) = \text{confidence}(\mathcal{A} \rightarrow \mathcal{T}) / \text{support}(\mathcal{T})\]</span></p>
<p>и усиление (leverage), которое отражает, насколько интересной может быть более высокая частота <span class="math inline">\(\mathcal{A}\)</span> и <span class="math inline">\(\mathcal{T}\)</span> в сочетании с более низким подъемом:</p>
<p><span class="math display">\[\text{leverage}(\mathcal{A} \rightarrow \mathcal{T}) = \text{support}(\mathcal{A} \rightarrow \mathcal{T}) - \text{support}(\mathcal{A})\times\text{support}(\mathcal{T})\]</span></p>
<p>Первый алгоритм поиска ассоциативных правил был разработан в 1993 г. сотрудниками исследовательского центра IBM, что сразу возбудило интерес к этому направлению. Каждый год появлялось несколько новых алгоритмов (DHP, Partition, DIC и др.), из которых наиболее известным остался алгоритм “Apriori” (Agrawal, Srikant, 1994).</p>
<p>Пакет arules позволяет находить часто встречающиеся сочетания элементов в данных (frequent itemsets) и отбирать ассоциативные правила, обеспечивая интерфейс к модулям на языке C, которые реализуют алгоритмы “Apriori” и “Eclat”. Так как обычно обрабатываются большие множества наборов и правил, то для уменьшения объёмов требуемой памяти пакет содержит развитый инструментарий преобразования разреженных входных матриц в компактные наборы транзакций (Hahsler et al., 2016; Огнева, 2012).</p>
<p>Для реализации работы с алгоритмами выделения ассоциаций в <code>arules</code> реализованы специальные типы данных, относящиеся к объектам трех классов: входной массив транзакций (transactions) и на выходе - часто встречающиеся фрагменты данных (itemsets) и правила (rules).</p>
<p>Объекты класса <code>transactions</code> представляют собой специально организованные бинарные матрицы со строками-наборами и столбцами-признаками, содержащие значения элемента 1, если соответствующий признак есть в транзакции, и 0, если он отсутствует. В зависимости от типа данных и способа их загрузки, эти объекты могут иметь разные способы организации и состав дополнительных слотов. В частности, подкласс <code>itemMatrix</code> является одновременно средством представления разреженных матриц с использованием функционала пакета <code>Matrix</code>. Другим способом формирования экземпляров класса transactions является загрузка данных из файла функцией <code>read.transactions()</code>.</p>
<p>Для выделения ассоциативных правил вновь обратимся к нашему примеру по классификации лиц избирателей (см. рис. <a href="ch-5.html#fig:fig-5-1">5.1</a>). Метод <code>&quot;basket&quot;</code> функции <code>read.transactions()</code> предполагает, что каждая строка в файле представляет собой одну транзакцию, в которой признаки (т.е. их метки) разделены символами <code>sep</code> (по умолчанию - запятая). Переформируем исходную таблицу данных в файл необходимого формата:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">DFace &lt;-<span class="st"> </span><span class="kw">read.delim</span>(<span class="dt">file =</span> <span class="st">&quot;data/Faces.txt&quot;</span>, 
                    <span class="dt">header =</span> <span class="ot">TRUE</span>, <span class="dt">row.names =</span> <span class="dv">1</span>)
Class &lt;-<span class="st"> </span>DFace$Class
DFaceN &lt;-<span class="st"> </span>DFace[, -<span class="dv">17</span>]
<span class="kw">colnames</span>(DFaceN) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;голова_круглая&quot;</span>, <span class="st">&quot;уши_оттопырен&quot;</span>,
                      <span class="st">&quot;нос_круглый&quot;</span>, <span class="st">&quot;глаза_круглые&quot;</span>, <span class="st">&quot;лоб_морщины&quot;</span>,
                      <span class="st">&quot;носогубн_складка&quot;</span>, <span class="st">&quot;губы_толстые&quot;</span>, <span class="st">&quot;волосы&quot;</span>, 
                      <span class="st">&quot;усы&quot;</span>, <span class="st">&quot;борода&quot;</span>, <span class="st">&quot;очки&quot;</span>, <span class="st">&quot;родинка_щеке&quot;</span>, <span class="st">&quot;бабочка&quot;</span>,
                      <span class="st">&quot;брови_подняты&quot;</span>, <span class="st">&quot;серьга&quot;</span>, <span class="st">&quot;курит_трубка&quot;</span>)
Class[Class ==<span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> &quot;Патриот&quot;</span>
Class[Class ==<span class="st"> </span><span class="dv">2</span>] &lt;-<span class="st"> &quot;Демократ&quot;</span>
items_list &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span>:<span class="kw">nrow</span>(DFaceN), function(i) 
    <span class="kw">paste</span>(<span class="kw">c</span>(Class[i], <span class="kw">colnames</span>(DFaceN[i, DFaceN[i, ] ==<span class="st"> </span><span class="dv">1</span>])),
          <span class="dt">collapse =</span> <span class="st">&quot;,&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>))
<span class="kw">head</span>(items_list)</code></pre></div>
<pre><code>## [1] &quot;Патриот,уши_оттопырен,лоб_морщины,носогубн_складка,усы,борода,очки,бабочка,брови_подняты,курит_трубка&quot;              
## [2] &quot;Патриот,голова_круглая,нос_круглый,глаза_круглые,губы_толстые,волосы,борода,очки,родинка_щеке,серьга&quot;               
## [3] &quot;Патриот,глаза_круглые,лоб_морщины,носогубн_складка,волосы,усы,очки,родинка_щеке,бабочка,курит_трубка&quot;               
## [4] &quot;Патриот,уши_оттопырен,нос_круглый,носогубн_складка,губы_толстые,борода,очки,брови_подняты,серьга,курит_трубка&quot;      
## [5] &quot;Патриот,голова_круглая,уши_оттопырен,глаза_круглые,носогубн_складка,волосы,борода,родинка_щеке,брови_подняты,серьга&quot;
## [6] &quot;Патриот,нос_круглый,лоб_морщины,носогубн_складка,губы_толстые,усы,очки,бабочка,серьга,курит_трубка&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">write</span>(items_list, <span class="dt">file =</span> <span class="st">&quot;data/face_basket.txt&quot;</span>)</code></pre></div>
<p>Загрузим данные из файла и создадим объект <code>itemMatrix</code>. Информацию о сформированном массиве транзакций можно получить, выполнив команды <code>inspect()</code> и <code>summary()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(arules)
trans =<span class="st"> </span><span class="kw">read.transactions</span>(<span class="st">&quot;data/face_basket.txt&quot;</span>, 
                          <span class="dt">format =</span> <span class="st">&quot;basket&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;,&quot;</span>)
<span class="kw">inspect</span>(trans)   <span class="co">#  Выводимые данные не показаны</span></code></pre></div>
<pre><code>##      items             
## [1]  {бабочка,         
##       брови_подняты,   
##       борода,          
##       лоб_морщины,     
##       курит_трубка,    
##       очки,            
##       Патриот,         
##       носогубн_складка,
##       усы,             
##       уши_оттопырен}   
## [2]  {глаза_круглые,   
##       борода,          
##       голова_круглая,  
##       волосы,          
##       губы_толстые,    
##       родинка_щеке,    
##       очки,            
##       Патриот,         
##       нос_круглый,     
##       серьга}          
## [3]  {бабочка,         
##       глаза_круглые,   
##       волосы,          
##       родинка_щеке,    
##       лоб_морщины,     
##       курит_трубка,    
##       очки,            
##       Патриот,         
##       носогубн_складка,
##       усы}             
## [4]  {брови_подняты,   
##       борода,          
##       губы_толстые,    
##       курит_трубка,    
##       очки,            
##       Патриот,         
##       нос_круглый,     
##       носогубн_складка,
##       серьга,          
##       уши_оттопырен}   
## [5]  {брови_подняты,   
##       глаза_круглые,   
##       борода,          
##       голова_круглая,  
##       волосы,          
##       родинка_щеке,    
##       Патриот,         
##       носогубн_складка,
##       серьга,          
##       уши_оттопырен}   
## [6]  {бабочка,         
##       губы_толстые,    
##       лоб_морщины,     
##       курит_трубка,    
##       очки,            
##       Патриот,         
##       нос_круглый,     
##       носогубн_складка,
##       серьга,          
##       усы}             
## [7]  {бабочка,         
##       брови_подняты,   
##       глаза_круглые,   
##       борода,          
##       голова_круглая,  
##       курит_трубка,    
##       Патриот,         
##       серьга,          
##       усы,             
##       уши_оттопырен}   
## [8]  {бабочка,         
##       глаза_круглые,   
##       борода,          
##       губы_толстые,    
##       очки,            
##       Патриот,         
##       нос_круглый,     
##       носогубн_складка,
##       серьга,          
##       усы}             
## [9]  {бабочка,         
##       Демократ,        
##       брови_подняты,   
##       глаза_круглые,   
##       борода,          
##       родинка_щеке,    
##       курит_трубка,    
##       нос_круглый,     
##       носогубн_складка,
##       усы}             
## [10] {бабочка,         
##       Демократ,        
##       брови_подняты,   
##       борода,          
##       губы_толстые,    
##       очки,            
##       нос_круглый,     
##       носогубн_складка,
##       серьга,          
##       уши_оттопырен}   
## [11] {Демократ,        
##       брови_подняты,   
##       борода,          
##       голова_круглая,  
##       родинка_щеке,    
##       лоб_морщины,     
##       нос_круглый,     
##       носогубн_складка,
##       усы,             
##       уши_оттопырен}   
## [12] {Демократ,        
##       брови_подняты,   
##       голова_круглая,  
##       губы_толстые,    
##       родинка_щеке,    
##       лоб_морщины,     
##       очки,            
##       нос_круглый,     
##       серьга,          
##       усы}             
## [13] {бабочка,         
##       Демократ,        
##       глаза_круглые,   
##       голова_круглая,  
##       волосы,          
##       губы_толстые,    
##       лоб_морщины,     
##       курит_трубка,    
##       усы,             
##       уши_оттопырен}   
## [14] {Демократ,        
##       брови_подняты,   
##       глаза_круглые,   
##       губы_толстые,    
##       курит_трубка,    
##       очки,            
##       нос_круглый,     
##       серьга,          
##       усы,             
##       уши_оттопырен}   
## [15] {бабочка,         
##       Демократ,        
##       брови_подняты,   
##       глаза_круглые,   
##       борода,          
##       волосы,          
##       губы_толстые,    
##       курит_трубка,    
##       носогубн_складка,
##       уши_оттопырен}   
## [16] {бабочка,         
##       Демократ,        
##       глаза_круглые,   
##       волосы,          
##       губы_толстые,    
##       курит_трубка,    
##       очки,            
##       нос_круглый,     
##       серьга,          
##       уши_оттопырен}</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(trans)   <span class="co">#  Выводимые данные показаны частично</span></code></pre></div>
<pre><code>## transactions as itemMatrix in sparse format with
##  16 rows (elements/itemsets/transactions) and
##  18 columns (items) and a density of 0.5555556 
## 
## most frequent items:
##       бабочка брови_подняты глаза_круглые        борода  губы_толстые 
##            10            10            10            10            10 
##       (Other) 
##           110 
## 
## element (itemset/transaction) length distribution:
## sizes
## 10 
## 16 
## 
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##      10      10      10      10      10      10 
## 
## includes extended item information - examples:
##          labels
## 1       бабочка
## 2      Демократ
## 3 брови_подняты</code></pre>
<p>Для объектов класса <code>itemMatrix</code> можно осуществить быструю визуализацию данных или построить график распределения частот встречаемости признаков (рис. <a href="ch-5.html#fig:fig-5-4">5.4</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">image</span>(trans)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-5-4"></span>
<img src="_main_files/figure-html/fig-5-4-1.png" alt="Матрица признаки/транзакции и частотное распределение встречаемости признаков в транзакциях" width="576" />
<p class="caption">
Рисунок 5.4: Матрица признаки/транзакции и частотное распределение встречаемости признаков в транзакциях
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">itemFrequencyPlot</span>(trans, <span class="dt">support =</span> <span class="fl">0.1</span>, <span class="dt">cex.names =</span> <span class="fl">0.8</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-5-4"></span>
<img src="_main_files/figure-html/fig-5-4-2.png" alt="Матрица признаки/транзакции и частотное распределение встречаемости признаков в транзакциях" width="576" />
<p class="caption">
Рисунок 5.4: Матрица признаки/транзакции и частотное распределение встречаемости признаков в транзакциях
</p>
</div>
<p>Поиск ассоциативных правил является не вполне тривиальной задачей, т.к. с ростом числа элементов в <span class="math inline">\(\mathbf{А}\)</span> экспоненциально растет число их потенциальных комбинаций. Алгоритм “Apriori” является итерационным, при этом сначала выполняются действия для одноэлементных наборов, затем для 2-х, 3-х элементных и т.д. (т.е. он во многом напоминает алгоритм “Кора”).</p>
<p>На первом шаге первой итерации алгоритма подсчитываются одноэлементные часто встречающиеся наборы. Для этого необходимо пройтись по всему массиву данных и подсчитать для них поддержку, т.е. сколько раз набор встречается в имеющемся наборе данных. При последующем поиске <span class="math inline">\(k\)</span>-элементных наборов генерация претендентов состоит из двух фаз - формирование кандидатов нового уровня на основе <span class="math inline">\((k - 1)\)</span>-элементных наборов, которые были определены на предыдущей итерации алгоритма, и удаление избыточных кандидатов. После того как найдены все часто встречающиеся наборы элементов, выполняют процедуру непосредственного извлечения правил из построенного хеш-дерева (Зайцев, 2009).</p>
<p>Результатом анализа транзакций с помощью пакета arules являются объекты класса associations, включающие описания множества отношений между признаками (в виде часто встречающихся фрагментов, или правил), которые отбираются в соответствии с различными перечисленными выше мерами качества. Подкласс <code>rules</code> состоит из двух объектов <code>itemMatrix</code>, представляющих левую <code>lhs</code> (left-hand-side) и правую <code>rhs</code> (right-hand-side) сторону правила <span class="math inline">\(\mathcal{A} \rightarrow \mathcal{T}\)</span>, т.е. <span class="math inline">\(\mathcal{A}\)</span> - <code>lhs</code>, <span class="math inline">\(\mathcal{T}\)</span> - <code>rhs</code>.</p>
<p>Формирование правил осуществляется функцией <code>apriori()</code> с указанием пороговых значений поддержки и достоверности. Функция <code>summary()</code> обеспечивает частотный анализ правил по их длине и достигнутым мерам качества:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rules &lt;-<span class="st"> </span><span class="kw">apriori</span>(trans,
                 <span class="dt">parameter =</span> <span class="kw">list</span>(<span class="dt">support =</span> <span class="fl">0.01</span>, <span class="dt">confidence =</span> <span class="fl">0.6</span>))</code></pre></div>
<pre><code>## Apriori
## 
## Parameter specification:
##  confidence minval smax arem  aval originalSupport maxtime support minlen
##         0.6    0.1    1 none FALSE            TRUE       5    0.01      1
##  maxlen target   ext
##      10  rules FALSE
## 
## Algorithmic control:
##  filter tree heap memopt load sort verbose
##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE
## 
## Absolute minimum support count: 0 
## 
## set item appearances ...[0 item(s)] done [0.00s].
## set transactions ...[18 item(s), 16 transaction(s)] done [0.00s].
## sorting and recoding items ... [18 item(s)] done [0.00s].
## creating transaction tree ... done [0.00s].
## checking subsets of size 1 2 3 4 5 6 7 8 9 10 done [0.00s].
## writing ... [48306 rule(s)] done [0.04s].
## creating S4 object  ... done [0.03s].</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(rules)  <span class="co">#  Результаты показаны частично</span></code></pre></div>
<pre><code>## set of 48306 rules
## 
## rule length distribution (lhs + rhs):sizes
##     1     2     3     4     5     6     7     8     9    10 
##    12   138   916  3892  9860 14560 11814  5526  1428   160 
## 
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1.00    5.00    6.00    6.14    7.00   10.00 
## 
## summary of quality measures:
##     support          confidence          lift      
##  Min.   :0.06250   Min.   :0.6000   Min.   :0.960  
##  1st Qu.:0.06250   1st Qu.:1.0000   1st Qu.:1.600  
##  Median :0.06250   Median :1.0000   Median :1.600  
##  Mean   :0.07788   Mean   :0.9781   Mean   :1.750  
##  3rd Qu.:0.06250   3rd Qu.:1.0000   3rd Qu.:1.600  
##  Max.   :0.62500   Max.   :1.0000   Max.   :2.667  
## 
## mining info:
##   data ntransactions support confidence
##  trans            16    0.01        0.6</code></pre>
<p>Всего было отобрано 48306 правил, длина которых большей частью составляла от 5 до 7 элементов. Функция <code>plot()</code> из пакета <code>arulesViz</code> позволяет получать различные формы визуализации синтезированных правил, в том числе, анализ изменчивости их мер качества (рис. <a href="ch-5.html#fig:fig-5-5">5.5</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(arulesViz)
<span class="kw">plot</span>(rules, <span class="dt">measure =</span> <span class="kw">c</span>(<span class="st">&quot;support&quot;</span>, <span class="st">&quot;lift&quot;</span>), <span class="dt">shading =</span> <span class="st">&quot;confidence&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-5-5"></span>
<img src="_main_files/figure-html/fig-5-5-1.png" alt="Поддержка, лифт и достоверность сгенерированных правил" width="576" />
<p class="caption">
Рисунок 5.5: Поддержка, лифт и достоверность сгенерированных правил
</p>
</div>
<p>Для решения задачи выявления характерных особенностей групп электората нас интересуют, в первую очередь, высококачественные правила, имеющие соответствующий признак группы в правой части. Тогда патриотов можно будет легко узнать по их облику (рис. <a href="ch-5.html#fig:fig-5-6">5.6</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rulesPat &lt;-<span class="st"> </span><span class="kw">subset</span>(rules, 
                   <span class="dt">subset =</span> rhs %in%<span class="st"> &quot;Патриот&quot;</span> &amp;<span class="st"> </span>lift &gt;<span class="st"> </span><span class="fl">1.8</span>)
<span class="kw">inspect</span>(<span class="kw">head</span>(rulesPat, <span class="dt">n =</span> <span class="dv">10</span>, <span class="dt">by =</span> <span class="st">&quot;support&quot;</span>))</code></pre></div>
<pre><code>##      lhs                   rhs       support confidence lift
## [1]  {глаза_круглые,                                        
##       борода,                                               
##       серьга}           =&gt; {Патриот}  0.2500          1    2
## [2]  {очки,                                                 
##       носогубн_складка,                                     
##       усы}              =&gt; {Патриот}  0.2500          1    2
## [3]  {бабочка,                                              
##       очки,                                                 
##       усы}              =&gt; {Патриот}  0.2500          1    2
## [4]  {курит_трубка,                                         
##       очки,                                                 
##       носогубн_складка} =&gt; {Патриот}  0.2500          1    2
## [5]  {бабочка,                                              
##       очки,                                                 
##       носогубн_складка,                                     
##       усы}              =&gt; {Патриот}  0.2500          1    2
## [6]  {волосы,                                               
##       родинка_щеке}     =&gt; {Патриот}  0.1875          1    2
## [7]  {лоб_морщины,                                          
##       очки,                                                 
##       носогубн_складка} =&gt; {Патриот}  0.1875          1    2
## [8]  {лоб_морщины,                                          
##       курит_трубка,                                         
##       носогубн_складка} =&gt; {Патриот}  0.1875          1    2
## [9]  {бабочка,                                              
##       лоб_морщины,                                          
##       носогубн_складка} =&gt; {Патриот}  0.1875          1    2
## [10] {лоб_морщины,                                          
##       курит_трубка,                                         
##       очки}             =&gt; {Патриот}  0.1875          1    2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">head</span>(<span class="kw">sort</span>(rulesPat, <span class="dt">by =</span> <span class="st">&quot;support&quot;</span>), <span class="dv">10</span>),
     <span class="dt">method =</span> <span class="st">&quot;paracoord&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-5-6"></span>
<img src="_main_files/figure-html/fig-5-6-1.png" alt="График 10 лучших правил для патриотов в параллельных координатах" width="576" />
<p class="caption">
Рисунок 5.6: График 10 лучших правил для патриотов в параллельных координатах
</p>
</div>
<p>График в параллельных координатах (<code>method=&quot;paracoord&quot;</code>) на рис. <a href="ch-5.html#fig:fig-5-6">5.6</a> показывает, как формируются комбинации признаков правой части при увеличении ее размера, а толщина линий соответствует уровню поддержки.</p>
<p>Аналогичные правила могут быть отобраны для группы демократично настроенных избирателей (рис. <a href="ch-5.html#fig:fig-5-7">5.7</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rulesDem &lt;-<span class="st"> </span><span class="kw">subset</span>(rules, <span class="dt">subset =</span> rhs %in%<span class="st"> &quot;Демократ&quot;</span> &amp;<span class="st"> </span>lift &gt;<span class="st"> </span><span class="fl">1.8</span>)
<span class="kw">inspect</span>(<span class="kw">head</span>(rulesDem, <span class="dt">n =</span> <span class="dv">10</span>, <span class="dt">by =</span> <span class="st">&quot;support&quot;</span>))</code></pre></div>
<pre><code>##      lhs                 rhs        support confidence lift
## [1]  {брови_подняты,                                       
##       нос_круглый,                                         
##       усы}            =&gt; {Демократ}  0.2500          1    2
## [2]  {глаза_круглые,                                       
##       губы_толстые,                                        
##       уши_оттопырен}  =&gt; {Демократ}  0.2500          1    2
## [3]  {глаза_круглые,                                       
##       губы_толстые,                                        
##       курит_трубка}   =&gt; {Демократ}  0.2500          1    2
## [4]  {бабочка,                                             
##       губы_толстые,                                        
##       уши_оттопырен}  =&gt; {Демократ}  0.2500          1    2
## [5]  {глаза_круглые,                                       
##       губы_толстые,                                        
##       курит_трубка,                                        
##       уши_оттопырен}  =&gt; {Демократ}  0.2500          1    2
## [6]  {голова_круглая,                                      
##       лоб_морщины}    =&gt; {Демократ}  0.1875          1    2
## [7]  {голова_круглая,                                      
##       лоб_морщины,                                         
##       усы}            =&gt; {Демократ}  0.1875          1    2
## [8]  {родинка_щеке,                                        
##       нос_круглый,                                         
##       усы}            =&gt; {Демократ}  0.1875          1    2
## [9]  {брови_подняты,                                       
##       родинка_щеке,                                        
##       нос_круглый}    =&gt; {Демократ}  0.1875          1    2
## [10] {брови_подняты,                                       
##       родинка_щеке,                                        
##       усы}            =&gt; {Демократ}  0.1875          1    2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">head</span>(<span class="kw">sort</span>(rulesDem, <span class="dt">by =</span> <span class="st">&quot;support&quot;</span>), <span class="dv">10</span>), <span class="dt">method =</span> <span class="st">&quot;graph&quot;</span>,
     <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">nodeCol =</span> <span class="kw">grey.colors</span>(<span class="dv">10</span>), 
                    <span class="dt">edgeCol =</span> <span class="kw">grey</span>(.<span class="dv">7</span>), <span class="dt">alpha =</span> <span class="dv">1</span>))</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-5-7"></span>
<img src="_main_files/figure-html/fig-5-7-1.png" alt="Визуализация в форме графа 10 лучших правил для демократов" width="576" />
<p class="caption">
Рисунок 5.7: Визуализация в форме графа 10 лучших правил для демократов
</p>
</div>
<p>Метод <code>&quot;graph&quot;</code> функции <code>plot()</code> показывает правила и составляющие их признаки в виде графа, размер узлов которого пропорционален уровню поддержки каждого представленного правила (рис. <a href="ch-5.html#fig:fig-5-7">5.7</a>).</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-4.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-6.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
