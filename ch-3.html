<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Классификация, регрессия, алгоритмы Data Mining с использованием R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Реализация алгоритмов Data Mining с использованием R">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Классификация, регрессия, алгоритмы Data Mining с использованием R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Реализация алгоритмов Data Mining с использованием R" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Классификация, регрессия, алгоритмы Data Mining с использованием R" />
  
  <meta name="twitter:description" content="Реализация алгоритмов Data Mining с использованием R" />
  

<meta name="author" content="Шитиков В. К., Мастицкий С. Э.">


<meta name="date" content="2017-03-24">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="ch-2.html">
<link rel="next" href="ch-4.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Аннотация</a></li>
<li class="chapter" data-level="1" data-path="ch-1.html"><a href="ch-1.html"><i class="fa fa-check"></i><b>1</b> Реализация моделей Data Mining в среде R</a><ul>
<li class="chapter" data-level="1.1" data-path="ch-1.html"><a href="ch-1.html#section_1_1"><i class="fa fa-check"></i><b>1.1</b> Data Mining как направление анализа данных </a><ul>
<li><a href="ch-1.html#------data-mining"><em>От статистического анализа разового эксперимента к Data Mining</em></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-2.html"><a href="ch-2.html"><i class="fa fa-check"></i><b>2</b> Статистические модели: критерии и методы оценивания их качества</a><ul>
<li class="chapter" data-level="2.1" data-path="ch-2.html"><a href="ch-2.html#sec_2_1"><i class="fa fa-check"></i><b>2.1</b> Основные шаги построения и верификации моделей</a></li>
<li class="chapter" data-level="2.2" data-path="ch-2.html"><a href="ch-2.html#sec_2_2"><i class="fa fa-check"></i><b>2.2</b> Использование алгоритмов ресэмплинга для тестирования моделей и оптимизации их параметров</a></li>
<li class="chapter" data-level="2.3" data-path="ch-2.html"><a href="ch-2.html#sec_2_3"><i class="fa fa-check"></i><b>2.3</b> Модели для предсказания класса объектов</a></li>
<li class="chapter" data-level="2.4" data-path="ch-2.html"><a href="ch-2.html#sec_2_4"><i class="fa fa-check"></i><b>2.4</b> Проецирование многомерных данных на плоскости</a></li>
<li class="chapter" data-level="2.5" data-path="ch-2.html"><a href="ch-2.html#sec_2_5"><i class="fa fa-check"></i><b>2.5</b> Многомерный статистический анализ данных</a></li>
<li class="chapter" data-level="2.6" data-path="ch-2.html"><a href="ch-2.html#sec_2_6"><i class="fa fa-check"></i><b>2.6</b> Методы кластеризации</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-3.html"><a href="ch-3.html"><i class="fa fa-check"></i><b>3</b> Пакет <code>caret</code> - инструмент построения статистических моделей в R</a><ul>
<li class="chapter" data-level="3.1" data-path="ch-3.html"><a href="ch-3.html#---------caret"><i class="fa fa-check"></i><b>3.1</b> Универсальный интерфейс доступа к функциям машинного обучения в пакете <code id="sec_3_1">caret</code></a></li>
<li class="chapter" data-level="3.2" data-path="ch-3.html"><a href="ch-3.html#sec_3_2"><i class="fa fa-check"></i><b>3.2</b> Обнаружение и удаление “ненужных” предикторов</a></li>
<li class="chapter" data-level="3.3" data-path="ch-3.html"><a href="ch-3.html#sec_3_3"><i class="fa fa-check"></i><b>3.3</b> Предварительная обработка: преобразование и групповая трансформация переменных</a></li>
<li class="chapter" data-level="3.4" data-path="ch-3.html"><a href="ch-3.html#sec_3_4"><i class="fa fa-check"></i><b>3.4</b> Заполнение пропущенных значений в данных</a></li>
<li class="chapter" data-level="3.5" data-path="ch-3.html"><a href="ch-3.html#-train---caret"><i class="fa fa-check"></i><b>3.5</b> Функция <code>train()</code> из пакета <code id="sec_3_5">caret</code></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-4.html"><a href="ch-4.html"><i class="fa fa-check"></i><b>4</b> Построение регрессионных моделей различного типа</a><ul>
<li class="chapter" data-level="4.1" data-path="ch-4.html"><a href="ch-4.html#sec_4_1"><i class="fa fa-check"></i><b>4.1</b> Селекция оптимального набора предикторов линейной модели</a><ul>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#-----"><i class="fa fa-check"></i>Полная регрессионная модель и пошаговая процедура</a></li>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#--"><i class="fa fa-check"></i>Рекурсивное исключение переменных</a></li>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#-"><i class="fa fa-check"></i>Генетический алгоритм</a></li>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#------"><i class="fa fa-check"></i>Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ch-4.html"><a href="ch-4.html#sec_4_2"><i class="fa fa-check"></i><b>4.2</b> Регуляризация, частные наименьшие квадраты и kNN-регрессия</a><ul>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#---"><i class="fa fa-check"></i>Регрессия по методу “лассо”</a></li>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#----pls"><i class="fa fa-check"></i>Метод частных наименьших квадратов (PLS)</a></li>
<li><a href="ch-4.html#---k--">Регрессия по методу <em>k</em> ближайших соседей</a></li>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#-------1"><i class="fa fa-check"></i>Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ch-4.html"><a href="ch-4.html#sec_4_3"><i class="fa fa-check"></i><b>4.3</b> Построение деревьев регрессии</a><ul>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#-----"><i class="fa fa-check"></i>Построение деревьев на основе рекурсивного разбиения</a></li>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#------"><i class="fa fa-check"></i>Построение деревьев с использованием алгортма условного вывода</a></li>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#-------2"><i class="fa fa-check"></i>Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-4.html"><a href="ch-4.html#sec_4_4"><i class="fa fa-check"></i><b>4.4</b> Ансамбли моделей: бэггинг, случайные леса, бустинг</a><ul>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#---"><i class="fa fa-check"></i>Бэггинг и случайные леса</a></li>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#d091d183d181d182d0b8d0bdd0b3"><i class="fa fa-check"></i>Бустинг</a></li>
<li class="chapter" data-level="" data-path="ch-4.html"><a href="ch-4.html#-------3"><i class="fa fa-check"></i>Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="ch-4.html"><a href="ch-4.html#sec_4_5"><i class="fa fa-check"></i><b>4.5</b> Сравнение построенных моделей и оценка информативности предикторов</a></li>
<li class="chapter" data-level="4.6" data-path="ch-4.html"><a href="ch-4.html#sec_4_6"><i class="fa fa-check"></i><b>4.6</b> Деревья регрессии с многомерным откликом</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-5.html"><a href="ch-5.html"><i class="fa fa-check"></i><b>5</b> Бинарные матрицы и ассоциативные правила</a><ul>
<li class="chapter" data-level="5.1" data-path="ch-5.html"><a href="ch-5.html#sec_5_1"><i class="fa fa-check"></i><b>5.1</b> Классификация в бинарных пространствах с использованием классических моделей</a></li>
<li class="chapter" data-level="5.2" data-path="ch-5.html"><a href="ch-5.html#sec_5_2"><i class="fa fa-check"></i><b>5.2</b> Бинарные деревья решений</a></li>
<li class="chapter" data-level="5.3" data-path="ch-5.html"><a href="ch-5.html#sec_5_3"><i class="fa fa-check"></i><b>5.3</b> Поиск логических закономерностей в данных</a></li>
<li class="chapter" data-level="5.4" data-path="ch-5.html"><a href="ch-5.html#sec_5_4"><i class="fa fa-check"></i><b>5.4</b> Алгоритмы выделения ассоциативных правил</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-6.html"><a href="ch-6.html"><i class="fa fa-check"></i><b>6</b> Бинарные классфикаторы с различными разделяющими поверхностями</a><ul>
<li class="chapter" data-level="6.1" data-path="ch-6.html"><a href="ch-6.html#sec_6_1"><i class="fa fa-check"></i><b>6.1</b> Дискриминантный анализ</a></li>
<li class="chapter" data-level="6.2" data-path="ch-6.html"><a href="ch-6.html#sec_6_2"><i class="fa fa-check"></i><b>6.2</b> Дискриминантный анализ</a></li>
<li class="chapter" data-level="6.3" data-path="ch-6.html"><a href="ch-6.html#sec_6_3"><i class="fa fa-check"></i><b>6.3</b> Классификаторы с использованием нелинейных разделяющих поверхностей</a></li>
<li class="chapter" data-level="6.4" data-path="ch-6.html"><a href="ch-6.html#sec_6_4"><i class="fa fa-check"></i><b>6.4</b> Деревья классификации, случайный лес и логистическая регрессия</a></li>
<li class="chapter" data-level="6.5" data-path="ch-6.html"><a href="ch-6.html#sec_6_5"><i class="fa fa-check"></i><b>6.5</b> Процедуры сравнения эффективности моделей классификации</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Классификация, регрессия, алгоритмы Data Mining с использованием R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch_3" class="section level1">
<h1><span class="header-section-number">ГЛАВА 3</span> Пакет <code>caret</code> - инструмент построения статистических моделей в R</h1>
<div id="---------caret" class="section level2">
<h2><span class="header-section-number">3.1</span> Универсальный интерфейс доступа к функциям машинного обучения в пакете <code id="sec_3_1">caret</code></h2>
<p>Использование сложных алгоритмов построения статистических моделей становится все более распространенной тенденцией в самых разных областях - от академических исследований до всевозможных бизнес-приложений. Среда статистических вычислений R отличается особенно богатым набором пакетов для создания различных моделей классификации и регрессии. Однако такое высокое разнообразие реализованных алгоритмов создает и ряд некоторых проблем.</p>
<p>Для аналитика становится затруднительным выбрать конкретный алгоритм машинного обучения, составить план оптимизации параметров, учесть синтаксические нюансы каждой функции и отследить особенности ее выполнения. Кроме того, процедуры, позволяющие реализовать полный цикл разработки предсказательных моделей, часто “разбросаны” по разным пакетам, что требует времени для поиска необходимых компонент и их освоения. Возникает потребность как-то интегрировать разные функции и методы в рамках некоторой единой надстройки, что позволило бы обобщить характерные для всех моделей процедуры вычислений.</p>
<p>Выполняя эту задачу, д-р М. Кун с сотрудниками предприняли попытку разработать универсальный интерфейс, предоставляющий доступ к основым алгоритмам машинного обучения, реализованным в R и других специализированных статистических системах (например, Weka). Разработчики поставили перед собой следующие задачи:</p>
<ul>
<li>учет имеющихся синтаксических различий между используемыми функциями R при построении моделей классификации и регрессии и тестировании их возможностей для прогнозирования;</li>
<li>развитие ряда полуавтоматических, интеллектуальных подходов и критериев для оптимизации настраиваемых коэффициентов и параметров многих из этих моделей с использованием алгоритмов ресэмплинга;</li>
<li>создание пакета с постоянно расширяющимся набором методов;</li>
<li>реализация параллельных вычислений при подгонке моделей.</li>
</ul>
<p>Результатом этой работы стал пакет <code>caret</code> (сокращение от <strong>C</strong>lassication <strong>a</strong>nd <strong>R</strong>egression <strong>T</strong>raining), который сегодня стал одним из наиболее популярных инструментов среди пользователей R, занимающихся разработкой предсказательных моделей. Основные возможности пакета <code>caret</code> достаточно полно представлены в публикациях разработчиков (Kuhn, 2008, 2012, 2013; Kuhn, Johnson, 2013).</p>
<p>На момент написания этой книги пакет <code>caret</code> включал унифицированный интерфейс со 147 функциями из 27 пакетов, которые автоматически подгружаются по мере необходимости (в случае, когда они установлены в системе). Наиболее распространенная технология работы с функциями пакета связана с автоматизированным нахождением оптимальных значений гиперпараметров моделей (tuning parameters), которые обычно невозможно вычислить аналитически, и построена по следующей формальной схеме:</p>
<p><img src="figures/caret_workflow.png" width="680px" style="display: block; margin: auto;" /></p>
<p>Кроме возможностей для настройки параметров, пакет <code>caret</code> содержит набор различных функций, способствующих реализации <em>полного цикла</em> разработки предсказательных моделей, начиная от сервисных процедур подготовки исходных данных до детальной оценки важности переменных, включенных в итоговую модель. Перечислим основные этапы построения моделей и функции пакета, реализующие их выполнение:</p>
<ol style="list-style-type: decimal">
<li><em>Разбиение</em> исходных данных на обучающую и контрольную выборки. Формирование индексов любой такой последовательности для вектора x случайным образом в заданном соотношении <code>p</code> осуществляется функцией <code>createDataPartition(x, p = 3/4, list = FALSE)</code>, что, в целом, эквивалентно использованию функции <code>sample(length(x), size = 0.75*length(x))</code>. C помощью аналогичных функций <code>createResample()</code>, <code>createFolds()</code> и <code>createMultiFolds()</code> можно создать бутстреп-выборки или произвольные разбиения на части в любом количестве повторностей.</li>
<li><em>Разведочный анализ</em> исходных переменных. Графический анализ характера распределения данных и их взаимной зависимости, наличие выбросов и других аномальных ситуаций может быть выполнен с помощью функции <code>featurePlot()</code>. Функция <code>nearZeroVar()</code> позволяет исключить предикторы, дисперсия значений которых близка к нулю, а <code>findCorrelation()</code> позволяет выявить переменные, которые в значительной мере коррелируют с другими признаками.</li>
<li><em>Предварительная обработка</em> исходной выборки (“выравнивание” дисперсий, приведение к нормальному распределению, сглаживание выбросов, заполнение пропущенных значений и проч., что часто является эффективной мерой улучшения структуры данных). Функция <code>preProcess()</code> и метод <code>predict.preProcess()</code> могут выполнить большое количество различных процедур трансформации данных и других операций предобработки.</li>
<li><em>Обучение моделей</em>, нахождение оптимальных гиперпараметров и оценка точности предсказания. Последние две задачи реализуются с использованием разнообразных методов ресэмплинга. Все эти процедуры выполняются функцией <code>train()</code>, а <code>predict.train()</code> позволяет получать предсказания значений переменной-отклика на основе выбранной оптимальной модели.</li>
<li><em>Оценка “важности” предикторов</em> (variable importance) и селекция оптимального их набора. Функция <code>varImp()</code> рассчитывает для каждой переменной количественные показатели, отражающих их вклад в получение точных предсказаний в рамках построенной модели. Отбор информативного комплекса переменных может быть осуществлен также с помощью функции <code>rfe()</code>, выполняющей рекурсивное исключение переменных, или функции <code>gafs ()</code>, в которой реализован генетический алгоритм.</li>
</ol>
<p>В последующих разделах будут рассмотрены подробности использования как перечисленных, так и других функций, входящих в пакет <code>caret</code>.</p>

</div>
<div id="sec_3_2" class="section level2">
<h2><span class="header-section-number">3.2</span> Обнаружение и удаление “ненужных” предикторов</h2>
<p>Разведочный анализ исходных данных играет очень важную роль в процессе создании эффективных предсказательных моделей. Главная его цель - понимание свойств имеющихся в наличии переменных, таких как закономерность распределения, наличие выбросов или эффекта очень низкой дисперсии, характер взаимоотношений между откликом и предикторами, оценка уровня мультиколлинеарности и др. Поскольку многие алгоритмы могут быть чувствительными к наличию предикторов, которые не несут в себе никакой или почти никакой информации, то уже на предварительном этапе некоторые из них разумно идентифицировать как “ненужные” и в дальнейшем исключить из рассмотрения.</p>
<p>В качестве примера используем набор данных <code>GermanCredit</code>, входящий в состав пакета caret. Он содержит информацию по 1000 клиентам одного из немецких банков, (подробное описание см. на сайте <a href="https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29">UCI Machine Learning Repository</a>). Каждый клиент описан в пространстве 61 признака, которые могут использоваться для предсказания класса кредитоспособности: отклика <code>Class</code>, принимающего два значения <code>Good</code> (хороший) и <code>Bad</code> (плохой).</p>
<p>Предположим, что мы имеем дело с экстремальным случаем, когда некоторый предиктор представлен только одним уникальным значением (например, у всех клиентов банка значения этой переменной равны 1). При таком сценарии дисперсия предиктора равна нулю и он бесполезен для предсказания интересующего нас отклика. В других случаях дисперсия может быть отличной от 0, но все же недостаточно высокой для того, чтобы сделать соответствующую переменную полезной для предсказания отклика. Подобные предикторы с околонулевой дисперсией (near-zero variance) рекомендуется удалять из дальнейшего анализа (Kuhn, Johnson, 2013).</p>
<p>Но как обнаружить такие предикторы? Одним из свойств вариационного ряда является доля уникальных значений по сравнению с общим числом наблюдений:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
<span class="kw">data</span>(GermanCredit)
(u &lt;-<span class="kw">unique</span>(GermanCredit$ResidenceDuration))</code></pre></div>
<pre><code>## [1] 4 2 3 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Доля уникальных значений:</span>
<span class="kw">length</span>(u)/<span class="kw">nrow</span>(GermanCredit)</code></pre></div>
<pre><code>## [1] 0.004</code></pre>
<p>Как видим, 1000 клиентов представлены только четырьмя уникальными значениями, доля которых составляет лишь 0.4% от их общего числа.</p>
<p>Однако сама по себе эта доля ни о чем не говорит, поскольку подавляющее большинство признаков в таблице <code>GermanCredit</code> являются индикаторными переменными, обозначающими наличие или отсутствие того или иного свойства у клиента, т.е. представлены только значениям 1 и 0. Важной является не только низкая доля уникальных значений, но еще и относительная частота этих значений. Поэтому рекомендуется (Kuhn, Johnson, 2013) рассчитывать отношение частоты наиболее часто встречающегося значения к частоте второго по встречаемости значения. Высокое отношение будет указывать на явный дисбаланс в частотах уникальных значений и, как следствие, на низкую дисперсию:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(t&lt;-<span class="st"> </span><span class="kw">sort</span>(<span class="kw">table</span>(GermanCredit$ResidenceDuration), <span class="dt">decreasing =</span> <span class="ot">TRUE</span>))</code></pre></div>
<pre><code>## 
##   4   2   3   1 
## 413 308 149 130</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">t[<span class="dv">1</span>]/t[<span class="dv">2</span>]</code></pre></div>
<pre><code>##        4 
## 1.340909</code></pre>
<p>На практике предлагается придерживаться следующих эмпирических правил для заключения о том, что некоторый предиктор обладает околонулевой дисперсией:</p>
<ul>
<li>доля его уникальных значений от общего числа наблюдений составляет не более 10%;</li>
<li>отношение частот первых двух наиболее обычных его значений превышает 20.</li>
</ul>
<p>В состав пакета caret входит функция <code>nearZeroVar()</code>, которая позволяет автоматически обнаружить предикторы, удовлетворяющие этим двум условиям:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Создадим копию данных без столбца с откликом Class:</span>
gcred =<span class="st"> </span>GermanCredit[, -<span class="dv">10</span>]
<span class="co"># Функция nearZeroVar() возращает вектор номеров переменных,</span>
<span class="co"># обладающих околонулевой дисперсией:</span>
(<span class="dt">nz =</span> <span class="kw">nearZeroVar</span>(gcred))</code></pre></div>
<pre><code>##  [1]  9 14 15 23 24 26 27 29 33 44 46 53 58</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="st">&quot;Имена этих переменных:&quot;</span>); <span class="kw">names</span>(gcred)[nz]</code></pre></div>
<pre><code>## [1] &quot;Имена этих переменных:&quot;</code></pre>
<pre><code>##  [1] &quot;ForeignWorker&quot;                     
##  [2] &quot;CreditHistory.NoCredit.AllPaid&quot;    
##  [3] &quot;CreditHistory.ThisBank.AllPaid&quot;    
##  [4] &quot;Purpose.DomesticAppliance&quot;         
##  [5] &quot;Purpose.Repairs&quot;                   
##  [6] &quot;Purpose.Vacation&quot;                  
##  [7] &quot;Purpose.Retraining&quot;                
##  [8] &quot;Purpose.Other&quot;                     
##  [9] &quot;SavingsAccountBonds.gt.1000&quot;       
## [10] &quot;Personal.Female.Single&quot;            
## [11] &quot;OtherDebtorsGuarantors.CoApplicant&quot;
## [12] &quot;OtherInstallmentPlans.Stores&quot;      
## [13] &quot;Job.UnemployedUnskilled&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Удаляем предикторы с околонулевой дисперсией:</span>
gcred.clean =<span class="st"> </span>gcred[, -nz]</code></pre></div>
<p>Описанная процедура вызывает, вероятно, некоторые сомнения у читателя, знакомого с основами метрологии. Во-первых, сама по себе близость дисперсии к нулевому значению ни о чем не говорит, поскольку все зависит от шкалы измерений (для субмолекулярных конструкций изменения размера в миллимикроны могут быть существенными, в то время, как для астрономических наблюдений ошибка в несколько километров является ничтожной). Поэтому дисперсия измерений считается недопустимо малой, если она не превосходит оценки погрешности измерений (или ошибки воспроизведения опыта). Во-вторых, следует уточнить, что эта процедура является эвристикой, полезной лишь для наблюдений, измеренных в порядковых или счетных шкалах с небольшим интервалом значений.</p>
<p>Другая проблема использования таких классических методов, как линейная регрессия или логистическая регрессия заключается в <em>мультиколлинеарности</em>. Наличие нескольких высоко коррелирующих друг с другом предикторов может привести к созданию неустойчивых решений или вообще сделать построение модели невозможным. Поскольку такие переменные несут, по сути, одинаковую информацию, то удаление части из них не приведет к заметному снижению качества модели.</p>
<p>Выберем наиболее коррелирующие пары переменных из таблицы GermanCredit. Составим для этого специальную функцию (А. Шипунов):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Наибольшие значения треугольной матрицы</span>
top.mat &lt;-<span class="st"> </span>function(X, <span class="dt">level =</span> <span class="fl">0.45</span>, <span class="dt">N =</span> <span class="dv">12</span>, <span class="dt">values =</span> <span class="ot">TRUE</span>) {
    X.nam &lt;-<span class="st"> </span><span class="kw">row.names</span>(X)
    X.tri &lt;-<span class="st"> </span><span class="kw">as.vector</span>(<span class="kw">lower.tri</span>(X))
    X.rep.g &lt;-<span class="st"> </span><span class="kw">rep</span>(X.nam, <span class="kw">length</span>(X.nam))[X.tri]
    X.rep.e &lt;-<span class="st"> </span><span class="kw">rep</span>(X.nam, <span class="dt">each =</span> <span class="kw">length</span>(X.nam))[X.tri]
    X.vec &lt;-<span class="st"> </span><span class="kw">as.vector</span>(X)[X.tri]
    X.df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Var1 =</span> X.rep.g, <span class="dt">Var2 =</span> X.rep.e, <span class="dt">Value =</span> X.vec)
    {if (values)
    {X.df &lt;-<span class="st"> </span>X.df[<span class="kw">abs</span>(X.df$Value) &gt;=<span class="st"> </span>level, ]
    X.df &lt;-<span class="st"> </span>X.df[<span class="kw">order</span>(-<span class="kw">abs</span>(X.df$Value)), ]}
        else
        {X.df &lt;-<span class="st"> </span>X.df[<span class="kw">order</span>(-<span class="kw">abs</span>(X.df$Value)), ]
        X.df &lt;-<span class="st"> </span>X.df[<span class="dv">1</span>:N, ]}}
    <span class="kw">row.names</span>(X.df) &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="dt">along =</span> X.df$Value)
    <span class="kw">return</span>(X.df)
}

<span class="kw">top.mat</span>(<span class="kw">cor</span>(gcred.clean))</code></pre></div>
<pre><code>##                                      Var1                           Var2
## 1              OtherInstallmentPlans.None     OtherInstallmentPlans.Bank
## 2                         Housing.ForFree               Property.Unknown
## 3                    Personal.Male.Single      Personal.Female.NotSingle
## 4                             Housing.Own                   Housing.Rent
## 5        OtherDebtorsGuarantors.Guarantor    OtherDebtorsGuarantors.None
## 6                  CreditHistory.Critical         CreditHistory.PaidDuly
## 7                     Job.SkilledEmployee          Job.UnskilledResident
## 8                                  Amount                       Duration
## 9             SavingsAccountBonds.Unknown     SavingsAccountBonds.lt.100
## 10                        Housing.ForFree                    Housing.Own
## 11 Job.Management.SelfEmp.HighlyQualified            Job.SkilledEmployee
## 12                 CreditHistory.PaidDuly          NumberExistingCredits
## 13                 CreditHistory.Critical          NumberExistingCredits
## 14             CheckingAccountStatus.none     CheckingAccountStatus.lt.0
## 15             CheckingAccountStatus.none CheckingAccountStatus.0.to.200
## 16                            Housing.Own               Property.Unknown
##         Value
## 1  -0.8405461
## 2   0.7798526
## 3  -0.7380357
## 4  -0.7359677
## 5  -0.7314079
## 6  -0.6836174
## 7  -0.6524383
## 8   0.6249842
## 9  -0.5832811
## 10 -0.5484452
## 11 -0.5438517
## 12 -0.5403545
## 13  0.5013637
## 14 -0.4953575
## 15 -0.4891356
## 16 -0.4764963</code></pre>
<p>В состав пакета caret входит функция <code>findCorrelation()</code>, которая, как следует из ее названия, находит предикторы, чей уровень корреляции с другими предикторами в среднем превышает некоторый заданный пользователем порог (аргумент <code>cutoff</code>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Функция findCorrelation() возвращает вектор </span>
<span class="co"># номеров переменных с высокой корреляцией:</span>
(<span class="dt">highCor =</span> <span class="kw">findCorrelation</span>(<span class="kw">cor</span>(gcred.clean), <span class="dt">cutoff =</span> <span class="fl">0.75</span>))</code></pre></div>
<pre><code>## [1] 40 41</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="st">&quot;Имена этих переменных:&quot;</span>); <span class="kw">names</span>( gcred.clean)[highCor]</code></pre></div>
<pre><code>## [1] &quot;Имена этих переменных:&quot;</code></pre>
<pre><code>## [1] &quot;Property.Unknown&quot;           &quot;OtherInstallmentPlans.Bank&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Удаляем эти переменные:</span>
gcred.clean =<span class="st">  </span>gcred.clean[, -highCor]</code></pre></div>
<p>Наконец, специальная функция <code>findLinearCombos()</code> предназначена для нахождения и исключения переменных, связанных линейными зависимостями. Если в выборке, например, есть переменная, которая может быть выражена через сумму нескольких других переменных, то такой предиктор можно рассматривать как малоинформативный (несущий дублирующую информацию) и его можно исключить из построения модели. Результатом функции <code>findLinearCombos()</code> является список из двух элементов: <code>$linearCombos</code> – список найденных линейных комбинаций и <code>$remove</code> - вектор индексов переменных, которые можно выразить через линейную комбинацию остальных переменных.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(linCombo &lt;-<span class="st"> </span><span class="kw">findLinearCombos</span>(gcred.clean))</code></pre></div>
<pre><code>## $linearCombos
## $linearCombos[[1]]
## [1] 30  9 10 11 12 26 27 28 29
## 
## $linearCombos[[2]]
## [1] 34  9 10 11 12 31 32 33
## 
## $linearCombos[[3]]
## [1] 43  9 10 11 12 41 42
## 
## 
## $remove
## [1] 30 34 43</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Удаляем эти переменные:</span>
gcred.clean =<span class="st">  </span>gcred.clean[, -linCombo$remove]
<span class="kw">dim</span>(gcred.clean)</code></pre></div>
<pre><code>## [1] 1000   43</code></pre>
<p>В результате всех этих операций число предикторов сократилось с 61 до 43.</p>
<p>Отметим, что очистка исходных данных от “ненужных” предикторов с использованием представленных функций может показаться излишне радикальной. Во-первых, например, непонятно, почему рекомендуется удалить признак <code>OtherInstallmentPlans.None</code>, а не составляющий с ним корреляционную пару <code>OtherInstallmentPlans.Bank</code>. Во-вторых, разработаны более продвинутые методы минимизации “корреляционных плеяд”: использование фактора инфляции дисперсии <span class="math inline">\(VIF\)</span> или различные последовательные алгоритмы селекции. Наконец, для некоторых алгоритмов вышеописанные проблемы не несут реальной угрозы: например, деревья принятия решений нечувствительны к признакам с околонулевой дисперсией, а регрессия на главные компоненты способна преодолеть эффект мультиколлинеарности. Однако в условиях очень большого числа переменных подобные “простые” методы редукции могут оказаться неожиданно эффективными. Кроме того, при большом числе предикторов удаление “ненужных” переменных может ускорить вычисления.</p>

</div>
<div id="sec_3_3" class="section level2">
<h2><span class="header-section-number">3.3</span> Предварительная обработка: преобразование и групповая трансформация переменных</h2>
<p>Необходимость преобразования исходных значений предикторов может быть вызвана разными причинами. Например, некоторые статистические методы требуют, чтобы все предикторы измерялись в одинаковых единицах. В других случаях качество модели может в значительной мере зависеть от характера распределения данных или наличия выбросов. Большинство наиболее распространенных способов преобразования количественных предикторов может быть реализовано функцией <code>preProcess()</code> из пакета <code>caret</code>. Ее основными аргументами являются следующие:</p>
<p><code>preProcess(x, method = c(&quot;center&quot;, &quot;scale&quot;), na.remove = TRUE, verbose = FALSE)</code>,</p>
<ul>
<li><code>x</code> - таблица или матрица с исходными данными (все переменные должны быть количественными);</li>
<li><code>method</code> - список с названиями методов трансформации;</li>
<li><code>verbose</code> - флаг для указания необходимости выводить протокол обработки.</li>
</ul>
<p>Методы предобработки можно условно разделить на три группы: * преобразование отдельных предикторов: <code>&quot;center&quot;</code>, <code>&quot;scale&quot;</code>, <code>&quot;range&quot;</code>, <code>&quot;expoTrans&quot;</code>, <code>&quot;BoxCox&quot;</code>, <code>&quot;YeoJohnson&quot;</code>; * групповая трансформация подмножества переменных: <code>&quot;spatialSign&quot;</code>, <code>&quot;pca&quot;</code> и <code>&quot;ica&quot;</code>; * заполнение пропущенных значений: <code>&quot;knnImpute&quot;</code>, <code>&quot;bagImpute&quot;</code>, <code>&quot;medianImpute&quot;</code>.</p>
<p>Для приведения всех переменных к одинаковым единицам измерения служит стандартизация, являющаяся самой распространенной операцией предобработки (и потому задается в <code>preProcess()</code> по умолчанию). Она заключается в вычитании из исходного значения <span class="math inline">\(x_i\)</span> некоторой переменной <span class="math inline">\(X\)</span> соответствующего среднего значения (“центрирование”, или <code>&quot;center&quot;</code>) и последующего деления полученной разницы на стандартное отклонение этой переменной σx (“нормализация”, или <code>&quot;scale&quot;</code>): <span class="math display">\[x&#39;_i = \frac{(x_i - \bar{x})}{\sigma_x}.\]</span></p>
<p>В результате стандартизации все количественные переменные будут иметь среднее значение, равное 0, и стандартное отклонение, равное 1.</p>
<p>Например, в таблице <code>GermanCredit</code> содержатся переменные, измеренные в разных шкалах: размер кредита <code>Amount</code> измеряется в немецких марках, возраст клиента <code>Age</code> - в годах, и т.д. Как следствие, размах значений переменных также существенно разнится: индикаторные переменные по определению варьируют от 0 до 1, тогда как размер кредита изменяется от 250 до 18424 марок. Выполним стандартизацию трех метрических переменных:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(GermanCredit)
TransPred &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Duration&quot;</span>, <span class="st">&quot;Amount&quot;</span>, <span class="st">&quot;Age&quot;</span>)
preVar &lt;-<span class="st"> </span><span class="kw">preProcess</span>(GermanCredit[, TransPred])
TransVar =<span class="st"> </span><span class="kw">predict</span>(preVar, GermanCredit[, TransPred])
<span class="kw">print</span>(<span class="st">&quot;До преобразования:&quot;</span>)</code></pre></div>
<pre><code>## [1] &quot;До преобразования:&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(GermanCredit[,TransPred]) </code></pre></div>
<pre><code>##     Duration        Amount           Age       
##  Min.   : 4.0   Min.   :  250   Min.   :19.00  
##  1st Qu.:12.0   1st Qu.: 1366   1st Qu.:27.00  
##  Median :18.0   Median : 2320   Median :33.00  
##  Mean   :20.9   Mean   : 3271   Mean   :35.55  
##  3rd Qu.:24.0   3rd Qu.: 3972   3rd Qu.:42.00  
##  Max.   :72.0   Max.   :18424   Max.   :75.00</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="st">&quot;После преобразования:&quot;</span>)</code></pre></div>
<pre><code>## [1] &quot;После преобразования:&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(TransVar) </code></pre></div>
<pre><code>##     Duration           Amount             Age         
##  Min.   :-1.4017   Min.   :-1.0703   Min.   :-1.4545  
##  1st Qu.:-0.7383   1st Qu.:-0.6751   1st Qu.:-0.7513  
##  Median :-0.2407   Median :-0.3372   Median :-0.2238  
##  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  
##  3rd Qu.: 0.2568   3rd Qu.: 0.2483   3rd Qu.: 0.5674  
##  Max.   : 4.2373   Max.   : 5.3681   Max.   : 3.4683</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># при необходимости, обновление переменных</span>
GermanCredit[, TransPred] &lt;-<span class="st"> </span>TransVar</code></pre></div>
<p>Опция method функции <code>preProcess()</code> может принимать и другие значения. В частности, значение <code>&quot;range&quot;</code> приводит значения переменных к диапазону [0, 1], а <code>&quot;expoTrans&quot;</code> выполняет вычисление экспоненциальной функции.</p>
<p>Некоторые из переменных имеют также явно выраженные асимметричные распределения (например, <code>Amount</code> и <code>Age</code>), что может представлять проблему для классических статистических методов. Часто решить эту проблему позволяют такие простые преобразования исходных значений, как извлечение квадратного корня, логарифмирование или расчет обратных значений. Если истинное нормализующее преобразование неизвестно, лучшим считается преобразование Бокса-Кокса (Box-Cox transformation - Box, Cox, 1964), которое позволяет найти оптимальное решение, в первую очередь, для нормализации дисперсии.</p>
<p>Универсальное семейство преобразований Бокса-Кокса (БК) случайной величины x зависит от значения параметра <span class="math inline">\(\lambda\)</span>:</p>
<ul>
<li>при <span class="math inline">\(\lambda \neq 0\)</span> мы имеем дело со степенным преобразованием вида <span class="math inline">\(y(\lambda) = \frac{x^{\lambda} -1 }{\lambda}\)</span>, где показатель степени может принимать любые положительные или отрицательные значения;</li>
<li>в частных случаях после подстановки параметра <span class="math inline">\(\lambda\)</span> в основную формулу будем иметь: <span class="math inline">\(y = 1/x\)</span> при <span class="math inline">\(\lambda = -1\)</span>; <span class="math inline">\(y = 1/\sqrt{x}\)</span> при <span class="math inline">\(\lambda = -0.5\)</span>; <span class="math inline">\(y = \sqrt{x}\)</span> при <span class="math inline">\(\lambda = 0.5\)</span>; <span class="math inline">\(y = x^2\)</span> при <span class="math inline">\(\lambda = 2\)</span> и т.д.;</li>
<li>при = 0 используется логарифмическое преобразование <span class="math inline">\(y(\lambda) = \log x\)</span> (поскольку деление на нуль приводит к неопределенности).</li>
</ul>
<p>Таким образом, большинство “простых формул” представляют собой лишь частный случай преобразования БК. Применим к метрическим показателям таблицы БК-преобразование</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(preBox &lt;-<span class="st"> </span><span class="kw">preProcess</span>(GermanCredit[, TransPred], <span class="dt">method =</span> <span class="st">&quot;BoxCox&quot;</span>))</code></pre></div>
<pre><code>## Created from 1000 samples and 0 variables
## 
## Pre-processing:
##   - ignored (0)</code></pre>
<p>Для предиктора <code>Amount</code> мы получили расчетное значение <span class="math inline">\(\lambda = -0.1\)</span>. Результат преобразования с этим параметром представим графически (рис. <a href="ch-3.html#fig:fig-3-1">3.1</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">BoxVar &lt;-<span class="st"> </span><span class="kw">predict</span>(preBox,GermanCredit[, TransPred]) 
y &lt;-<span class="st"> </span><span class="kw">factor</span>(GermanCredit$Class)
<span class="kw">trellis.par.set</span>(<span class="dt">theme =</span> <span class="kw">col.whitebg</span>(), <span class="dt">warn =</span> <span class="ot">FALSE</span>)
<span class="kw">featurePlot</span>(GermanCredit$Amount, y, <span class="st">&quot;density&quot;</span>, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Amount&quot;</span>, <span class="st">&quot;&quot;</span>))</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-47-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">featurePlot</span>(BoxVar[, <span class="dv">2</span>], y, <span class="st">&quot;density&quot;</span>, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Box-Amount&quot;</span>, <span class="st">&quot;&quot;</span>))</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-3-1"></span>
<img src="_main_files/figure-html/fig-3-1-1.png" alt="Результат преобразования Бокса-Кокса для размера кредита в немецком банке" width="672" />
<p class="caption">
Рисунок 3.1: Результат преобразования Бокса-Кокса для размера кредита в немецком банке
</p>
</div>
<p>При значении параметра <code>method=&quot;YeoJohnson&quot;</code> выполняется трансформация Йео-Джонсона (Yeo-Johnson), которое весьма похожа на БК-преобразование, но учитывает в расчетах нулевые и отрицательные значения обрабатываемых переменных. Другой функцией, также позволяющей оценить оптимальное значение <span class="math inline">\(\lambda\)</span> для набора исходных данных, является функция <code>BoxCoxTrans()</code> из пакета <code>caret</code>.</p>
<p>В ряде случаев возникает необходимость одновременного преобразования целой группы предикторов или сразу всех предикторов, входящих в тот или иной набор данных. Как правило, такая необходимость возникает при наличии многомерных выбросов, а также когда исследователь желает снизить размерность исходной задачи, например, за счет удаления некоторого количества высоко коррелирующих предикторов.</p>
<p>Если ожидается, что тот или иной метод построения предсказательной модели чувствителен к наличию многомерных выбросов, исходные данные можно преобразовать при помощи метода пространственных знаков (spatial sign – Serneels et al., 2006). С математической точки зрения, этот метод -проецирует значения предикторов на поверхность многомерной сферы, в результате чего отдельные наблюдения становятся равноудаленными от центра этой сферы: <span class="math inline">\(X_{ij}&#39; = X_{ij} / \sum_{i=1}^m X_{ij}^2\)</span>, где <span class="math inline">\(m\)</span> – это число преобразуемых предикторов.</p>
<p>Поскольку знаменатель выражает квадрат расстояния до центра распределения предикторов, перед применением этого метода важно выполнить стандартизацию всех предикторов:</p>
<p><code>preProcess(х, method = c(&quot;center&quot;, &quot;scale&quot;, &quot;spatialSign&quot;))</code>,</p>
<p>чем будет достигнут примерно одинаковый их вклад в величину квадрата расстояния. Кроме того, важно уточнить, что поскольку это преобразование применяется одновременно к группе предикторов, то последующее изменение их состава может исказить смысл проведенной трансформации.</p>
<p>Как было показано в разделе <a href="ch-2.html#sec_2_4">2.4</a>, одним из наиболее популярных методов снижения размерности исходного набора данных является анализ главных компонент PCA (principle components analysis). Этот метод позволяет сформировать ортогональную систему координат, обеспечивающую оптимальное расположение точек объектов относительно осей главных компонент нового редуцированного пространства. Пересчет из исходной системы координат в <em>p</em>-мерное (<span class="math inline">\(m &gt; p\)</span>) пространство главных компонент осуществляется с использованием линейных ортогональных преобразований переменных <span class="math inline">\(X_i\)</span>: <span class="math inline">\(T_k = \sum_{i=1}^m P_{ik}(X_i - \mu_i)\)</span>, где <span class="math inline">\(P_{ik}\)</span> - нагрузки (компоненты собственного вектора, соответствующего <span class="math inline">\(k\)</span>-му собственному значению), <span class="math inline">\(k = 1, 2, \dots, p\)</span>. Основной критерий качества такого преобразования – достаточно высокая доля общей вариации исходных данных, объясняемая p первыми собственными значениями (например, 80% или 95%).</p>
<p>Для реализации PCA-процедуры с помощью <code>preProcess()</code> необходимо определить параметр <code>thresh</code> - кумулятивную долю дисперсии исходных данных, которая должна содержаться в главных компонентах (по умолчанию <code>thresh = 0.95</code>) или <code>pcaComp</code> - максимальное число главных компонент (по умолчанию этот параметр имеет значение <code>NULL</code>, т.е. он выключен, и оптимальное число главных компонент выбирается на основе <code>thresh</code>).</p>
<p>Построим две модели логистической регрессии на основе “очищенных” (см. раздел <a href="ch-3.html#sec_3_2">3.2</a>) данных таблицы <code>GermanCredit</code> и после РСА-преобразования:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span><span class="kw">factor</span>(GermanCredit$Class)
gcred &lt;-<span class="st"> </span>GermanCredit[, -<span class="dv">10</span>]
nz &lt;-<span class="st"> </span><span class="kw">nearZeroVar</span>(gcred)
gcred.clean &lt;-<span class="st"> </span>gcred[, -nz]
highCor &lt;-<span class="st"> </span><span class="kw">findCorrelation</span>(<span class="kw">cor</span>(gcred.clean), <span class="dt">cutoff =</span> <span class="fl">0.75</span>)
gcred.clean &lt;-<span class="st">  </span>gcred.clean[, -highCor]
linCombo &lt;-<span class="st"> </span><span class="kw">findLinearCombos</span>(gcred.clean)
gcred.clean &lt;-<span class="st">  </span>gcred.clean[, -linCombo$remove]
<span class="kw">dim</span>(gcred.clean)</code></pre></div>
<pre><code>## [1] 1000   43</code></pre>
<p>Определим оптимальную размерность пространства главных компонент по критерию Кайзера-Гуттмана, который рекомендует оставить только те главные компоненты, собственные значения которых превышают их среднее (рис. <a href="ch-3.html#fig:fig-3-2">3.2</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(vegan)
mod.pca &lt;-<span class="st"> </span><span class="kw">rda</span>(gcred.clean, <span class="dt">scale =</span> <span class="ot">TRUE</span>) 
ev &lt;-<span class="st"> </span>mod.pca$CA$eig 
<span class="co"># Иллюстрация Критерия Кайзера-Гуттмана</span>
<span class="kw">barplot</span>(ev, <span class="dt">col =</span> <span class="st">&quot;bisque&quot;</span>, <span class="dt">las =</span> <span class="dv">2</span>)
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="kw">mean</span>(ev), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>) 
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="st">&quot;Средние собственных значений&quot;</span>, <span class="dt">lwd =</span> <span class="dv">1</span>, <span class="dt">col =</span> <span class="dv">2</span>, <span class="dt">bty =</span> <span class="st">&quot;n&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-3-2"></span>
<img src="_main_files/figure-html/fig-3-2-1.png" alt="Оценка числа главных компонент по критерию Кайзера-Гуттмана" width="768" />
<p class="caption">
Рисунок 3.2: Оценка числа главных компонент по критерию Кайзера-Гуттмана
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">c</span>(<span class="kw">length</span>(ev[ev &gt;<span class="st"> </span><span class="kw">mean</span>(ev)]), <span class="kw">sum</span>(ev[ev &gt;<span class="st"> </span><span class="kw">mean</span>(ev)])/<span class="kw">sum</span>(ev)) </code></pre></div>
<pre><code>## [1] 19.0000000  0.7272241</code></pre>
<p>Сформируем новую таблицу предикторов из 19 главных компонент, которые объясняют 72.7% общей дисперсии:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prePCA &lt;-<span class="st"> </span><span class="kw">preProcess</span>(gcred.clean, 
                     <span class="dt">method =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>, <span class="st">&quot;pca&quot;</span>), <span class="dt">pcaComp =</span> <span class="dv">19</span>)
gcred.pca &lt;-<span class="st"> </span><span class="kw">predict</span>(prePCA,gcred.clean)</code></pre></div>
<p>Попробуем сравнить точность прогноза двух моделей логистической регрессии с использованием функции <code>train()</code>, которая будет предметом нашего подробного рассмотрения ниже. Для тестирования моделей будем многократно (times = 100) случайным образом делить всю выборку на обучающую (800 объектов или 80%) и контрольную (200 объектов или 20%), для чего с помощью функции <code>createDataPartition()</code> создадим соответствующую “заготовку” <code>train.index</code>. Метод тестирования <code>method = &quot;LGOCV&quot;</code> (многократное разбиение на обучающую и контрольную выборки) и <code>train.index</code> определим в специальном объекте <code>trControl</code>. На функцию <code>train()</code> подадим данные для построения модели, тип модели и условия тестирования:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train.index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(y, <span class="dt">p =</span> .<span class="dv">8</span>, <span class="dt">times =</span> <span class="dv">100</span>)
trControl =<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;LGOCV&quot;</span>, <span class="dt">index =</span> train.index)
<span class="kw">print</span>(<span class="st">&quot;Модель на основе исходного набора из 43 предикторов&quot;</span>)</code></pre></div>
<pre><code>## [1] &quot;Модель на основе исходного набора из 43 предикторов&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(modSource &lt;-<span class="st"> </span><span class="kw">train</span>(gcred.clean, y, <span class="st">&quot;glm&quot;</span>, 
        <span class="dt">family =</span> binomial, <span class="dt">trControl =</span> trControl))</code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 1000 samples
##   43 predictor
##    2 classes: &#39;Bad&#39;, &#39;Good&#39; 
## 
## No pre-processing
## Resampling: Repeated Train/Test Splits Estimated (100 reps, 75%) 
## Summary of sample sizes: 800, 800, 800, 800, 800, 800, ... 
## Resampling results:
## 
##   Accuracy  Kappa    
##   0.75205   0.3709181
## 
## </code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="st">&quot;Модель на основе 19 главных компонент&quot;</span>)</code></pre></div>
<pre><code>## [1] &quot;Модель на основе 19 главных компонент&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(modPCA &lt;-<span class="st"> </span><span class="kw">train</span>(gcred.pca, y, <span class="st">&quot;glm&quot;</span>, 
        <span class="dt">family =</span> binomial, <span class="dt">trControl =</span> trControl))</code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 1000 samples
##   19 predictor
##    2 classes: &#39;Bad&#39;, &#39;Good&#39; 
## 
## No pre-processing
## Resampling: Repeated Train/Test Splits Estimated (100 reps, 75%) 
## Summary of sample sizes: 800, 800, 800, 800, 800, 800, ... 
## Resampling results:
## 
##   Accuracy  Kappa    
##   0.75285   0.3692622
## 
## </code></pre>
<p>Нельзя сказать, что преобразовав переменные, мы получили более точную модель, но зато обошлись значительно меньшим числом переменных.</p>
<p>При значении параметра <code>method = &quot;ica&quot;</code> функции <code>preProcess()</code> используется другой метод снижения размерности пространства переменных – анализ независимых компонент ICA (Independent Component Analysis), который выполняет ту же задачу, что и РСА, однако основан на несколько иных математических концепциях и процедурах. В частности, цель ICA состоит в нахождении таких новых компонент (нового пространства латентных переменных), которые взаимно независимы в полном статистическом смысле.</p>
<p>Заметим в заключение, что общим недостатком любых преобразований является потеря возможности количественно интерпретировать роль отдельных предикторов, поскольку они больше не будут выражаться в исходных единицах измерения. Это, разумеется, не является большой проблемой, если модель разрабатывается исключительно для прогнозирования и не предназначена для объяснения влияния тех или иных переменных на прогнозируемый отклик. Однако важной составляющей моделирования является оценка вклада, или “важности”, каждого предиктора при получении предсказаний на основе той или иной модели.</p>
<p>Эти количественные показатели важности переменных могут быть рассчитаны функцией <code>varImp()</code>. Смысл оценивающих метрик варьирует в зависимости от конкретного алгоритма: например, для моделей классификации можно использовать площадь под ROC-кривой (см. раздел <a href="ch-2.html#sec_2_3">2.3</a>) и оценивать этот показатель путем добавления или исключения каждого предиктора модели (рис. <a href="ch-3.html#fig:fig-3-3">3.3</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">varImp</span>(modSource, <span class="dt">scale =</span> <span class="ot">FALSE</span>))</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-3-3"></span>
<img src="_main_files/figure-html/fig-3-3-1.png" alt="Важность отдельных показателей клиентов банка для оценки их кредитоспособности" width="768" />
<p class="caption">
Рисунок 3.3: Важность отдельных показателей клиентов банка для оценки их кредитоспособности
</p>
</div>
<p>Разумеется, если мы будем в этом же ключе анализировать модель на основе главных компонент, то столкнемся с системой трудно интерпретируемых показателей.</p>

</div>
<div id="sec_3_4" class="section level2">
<h2><span class="header-section-number">3.4</span> Заполнение пропущенных значений в данных</h2>
<p>К сожалению, на практике в ходе сбора данных далеко не всегда удается полностью укомплектовать исходные таблицы. Пропуски отдельных значений являются повсеместным явлением и поэтому, прежде чем начать применять статистические методы, обрабатываемые данные следует привести к “каноническому” виду. Для этого необходимо либо удалить фрагменты объектов с недостающими элементами, либо заменить имеющиеся пропуски на некоторые разумные значения.</p>
<p>Проблема “борьбы с пропусками” столь же сложна, как и сама статистика, поскольку в этой области существует впечатляющее множество подходов. В русскоязычных книгах по использованию R (Кабаков, 2014; Мастицкий, Шитиков, 2015) бегло представлены только некоторые функции пакета mice, который, несмотря на свою “продвинутость”, мало удобен для практической работы с данными умеренного и большого объема. Хорошей альтернативой являются методы <code>&quot;knnImpute&quot;</code>, <code>&quot;bagImpute&quot;</code> и <code>&quot;medianImpute&quot;</code> функции <code>preProcess()</code> из пакета <code>caret</code>, которую мы рассмотрели в разделе <a href="ch-3.html#sec_3_3">3.3</a> как инструмент для трансформации данных.</p>
<p>Используем в качестве примера для дальнейших упражнений таблицу <code>algae</code>, включенную в пакет <code>DMwR</code> и содержащую данные гидробиологических исследований обилия водорослей в различных реках. Каждое из 200 наблюдений содержит информацию о 18 переменных, в том числе:</p>
<ul>
<li>три номинальных переменных, описывающих размеры <code>size = c(&quot;large&quot;, &quot;medium&quot;, &quot;small&quot;)</code> и скорость течения реки <code>speed = c(&quot;high&quot;, &quot;low&quot;, &quot;medium&quot;)</code>, а также время года <code>season = c(&quot;autumn&quot;, &quot;spring&quot;, &quot;summer&quot;, &quot;winter&quot;)</code>, сопряженное с моментом взятия проб;</li>
<li>8 переменных, составляющих комплекс наблюдаемых гидрохимических показателей: максимальное значение рН <code>mxPH</code> (1), минимальное содержание кислорода <code>mnO2</code> (2), хлориды <code>Cl</code> (10), нитраты <code>NO3</code> (2), ионы аммония <code>NH4</code> (2), орто-фосфаты <code>oPO4</code> (2), общий минеральный фосфор <code>PO4</code> (2) и количество хлорофилла <em>а</em> <code>Chla</code> (12) (в скобках приведено число пропущенных значений);</li>
<li>средняя численность каждой из 7 групп водорослей <code>a1</code> - <code>a7</code> (видовой состав не идентифицировался).</li>
</ul>
<p>Читатель может самостоятельно воспользоваться функциями описательной статистики <code>summary()</code> или <code>describe()</code> из пакета <code>Hmisc</code>, а мы постараемся поддержать добрую традицию и привести парочку примеров диаграмм, построенных с использованием пакета <code>ggplot2</code> (рис. <a href="ch-3.html#fig:fig-3-4">3.4</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(DMwR)
<span class="kw">library</span>(ggplot2)
<span class="co"># summary(algae) # вывод не приводится</span>
<span class="kw">ggplot</span>(<span class="dt">data =</span> algae[!<span class="kw">is.na</span>(algae$mnO2), ], <span class="kw">aes</span>(speed , mnO2)) +<span class="st"> </span>
<span class="st">       </span><span class="kw">geom_violin</span>(<span class="kw">aes</span>(<span class="dt">fill =</span> speed), <span class="dt">trim =</span> <span class="ot">FALSE</span>, 
       <span class="dt">alpha =</span> <span class="fl">0.3</span>) +<span class="st"> </span>
<span class="st">       </span><span class="kw">geom_boxplot</span>(<span class="kw">aes</span>(<span class="dt">fill =</span> speed), <span class="dt">width =</span> <span class="fl">0.2</span>,
       <span class="dt">outlier.colour =</span> <span class="ot">NA</span>) +<span class="st"> </span>
<span class="st">       </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;NA&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-3-4"></span>
<img src="_main_files/figure-html/fig-3-4-1.png" alt="Распределения значений содержания кислорода в воде рек с разной скоростью течения" width="672" />
<p class="caption">
Рисунок 3.4: Распределения значений содержания кислорода в воде рек с разной скоростью течения
</p>
</div>
<p>На рис. <a href="ch-3.html#fig:fig-3-4">3.4</a> мы получили так называемую “скрипичную диаграмму” (violin plot), которая объединяет в себе идеи диаграмм размахов и кривых распределения вероятности. Суть достаточно проста: продольные края “ящиков с усами” (для сравнения приведены тоже) замещаются кривыми плотности вероятности. В итоге, например, легко можно выяснить не только тот факт, что в потоках с быстрым течением (high) содержание кислорода выше, но и ознакомиться с характером распределения соответствующих значений.</p>
<p>Другой пример - категоризованные графики, удобные для визуализации данных, разбитых на отдельные подмножества (категории), каждое из которых отображается в отдельной диаграмме подходящего типа. Такие диаграммы, или “панели” (от англ. panels, facets или multiples), определенным образом упорядочиваются и размещаются на одной странице. Из графиков, представленных на рис. <a href="ch-3.html#fig:fig-3-5">3.5</a>, легко увидеть, что численность водорослей группы а1 падает с увеличением концентрации фосфатов.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qplot</span>(PO4, a1, <span class="dt">data =</span> algae[!<span class="kw">is.na</span>(algae$PO4), ]) +
<span class="st">      </span><span class="kw">facet_grid</span>(<span class="dt">facets =</span> ~<span class="st"> </span>season) +
<span class="st">      </span><span class="kw">geom_smooth</span>(<span class="dt">color =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-3-5"></span>
<img src="_main_files/figure-html/fig-3-5-1.png" alt="Графики изменения обилия водорослей в зависимости от содержания минерального фосфора в разное время года" width="768" />
<p class="caption">
Рисунок 3.5: Графики изменения обилия водорослей в зависимости от содержания минерального фосфора в разное время года
</p>
</div>
<p>Однако в контексте темы этого раздела важно обратить внимание на то, что мы все время старались блокировать появление пропущенных значений: <code>algae[!is.na(algae$PO4), ]</code>. Если в обрабатываемой таблице обнаружены недостающие данные, то в общих чертах можно избрать одну из следующих возможных стратегий:</p>
<ul>
<li>удалить строки с неопределенностями;</li>
<li>заполнить неизвестные значения выборочными статистиками соответствующей переменной (среднее, медиана и т.д.), полагая, что взаимосвязь между переменными в имеющемся наборе данных отсутствует (это соответствует известному “наивному” подходу);</li>
<li>заполнить неизвестные значения с учетом корреляции между переменными или меры близости между наблюдениями; постараться обходить эту неприятную ситуацию, используя, например, формальный параметр na.rm некоторых функций.</li>
</ul>
<p>Последняя альтернатива является наиболее ограничивающей, поскольку она далеко не во всех случаях позволяет осуществить необходимый анализ. В свою очередь, удаление целых строк данных слишком радикально и может привести к серьезным потерям важной информации:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Число строк с пропущенными значениями:</span>
<span class="kw">nrow</span>(algae[!<span class="kw">complete.cases</span>(algae),])</code></pre></div>
<pre><code>## [1] 16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Их удаление:</span>
algae &lt;-<span class="st"> </span><span class="kw">na.omit</span>(algae)</code></pre></div>
<p>Можно удалить не все строки, а только те, в которых число пропущенных значений превышает, например, 20% от общего числа переменных, для чего существует специальная функция из пакета <code>DMwR</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(algae)
<span class="kw">manyNAs</span>(algae, <span class="fl">0.2</span>)</code></pre></div>
<pre><code>## [1]  62 199</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">algae &lt;-<span class="st"> </span>algae[-<span class="kw">manyNAs</span>(algae, <span class="fl">0.2</span>), ]</code></pre></div>
<p>В результате мы удалили только две строки (62-ю и 199-ю), где число пропущенных значений больше одного. Обратите внимание, что выполняя команду <code>data(algae)</code>, мы каждый раз обновляем в памяти содержимое этого набора данных.</p>
<p>Если мы готовы принять гипотезу о том, что зависимостей между переменными нет, то простым и часто весьма эффективным способом заполнения пропусков является использование средних значений. В том случае, если есть сомнения в нормальности распределения данных, предпочтительнее использовать медиану. Покажем, как это можно сделать с использованием функции <code>preProcess()</code> из пакета <code>caret</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(algae)
ind &lt;-<span class="st"> </span><span class="kw">apply</span>(algae, <span class="dv">1</span>, function(x) <span class="kw">sum</span>(<span class="kw">is.na</span>(x))) &gt;<span class="st"> </span><span class="dv">0</span>
algae[ind, <span class="dv">4</span>:<span class="dv">11</span>]</code></pre></div>
<pre><code>##     mxPH mnO2    Cl   NO3 NH4    oPO4     PO4  Chla
## 28  6.80 11.1 9.000 0.630  20   4.000      NA  2.70
## 38  8.00   NA 1.450 0.810  10   2.500   3.000  0.30
## 48    NA 12.6 9.000 0.230  10   5.000   6.000  1.10
## 55  6.60 10.8    NA 3.245  10   1.000   6.500    NA
## 56  5.60 11.8    NA 2.220   5   1.000   1.000    NA
## 57  5.70 10.8    NA 2.550  10   1.000   4.000    NA
## 58  6.60  9.5    NA 1.320  20   1.000   6.000    NA
## 59  6.60 10.8    NA 2.640  10   2.000  11.000    NA
## 60  6.60 11.3    NA 4.170  10   1.000   6.000    NA
## 61  6.50 10.4    NA 5.970  10   2.000  14.000    NA
## 62  6.40   NA    NA    NA  NA      NA  14.000    NA
## 63  7.83 11.7 4.083 1.328  18   3.333   6.667    NA
## 116 9.70 10.8 0.222 0.406  10  22.444  10.111    NA
## 161 9.00  5.8    NA 0.900 142 102.000 186.000 68.05
## 184 8.00 10.9 9.055 0.825  40  21.083  56.091    NA
## 199 8.00  7.6    NA    NA  NA      NA      NA    NA</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
pPmI &lt;-<span class="st"> </span><span class="kw">preProcess</span>(algae[, <span class="dv">4</span>:<span class="dv">11</span>], <span class="dt">method =</span> <span class="st">&#39;medianImpute&#39;</span>)
algae[, <span class="dv">4</span>:<span class="dv">11</span>] &lt;-<span class="st"> </span><span class="kw">predict</span>(pPmI, algae[, <span class="dv">4</span>:<span class="dv">11</span>])
(Imp.Med &lt;-<span class="st"> </span>algae[ind, <span class="dv">4</span>:<span class="dv">11</span>])</code></pre></div>
<pre><code>##     mxPH mnO2     Cl   NO3      NH4    oPO4      PO4   Chla
## 28  6.80 11.1  9.000 0.630  20.0000   4.000 103.2855  2.700
## 38  8.00  9.8  1.450 0.810  10.0000   2.500   3.0000  0.300
## 48  8.06 12.6  9.000 0.230  10.0000   5.000   6.0000  1.100
## 55  6.60 10.8 32.730 3.245  10.0000   1.000   6.5000  5.475
## 56  5.60 11.8 32.730 2.220   5.0000   1.000   1.0000  5.475
## 57  5.70 10.8 32.730 2.550  10.0000   1.000   4.0000  5.475
## 58  6.60  9.5 32.730 1.320  20.0000   1.000   6.0000  5.475
## 59  6.60 10.8 32.730 2.640  10.0000   2.000  11.0000  5.475
## 60  6.60 11.3 32.730 4.170  10.0000   1.000   6.0000  5.475
## 61  6.50 10.4 32.730 5.970  10.0000   2.000  14.0000  5.475
## 62  6.40  9.8 32.730 2.675 103.1665  40.150  14.0000  5.475
## 63  7.83 11.7  4.083 1.328  18.0000   3.333   6.6670  5.475
## 116 9.70 10.8  0.222 0.406  10.0000  22.444  10.1110  5.475
## 161 9.00  5.8 32.730 0.900 142.0000 102.000 186.0000 68.050
## 184 8.00 10.9  9.055 0.825  40.0000  21.083  56.0910  5.475
## 199 8.00  7.6 32.730 2.675 103.1665  40.150 103.2855  5.475</code></pre>
<p>Альтернативой “наивному” подходу является учет структуры связей между переменными. Например, можно воспользоваться тем, что между двумя формами фосфора <code>oPO4</code> и <code>PO4</code> существует тесная корреляционная связь. Тогда, например, можно избавиться от некоторых неопределенностей в показателе <code>PO4</code>, вычислив его пропущенные значения по уравнению простой регрессии:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(algae)
<span class="kw">lm</span>(PO4 ~<span class="st"> </span>oPO4, <span class="dt">data =</span> algae)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = PO4 ~ oPO4, data = algae)
## 
## Coefficients:
## (Intercept)         oPO4  
##      42.897        1.293</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Функция  вывода значений PO4 в зависимости от оPO4 </span>
fillPO4 &lt;-<span class="st"> </span>function(oP) {if (<span class="kw">is.na</span>(oP)) <span class="kw">return</span>(<span class="ot">NA</span>)
      else <span class="kw">return</span>(<span class="fl">42.897</span> +<span class="st"> </span><span class="fl">1.293</span> *<span class="st"> </span>oP)
}
<span class="co"># Восстановление пропущенных значений PO4</span>
algae[<span class="kw">is.na</span>(algae$PO4), <span class="st">&#39;PO4&#39;</span>] &lt;-
<span class="st">    </span><span class="kw">sapply</span>(algae[<span class="kw">is.na</span>(algae$PO4), <span class="st">&#39;oPO4&#39;</span>], fillPO4)
algae[ind, <span class="dv">10</span>]</code></pre></div>
<pre><code>##  [1]  48.069   3.000   6.000   6.500   1.000   4.000   6.000  11.000
##  [9]   6.000  14.000  14.000   6.667  10.111 186.000  56.091      NA</code></pre>
<p>Одно из пропущенных значений удалось восстановить.</p>
<p>Разумеется, легко придти к мысли не утруждать себя перебором всех возможных корреляций, а учесть все связи одновременно и целиком. Использование метода <code>&quot;bagImpute&quot;</code> осуществляет для каждой из имеющихся переменных построение множественной бутстреп-агрегированной модели, или бэггинг-модели (bagging), на основе деревьев регрессии, используя все остальные переменные в качестве предикторов. Этот метод мудр и точен, но требует значительных затрат времени на вычисление, особенно при работе с данными большого объема:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ipred)
<span class="kw">data</span>(algae)
pPbI &lt;-<span class="st"> </span><span class="kw">preProcess</span>(algae[, <span class="dv">4</span>:<span class="dv">11</span>], <span class="dt">method =</span> <span class="st">&#39;bagImpute&#39;</span>)
algae[, <span class="dv">4</span>:<span class="dv">11</span>] &lt;-<span class="st"> </span><span class="kw">predict</span>(pPbI, algae[, <span class="dv">4</span>:<span class="dv">11</span>])
(Imp.Bag &lt;-<span class="st"> </span>algae[ind, <span class="dv">4</span>:<span class="dv">11</span>])</code></pre></div>
<pre><code>##         mxPH     mnO2        Cl      NO3      NH4      oPO4      PO4
## 28  6.800000 11.10000  9.000000 0.630000  20.0000   4.00000  31.3369
## 38  8.000000 10.47978  1.450000 0.810000  10.0000   2.50000   3.0000
## 48  8.030407 12.60000  9.000000 0.230000  10.0000   5.00000   6.0000
## 55  6.600000 10.80000 10.752686 3.245000  10.0000   1.00000   6.5000
## 56  5.600000 11.80000 10.752686 2.220000   5.0000   1.00000   1.0000
## 57  5.700000 10.80000 10.752686 2.550000  10.0000   1.00000   4.0000
## 58  6.600000  9.50000  8.943343 1.320000  20.0000   1.00000   6.0000
## 59  6.600000 10.80000 10.752686 2.640000  10.0000   2.00000  11.0000
## 60  6.600000 11.30000 10.752686 4.170000  10.0000   1.00000   6.0000
## 61  6.500000 10.40000 18.356586 5.970000  10.0000   2.00000  14.0000
## 62  6.400000 10.31829  8.943343 2.854249 211.2465  13.94070  14.0000
## 63  7.830000 11.70000  4.083000 1.328000  18.0000   3.33300   6.6670
## 116 9.700000 10.80000  0.222000 0.406000  10.0000  22.44400  10.1110
## 161 9.000000  5.80000 62.363149 0.900000 142.0000 102.00000 186.0000
## 184 8.000000 10.90000  9.055000 0.825000  40.0000  21.08300  56.0910
## 199 8.000000  7.60000 37.037966 3.290896 271.6944  68.15254 139.9038
##          Chla
## 28   2.700000
## 38   0.300000
## 48   1.100000
## 55   3.160793
## 56   3.160793
## 57   3.160793
## 58   3.160793
## 59   3.160793
## 60   3.160793
## 61   8.803676
## 62   3.160793
## 63   3.160793
## 116 65.724903
## 161 68.050000
## 184  3.804439
## 199 12.545207</code></pre>
<p>Наконец, третий метод функции <code>preProcess()</code> для заполнения пропусков - <code>&quot;knnImpute&quot;</code> - основан на простейшем, но чрезвычайно эффективном алгоритме <em>k</em> ближайших соседей (<em>k</em>-nearest neighbours) или kNN. В основе метода kNN лежит гипотеза о том, что тестируемый объект d будет иметь примерно тот же набор признаков, что и обучающие объекты в локальной области его ближайшего окружения (рис. <a href="ch-3.html#fig:fig-3-6">3.6</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:fig-3-6"></span>
<img src="figures/k_means.png" alt="Интерпретация метода *k* ближайших соседей" width="600px" />
<p class="caption">
Рисунок 3.6: Интерпретация метода <em>k</em> ближайших соседей
</p>
</div>
<p>Если речь идет о классификации, то неизвестный класс объекта определяется голосованием <span class="math inline">\(k\)</span> его ближайших соседей (на рис. <a href="ch-3.html#fig:fig-3-6">3.6</a> <span class="math inline">\(k = 5\)</span>). kNN-регрессия оценивает значение неизвестной координаты <span class="math inline">\(Y\)</span>, усредняя известные ее величины для тех же <span class="math inline">\(k\)</span> соседних точек.</p>
<p>Одна из важных проблем kNN - выбор метрики, на основе которой оценивается близость объектов. Наиболее общей формулой для подсчета расстояния в m-мерном пространстве между объектами <span class="math inline">\(\mathbf{X_1}\)</span> и <span class="math inline">\(\mathbf{X_2}\)</span> является мера Минковского: <span class="math display">\[ D_S (\mathbf{X_1}, \mathbf{X_2})  = (\sum |x_{1i} - x_{2i}|^p )^{1/r},\]</span></p>
<p>где <span class="math inline">\(i\)</span> изменяется от 1 до <span class="math inline">\(m\)</span>, а <span class="math inline">\(r\)</span> и <span class="math inline">\(p\)</span> - задаваемые исследователем параметры, с помощью которых можно осуществить нелинейное масштабирование расстояний между объектами. Мера расстояния по Евклиду получается, если принять в метрике Минковского <span class="math inline">\(r = p = 2\)</span>, и является, по-видимому, наиболее общей мерой расстояния, знакомой всем со школы по теореме Пифагора. При r = p = 1 имеем “манхеттенское расстояние” (или “расстояние городских кварталов”), не столь контрастно оценивающее большие разности координат x. Вторая проблема метода kNN заключается в решении вопроса о том, на мнение какого числа соседей k нам целесообразно положиться? В свое время мы обсудим этот вопрос детально, а сейчас будем ориентироваться на значение k = 5, используемое по умолчанию:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(algae)
pPkI &lt;-<span class="st"> </span><span class="kw">preProcess</span>(algae[, <span class="dv">4</span>:<span class="dv">11</span>], <span class="dt">method =</span> <span class="st">&#39;knnImpute&#39;</span>)
alg.stand &lt;-<span class="st"> </span><span class="kw">predict</span>(pPkI, algae[, <span class="dv">4</span>:<span class="dv">11</span>])</code></pre></div>
<p>Получив в результате применения <code>predict()</code> матрицу переменных с пропущенными значениями, заполненными этим методом, мы с удивлением обнаруживаем, что данные оказались стандартизованными (т.е. центрированными и нормированными на стандартное отклонение). Но а как иначе можно было вычислить меру близости по переменным, измеренным в разных шкалах? Пришлось для возвращения в исходное состояние применить обратную операцию:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span>pPkI$mean
sd &lt;-<span class="st"> </span>pPkI$std
algae[, <span class="dv">4</span>:<span class="dv">11</span>] &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(alg.stand, <span class="dv">1</span>, function (r) m +<span class="st"> </span>r *<span class="st"> </span>sd))
(Imp.Knn &lt;-<span class="st"> </span>algae[ind, <span class="dv">4</span>:<span class="dv">11</span>])</code></pre></div>
<pre><code>##      mxPH  mnO2      Cl    NO3     NH4     oPO4      PO4    Chla
## 28  6.800 11.10  9.0000 0.6300  20.000   4.0000  21.2400  2.7000
## 38  8.000  9.92  1.4500 0.8100  10.000   2.5000   3.0000  0.3000
## 48  7.762 12.60  9.0000 0.2300  10.000   5.0000   6.0000  1.1000
## 55  6.600 10.80  7.4974 3.2450  10.000   1.0000   6.5000  3.1800
## 56  5.600 11.80  7.4974 2.2200   5.000   1.0000   1.0000  3.1800
## 57  5.700 10.80  6.0474 2.5500  10.000   1.0000   4.0000  0.8600
## 58  6.600  9.50  5.3422 1.3200  20.000   1.0000   6.0000  0.9000
## 59  6.600 10.80  7.4974 2.6400  10.000   2.0000  11.0000  3.1800
## 60  6.600 11.30  7.7774 4.1700  10.000   1.0000   6.0000  3.9400
## 61  6.500 10.40 11.8864 5.9700  10.000   2.0000  14.0000  4.4000
## 62  6.400 10.24  4.4074 1.1638  60.040   5.0300  14.0000  0.8600
## 63  7.830 11.70  4.0830 1.3280  18.000   3.3330   6.6670  1.0200
## 116 9.700 10.80  0.2220 0.4060  10.000  22.4440  10.1110  5.5000
## 161 9.000  5.80 58.5314 0.9000 142.000 102.0000 186.0000 68.0500
## 184 8.000 10.90  9.0550 0.8250  40.000  21.0830  56.0910  0.9400
## 199 8.000  7.60 59.7922 3.6018 320.908 115.1684 185.9664 12.3958</code></pre>
<p>Наконец, зададимся следующим закономерным вопросом: а какой метод лучше? Обычно эта проблема не имеет теоретического решения, и исследователь полагается на собственную интуицию и опыт. Но мы можем оценить, насколько расходятся между собой результаты, полученные каждым способом заполнения. Для этого сформируем блок данных из <span class="math inline">\(3 \times 16 = 48\)</span> строк исходной таблицы с пропусками, заполненными тремя методами (<code>&quot;Med&quot;</code>, <code>&quot;Bag&quot;</code>, <code>&quot;Knn&quot;</code>), и выполним снижение размерности пространства переменных методом главных компонент в двумерное (см. раздел <a href="ch-2.html#sec_2_4">2.4</a>). Посмотрим, как “лягут карты” на плоскости (рис. <a href="ch-3.html#fig:fig-3-7">3.7</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ImpVal &lt;-<span class="st"> </span><span class="kw">rbind</span>(Imp.Med, Imp.Knn)
ImpVal &lt;-<span class="st"> </span><span class="kw">rbind</span>(ImpVal, Imp.Bag)
Imp.Metod &lt;-<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;Med&quot;</span>, <span class="dv">16</span>), <span class="kw">rep</span>(<span class="st">&quot;Knn&quot;</span>, <span class="dv">16</span>), <span class="kw">rep</span>(<span class="st">&quot;Bag&quot;</span>, <span class="dv">16</span>)))

<span class="kw">library</span>(vegan); <span class="kw">library</span>(RANN)
Imp.M &lt;-<span class="st"> </span><span class="kw">rda</span>(ImpVal ~<span class="st"> </span>Imp.Metod, ImpVal)
<span class="kw">plot</span>(Imp.M, <span class="dt">display =</span> <span class="st">&quot;sites&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;p&quot;</span>)
<span class="kw">ordihull</span>(Imp.M, Imp.Metod, <span class="dt">draw =</span> <span class="st">&quot;polygon&quot;</span>, <span class="dt">alpha =</span> <span class="dv">67</span>, 
         <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>), <span class="dt">label =</span> <span class="ot">TRUE</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-3-7"></span>
<img src="_main_files/figure-html/fig-3-7-1.png" alt="Ординационная диаграмма блоков данных таблицы `algae` с пропущенными значениями, заполненными разными способами" width="672" />
<p class="caption">
Рисунок 3.7: Ординационная диаграмма блоков данных таблицы <code>algae</code> с пропущенными значениями, заполненными разными способами
</p>
</div>
<p>На рис. <a href="ch-3.html#fig:fig-3-7">3.7</a> мы выделили контуром (hull), проведенным через крайние точки, области каждого из трех блоков данных и поместили метку метода в центры тяжести полученных многоугольников. Понятно, что медианное заполнение характеризуется меньшей вариацией результатов, поскольку игнорирует специфичность свойств каждого объекта. Оба других метода, учитывающих внутреннюю структуру данных, дали приблизительно похожие результаты.</p>

</div>
<div id="-train---caret" class="section level2">
<h2><span class="header-section-number">3.5</span> Функция <code>train()</code> из пакета <code id="sec_3_5">caret</code></h2>
<p>Как подчеркивалось ранее в разделе <a href="ch-3.html#sec_3_1">3.1</a>, пакет <code>caret</code> был разработан как эффективная надстройка, позволяющая унифицировать и интегрировать использование множества различных функций и методов построения моделей классификации и регрессии, представленных в других пакетах R. При этом происходит всестороннее тестирование и оптимизация настраиваемых коэффициентов и гиперпараметров моделей. Эта разработанная единая технология реализована в функции <code>train()</code>, использующей полуавтоматические интеллектуальные подходы и универсальные критерии качества с применением алгоритмов ресэмплинга.</p>
<p>Перед выполнением подгонки модели с помощью функции <code>train()</code> необходимо задать соответствующий алгоритм и всю совокупность условий процесса оптимизации параметров модели, для чего при помощи функции <code>trainControl()</code> создается специальный объект. Вызов этой функции со значениями, принимаемыми по умолчанию, имеет следующий вид:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;boot&quot;</span>, 
             <span class="dt">number =</span> <span class="kw">ifelse</span>(<span class="kw">grepl</span>(<span class="st">&quot;cv&quot;</span>, method), <span class="dv">10</span>, <span class="dv">25</span>), <span class="dt">p =</span> <span class="fl">0.75</span>,
             <span class="dt">repeats =</span> <span class="kw">ifelse</span>(<span class="kw">grepl</span>(<span class="st">&quot;cv&quot;</span>, method), <span class="dv">1</span>, number),
             <span class="dt">search =</span> <span class="st">&quot;grid&quot;</span>, <span class="dt">initialWindow =</span> <span class="ot">NULL</span>, <span class="dt">horizon =</span> <span class="dv">1</span>,
             <span class="dt">fixedWindow =</span> <span class="ot">TRUE</span>, <span class="dt">verboseIter =</span> <span class="ot">FALSE</span>, <span class="dt">returnData =</span> <span class="ot">TRUE</span>,
             <span class="dt">returnResamp =</span> <span class="st">&quot;final&quot;</span>, <span class="dt">savePredictions =</span> <span class="ot">FALSE</span>, 
             <span class="dt">classProbs =</span> <span class="ot">FALSE</span>, <span class="dt">summaryFunction =</span> defaultSummary,
             <span class="dt">selectionFunction =</span> <span class="st">&quot;best&quot;</span>, <span class="dt">seeds =</span> <span class="ot">NA</span>,
             <span class="dt">preProcOptions =</span> <span class="kw">list</span>(<span class="dt">thresh =</span> <span class="fl">0.95</span>, <span class="dt">ICAcomp =</span> <span class="dv">3</span>, <span class="dt">k =</span> <span class="dv">5</span>),
             <span class="dt">sampling =</span> <span class="ot">NULL</span>, <span class="dt">index =</span> <span class="ot">NULL</span>, <span class="dt">indexOut =</span> <span class="ot">NULL</span>,
             <span class="dt">timingSamps =</span> <span class="dv">0</span>, <span class="dt">predictionBounds =</span> <span class="kw">rep</span>(<span class="ot">FALSE</span>, <span class="dv">2</span>), 
             <span class="dt">adaptive =</span> <span class="kw">list</span>(<span class="dt">min =</span> <span class="dv">5</span>, <span class="dt">alpha =</span> <span class="fl">0.05</span>, <span class="dt">method =</span> <span class="st">&quot;gls&quot;</span>, <span class="dt">complete =</span> <span class="ot">TRUE</span>),
             <span class="dt">trim =</span> <span class="ot">FALSE</span>, <span class="dt">allowParallel =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>Остановимся на описании наиболее важных аргументов этой функции:</p>
<ul>
<li><code>method</code> - метод ресэмплинга: <code>&quot;boot&quot;</code>, <code>&quot;boot632&quot;</code>, <code>&quot;cv&quot;</code>, <code>&quot;repeatedcv&quot;</code>, <code>&quot;LOOCV&quot;</code>, <code>&quot;LGOCV&quot;</code> (для повторяющихся разбиений на обучающую и проверочную выборки), <code>&quot;none&quot;</code> (исследуется только модель на обучающей выборке), <code>&quot;oob&quot;</code> (для таких алгоритмов, как случайный лес, бэггинг деревьев и др.), <code>&quot;adaptive_cv&quot;</code>, <code>&quot;adaptive_boot&quot;</code> или <code>&quot;adaptive_LGOCV&quot;</code>;</li>
<li><code>number</code> - задает число итераций ресэмплинга, в частности, число блоков (<code>folds</code>) при перекрестной проверке;</li>
<li><code>repeats</code> - число повторностей (только для <em>k</em>-кратной перекрестной проверки);</li>
<li><code>p</code> - доля обучающей выборки от общего объема при выполнении перекрестной проверки;</li>
<li><code>verboseIter</code> - <code>TRUE</code> означает, что в ходе вычислений <code>caret</code> будет показывать, на каком этапе они находятся (это удобно для оценки оставшегося времени вычислений, которое часто бывает очень большим);</li>
<li><code>search</code> - способ перебора параметров модели (по сетке - <code>&quot;grid&quot;</code>, или случайным назначением - <code>&quot;random&quot;</code>);</li>
<li><code>returnResamp</code> и <code>savePredictions</code> - определяют условия сохранения результатов выполнения ресэмплинга и прогнозируемых значений, т.е. <code>&quot;none&quot;</code>, только для итоговой модели <code>&quot;final&quot;</code> или все <code>&quot;all&quot;</code>;</li>
<li><code>classProbs</code> - <code>TRUE</code> означает, что в процессе вычислений алгоритм будет сохранять данные о вероятностях попадания объекта в каждый класс, а не только конечные метки класса;</li>
<li><code>summaryFunction</code> - определяет функцию, которая вычисляет метрику качества модели при ресэмплинге;</li>
<li><code>selectionFunction</code> - определяет функцию выбора оптимального значения настраиваемого параметра;</li>
<li><code>preProcOptions</code> - список опций, который передается функции предобработки данных <code>preProcess()</code>.</li>
</ul>
<p>Например, при создании объекта <code>ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10)</code> параметры перекрестной проверки будут иметь следующий смысл:</p>
<ul>
<li><code>method = &quot;repeatedcv&quot;</code> означает, что будет использоваться повторная перекрестная проверка (также возможна перекрестная проверка, leave-one-out проверка, и т.д.);</li>
<li><code>number = 10</code> означает, что в процессе перекрестной проверки выборку надо разбивать на 10 равных частей;</li>
<li><code>repeats = 10</code> означает, что повторная перекрестная проверка должна быть запущена 10 раз.</li>
</ul>
<p>Теперь перейдем непосредственно к описанию функции <code>train()</code>, которая имеет следующий формат вызова:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">train</span>(x, y, 
      <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>, 
      <span class="dt">preProcess =</span> <span class="ot">NULL</span>, 
      <span class="dt">weights =</span> <span class="ot">NULL</span>,
      <span class="dt">metric =</span> <span class="kw">ifelse</span>(<span class="kw">is.factor</span>(y), <span class="st">&quot;Accuracy&quot;</span>, <span class="st">&quot;RMSE&quot;</span>), 
      <span class="dt">maximize =</span> <span class="kw">ifelse</span>(metric %in%<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;RMSE&quot;</span>, <span class="st">&quot;logLoss&quot;</span>), <span class="ot">FALSE</span>, <span class="ot">TRUE</span>), 
      <span class="dt">trControl =</span> <span class="kw">trainControl</span>(),
      <span class="dt">tuneGrid =</span> <span class="ot">NULL</span>,
      <span class="dt">tuneLength =</span> <span class="dv">3</span>)</code></pre></div>
<p>Исходные данные задаются, как всегда, либо матрицей предикторов x и вектором отклика y, либо объектом formula с указанием таблицы данных data. Следующий аргумент - <code>method</code> - это, в сущности, название модели классификации или регрессии, которую необходимо построить и протестировать. Если выполнить команду names(<code>getModelInfo()</code>), то можно увидеть список из 233 доступных методов (это количество постоянно расширяется). Тот же список, но с различными возможностями поиска и сортировки можно посмотреть по следующим адресам в Интернете:</p>
<p><a href="http://topepo.github.io/caret/modelList.html" class="uri">http://topepo.github.io/caret/modelList.html</a> или<br />
<a href="http://topepo.github.io/caret/bytag.html" class="uri">http://topepo.github.io/caret/bytag.html</a></p>
<p>Можно просмотреть названия всех моделей, которые имеют, например, отношение к линейной регрессии:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
<span class="kw">ls</span>(<span class="kw">getModelInfo</span>(<span class="dt">model =</span> <span class="st">&quot;lm&quot;</span>))</code></pre></div>
<pre><code>##  [1] &quot;bayesglm&quot;       &quot;elm&quot;            &quot;glm&quot;            &quot;glm.nb&quot;        
##  [5] &quot;glmboost&quot;       &quot;glmnet&quot;         &quot;glmnet_h2o&quot;     &quot;glmStepAIC&quot;    
##  [9] &quot;lm&quot;             &quot;lmStepAIC&quot;      &quot;plsRglm&quot;        &quot;rlm&quot;           
## [13] &quot;vglmAdjCat&quot;     &quot;vglmContRatio&quot;  &quot;vglmCumulative&quot;</code></pre>
<p>С каждым методом связан набор гиперпараметров, подлежащих оптимизации. Можно убедиться в том, что простая линейная регрессия (<code>method = &quot;lm&quot;</code>) не имеет параметров для настройки:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">modelLookup</span>(<span class="st">&quot;lm&quot;</span>)</code></pre></div>
<pre><code>##   model parameter     label forReg forClass probModel
## 1    lm intercept intercept   TRUE    FALSE     FALSE</code></pre>
<p>В свою очередь, деревья решений <code>rpart</code>, которые мы рассмотрим позднее, имеют один параметр для настройки - Complexity Parameter (аббревиатура - <code>cp</code>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">modelLookup</span>(<span class="st">&quot;rpart&quot;</span>)</code></pre></div>
<pre><code>##   model parameter                label forReg forClass probModel
## 1 rpart        cp Complexity Parameter   TRUE     TRUE      TRUE</code></pre>
<p>Из сообщений функции <code>modelLookup()</code> можно также увидеть, что линейная регрессия не используется для классификации (<code>forClass= FALSE</code>), тогда как <code>rpart</code> (от “Recursive Partitioning and Regression Trees”) можно применять как для построения деревьев регрессии, так и классификации. В последнем случае модель осуществляет не только предсказание класса, но и оценивает апостериорные вероятности (<code>probModel = TRUE</code>).</p>
<p>Метод перекрестной проверки, заданный объектом <code>trControl = trainControl()</code>, обеспечивает сканирование настраиваемых параметров и оценку эффективности модели на каждой итерации по определенным критериям качества. При построении каждой частной модели предварительно осуществляется предобработка данных с использованием методов, перечисленных в <code>preProcess</code> (и с учетом опций <code>preProcOptions</code> объекта <code>trControl</code>).</p>
<p>По умолчанию аргумент <code>metric</code> использует в качестве критерия качества точность предсказания (<code>&quot;Accuracy&quot;</code>) в случае классификации (см. пример, рассмотренный в разделе <a href="ch-3.html#sec_3_3">3.3</a>) и корень из среднеквадратичного отклонения прогнозируемых значений от наблюдаемых (<code>&quot;RMSE&quot;</code>) для регрессии. Логический аргумент <code>maximize</code> уточняет, должен ли этот критерий быть максимизирован или минимизирован. Другие значения <code>metric</code> вместе с различными значениями аргументов <code>summaryFunction</code> и <code>selectionFunction</code> объекта <code>trControl</code> обеспечивают широкие возможности пользовательского назначения метрик для оценки эффективности моделей.</p>
<p>Количество перебираемых значений настраиваемого параметра задается аргументом <code>tuneLength</code>. Например, чтобы задать 30 повторов оценки параметра <code>cр</code> модели <code>rpart</code> вместо исходных трех, необходимо указать <code>tuneLength = 30</code>. Другой вариант - определить последовательность этих значений в списке, задающем сетку: например, <code>tuneGrid = expand.grid(.cp = 0.5^(1:10))</code>, если заранее известен диапазон, к которому принадлежит оптимизируемое значение.</p>
<p>После завершения перебора всех положенных комбинаций параметров модели создается объект класса <code>train</code>, соответствующие элементы которого можно извлечь с помощью суффикса <code>$</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ls</span>(mytrain)</code></pre></div>
<p>Приведем краткий пример на тему подбора полиномиальной регрессии для описания зависимости электрического сопротивления (Ом) мякоти фруктов киви от процентного содержания в ней сока, который подробно разбирался нами в разделе <a href="ch-2.html#sec_2_1">2.1</a>. Позже (раздел <a href="ch-2.html#sec_2_2">2.2</a>) мы показали, как найти оптимальную степень полинома <span class="math inline">\(d = 4\)</span> с использованием написанной нами функции скользящего контроля.</p>
<p>К сожалению, функция <code>train()</code> селекцию предикторов для метода <code>&quot;lm&quot;</code> не выполняет и оптимальную степень полинома не настраивает. Но мы можем выполнить любую перекрестную проверку для оценки динамики изменения критериев качества модели – среднеквадратичной ошибки <code>RMSE</code> и среднего коэффициента детерминации <code>RSquared</code> (рис. <a href="ch-3.html#fig:fig-3-8">3.8</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(DAAG)
<span class="kw">data</span>(<span class="st">&quot;fruitohms&quot;</span>)
<span class="kw">set.seed</span>(<span class="dv">123</span>) 
max.poly &lt;-<span class="st"> </span><span class="dv">7</span>
degree &lt;-<span class="st"> </span><span class="dv">1</span>:max.poly
RSquared &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, max.poly)
RMSE &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, max.poly)

<span class="co"># Выполним 10-кратную кросс-проверку с 10 повторностями</span>
fitControl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,
                           <span class="dt">number =</span> <span class="dv">10</span>, <span class="dt">repeats =</span> <span class="dv">10</span>)

<span class="co"># Тестируем модель для различных степеней:</span>
for (d in degree)  {
    f &lt;-<span class="st"> </span><span class="kw">bquote</span>(juice ~<span class="st"> </span><span class="kw">poly</span>(ohms, .(d)))
    LinearRegressor &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="kw">as.formula</span>(f),
                             <span class="dt">data =</span> fruitohms,
                             <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">trControl =</span> fitControl)
    RSquared[d] &lt;-<span class="st"> </span>LinearRegressor$results$Rsquared
    RMSE[d] &lt;-<span class="st"> </span>LinearRegressor$results$RMSE
}

<span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(gridExtra)
Degree.RegParams &lt;-<span class="st"> </span><span class="kw">data.frame</span>(degree, RSquared, RMSE)
a &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> degree, <span class="dt">y =</span> RSquared),
       <span class="dt">data =</span> Degree.RegParams) +<span class="st"> </span><span class="kw">geom_line</span>()
b &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> degree, <span class="dt">y =</span> RMSE),
       <span class="dt">data =</span> Degree.RegParams) +<span class="st"> </span><span class="kw">geom_line</span>()
<span class="kw">grid.arrange</span>(a, b, <span class="dt">ncol =</span> <span class="dv">2</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-3-8"></span>
<img src="_main_files/figure-html/fig-3-8-1.png" alt="Поиск степени функции полиномиальной регрессии с использованием функции `train()`" width="768" />
<p class="caption">
Рисунок 3.8: Поиск степени функции полиномиальной регрессии с использованием функции <code>train()</code>
</p>
</div>
<p>В отличие от ранее проведенных расчетов, минимум ошибки и максимум коэффициента детерминации имеют место при <span class="math inline">\(d = 5\)</span>.</p>
<p>Если не задавать непосредственно объект <code>trControl</code>, то по умолчанию вместо кросс-проверки функция <code>train()</code> осуществляет бутстреп критериев качества подгонки <code>RMSE</code> и <code>RSquared</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Poly5 &lt;-<span class="st"> </span><span class="kw">train</span>(ohms ~<span class="st"> </span><span class="kw">poly</span>(juice,<span class="dv">5</span>), <span class="dt">data =</span> fruitohms, <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)
<span class="kw">summary</span>(Poly5$finalModel)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3363.7  -508.9   -35.8   459.7  2797.7 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)         4360.0       83.2  52.406  &lt; 2e-16 ***
## `poly(juice, 5)1` -16750.2      941.3 -17.796  &lt; 2e-16 ***
## `poly(juice, 5)2`   4750.6      941.3   5.047 1.59e-06 ***
## `poly(juice, 5)3`   3945.6      941.3   4.192 5.27e-05 ***
## `poly(juice, 5)4`  -3371.2      941.3  -3.582 0.000492 ***
## `poly(juice, 5)5`   1057.3      941.3   1.123 0.263509    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 941.3 on 122 degrees of freedom
## Multiple R-squared:  0.7539, Adjusted R-squared:  0.7439 
## F-statistic: 74.76 on 5 and 122 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">lm</span>(ohms ~<span class="st"> </span><span class="kw">poly</span>(juice, <span class="dv">5</span>), <span class="dt">data =</span> fruitohms))</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = ohms ~ poly(juice, 5), data = fruitohms)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3363.7  -508.9   -35.8   459.7  2797.7 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       4360.0       83.2  52.406  &lt; 2e-16 ***
## poly(juice, 5)1 -16750.2      941.3 -17.796  &lt; 2e-16 ***
## poly(juice, 5)2   4750.6      941.3   5.047 1.59e-06 ***
## poly(juice, 5)3   3945.6      941.3   4.192 5.27e-05 ***
## poly(juice, 5)4  -3371.2      941.3  -3.582 0.000492 ***
## poly(juice, 5)5   1057.3      941.3   1.123 0.263509    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 941.3 on 122 degrees of freedom
## Multiple R-squared:  0.7539, Adjusted R-squared:  0.7439 
## F-statistic: 74.76 on 5 and 122 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Poly5</code></pre></div>
<pre><code>## Linear Regression 
## 
## 128 samples
##   1 predictor
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 128, 128, 128, 128, 128, 128, ... 
## Resampling results:
## 
##   RMSE      Rsquared 
##   981.9573  0.7238476
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE
## </code></pre>
<p>Мы получили в точности те же коэффициенты модели, что и при использовании базовой функции <code>lm()</code>, но для критериев <code>RMSE</code> и <code>RSquared</code> были найдены стандартные ошибки, позволяющие рассчитать доверительные интервалы этих статистик. Оценка проводилась по результатам 25 бутстреп-итераций, выполняемых функцией <code>train()</code> по умолчанию. Обратите внимание, что несмещенное бутстреп-значение коэффициента детерминации (0.734) несколько меньше, чем рассчитанное функцией <code>summary()</code> для финальной модели (0.754). В заключении приведем сокращенную таблицу доступных в <code>train()</code> моделей с указанием наименований их параметров. С нашей точки зрения, таблица полезна также тем, что является своеобразным путеводителем по пакетам R и реализованным в них статистическим методам. В последующих разделах мы приведем описание большинства перечисленных методов и продемонстрируем процесс оптимизации их параметров с помощью функции <code>train()</code>.</p>
<p>Список методов, моделей и их параметров, оптимизируемых с использованием функции <code>train()</code></p>
<table>
<thead>
<tr class="header">
<th align="left">﻿Модели</th>
<th align="center">Значение method</th>
<th align="center">Пакет</th>
<th align="center">Оптимизируемые параметры</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Деревья на основе рекурсивного деления (recursive partitioning)</td>
<td align="center">rpart</td>
<td align="center">rpart</td>
<td align="center">maxdepth</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">ctree</td>
<td align="center">party</td>
<td align="center">mincriterion</td>
</tr>
<tr class="odd">
<td align="left">Бустинг деревьев (boosted trees)</td>
<td align="center">gbm</td>
<td align="center">gbm</td>
<td align="center">interaction.depth, n.trees, shrinkage</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">blackboost</td>
<td align="center">mboost</td>
<td align="center">maxdepth, mstop</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="center">ada</td>
<td align="center">ada</td>
<td align="center">maxdepth, iter, nu</td>
</tr>
<tr class="even">
<td align="left">Другие модели бустинга (other boosted models)</td>
<td align="center">glmboost</td>
<td align="center">mboost</td>
<td align="center">mstop</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="center">gamboost</td>
<td align="center">mboost</td>
<td align="center">mstop</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">logitboost</td>
<td align="center">caTools</td>
<td align="center">nIter</td>
</tr>
<tr class="odd">
<td align="left">Случайный лес (random forest)</td>
<td align="center">rf</td>
<td align="center">randomForest</td>
<td align="center">mtry</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center">cforest</td>
<td align="center">party</td>
<td align="center">mtry</td>
</tr>
<tr class="odd">
<td align="left">Бэггинг-деревья (bagged trees)</td>
<td align="center">treebag</td>
<td align="center">ipred</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">Нейронные сети (neural networks)</td>
<td align="center">nnet</td>
<td align="center">nnet</td>
<td align="center">decay, size</td>
</tr>
<tr class="odd">
<td align="left">Частные наименьшие квадраты (partial least squares)</td>
<td align="center">pls</td>
<td align="center">pls, caret</td>
<td align="center">ncomp</td>
</tr>
<tr class="even">
<td align="left">Машина опорных векторов с RBF ядром (support vector machines RBF kernel)</td>
<td align="center">svmRadial</td>
<td align="center">kernlab</td>
<td align="center">sigma, C</td>
</tr>
<tr class="odd">
<td align="left">Машина опорных векторов с полиномиальным ядром (support vector machines polynomial kernel)</td>
<td align="center">svmPoly</td>
<td align="center">kernlab</td>
<td align="center">scale, degree, C</td>
</tr>
<tr class="even">
<td align="left">Гауссовы процессы с RBF ядром (Gaussian processes with RBF kernel)</td>
<td align="center">gaussprRadial</td>
<td align="center">kernlab</td>
<td align="center">sigma</td>
</tr>
<tr class="odd">
<td align="left">Гауссовы процессы с полиномиальным ядром (Gaussian processes with polynomial kernel)</td>
<td align="center">gaussprPoly</td>
<td align="center">kernlab</td>
<td align="center">scale, degree</td>
</tr>
<tr class="even">
<td align="left">Линейные модели наименьших квадратов (linear least squares)</td>
<td align="center">lm</td>
<td align="center">stats</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">Многомерные адаптивные регрессионные сплайны (multivariate adaptive regression splines MARS)</td>
<td align="center">earth, mars</td>
<td align="center">earth</td>
<td align="center">degree, nprune</td>
</tr>
<tr class="even">
<td align="left">Бэггинг-сплайны MARS (bagged MARS)</td>
<td align="center">bagEarth</td>
<td align="center">caret, earth</td>
<td align="center">degree, nprune</td>
</tr>
<tr class="odd">
<td align="left">Эластичные сети (elastic net)</td>
<td align="center">enet</td>
<td align="center">elasticnet</td>
<td align="center">lambda, fraction</td>
</tr>
<tr class="even">
<td align="left">Лассо (the lasso)</td>
<td align="center">lasso</td>
<td align="center">elasticnet</td>
<td align="center">fraction</td>
</tr>
<tr class="odd">
<td align="left">Машины релевантных векторов с RBF ядром (relevance vector machines RBF kernel)</td>
<td align="center">rvmRadial</td>
<td align="center">kernlab</td>
<td align="center">sigma</td>
</tr>
<tr class="even">
<td align="left">Машины релевантных векторов с полиномиальным ядром (relevance vector machines polynomial kernel</td>
<td align="center">rvmPoly</td>
<td align="center">kernlab</td>
<td align="center">scale, degree</td>
</tr>
<tr class="odd">
<td align="left">Линейный дискриминантный анализ (linear discriminant analysis)</td>
<td align="center">lda</td>
<td align="center">MASS</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">Пошаговый диагональный дискриминантный анализ (stepwise diagonal discriminant analysis)</td>
<td align="center">sddaLDA, sddaQDA</td>
<td align="center">SDDA</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">Логистическая регрессия для двух или более классов (logistic/multinomial regression)</td>
<td align="center">multinom</td>
<td align="center">nnet</td>
<td align="center">decay</td>
</tr>
<tr class="even">
<td align="left">Регуляризованный дискриминантный анализ (Regularized discriminant analysis)</td>
<td align="center">rda</td>
<td align="center">klaR</td>
<td align="center">lambda, gamma</td>
</tr>
<tr class="odd">
<td align="left">Гибкий дискриминантный анализ (Flexible discriminant analysis FDA)</td>
<td align="center">fda</td>
<td align="center">mda, earth</td>
<td align="center">degree, nprune</td>
</tr>
<tr class="even">
<td align="left">FDA на основе бэггинга (bagged FDA)</td>
<td align="center">bagFDA</td>
<td align="center">caret, earth</td>
<td align="center">degree, nprune</td>
</tr>
<tr class="odd">
<td align="left">Машины опорных векторов на основе метода наименьших квадратов c RBF ядром (least squares support vector machines RBF kernel)</td>
<td align="center">lssvmRadial</td>
<td align="center">kernlab</td>
<td align="center">sigma</td>
</tr>
<tr class="even">
<td align="left">Метод k-ближайших соседей (k nearest neighbors)</td>
<td align="center">knnЗ</td>
<td align="center">caret</td>
<td align="center">k</td>
</tr>
<tr class="odd">
<td align="left">Разделение по центроидам (nearest shrunken centroids)</td>
<td align="center">pam</td>
<td align="center">pamr</td>
<td align="center">threshold</td>
</tr>
<tr class="even">
<td align="left">Наивный байесовский классификатор (naive Bayes)</td>
<td align="center">nb</td>
<td align="center">klaR</td>
<td align="center">usekernel</td>
</tr>
<tr class="odd">
<td align="left">Обобщенный метод частных наименьших квадратов (Generalized partial least squares)</td>
<td align="center">gpls</td>
<td align="center">gpls</td>
<td align="center">K.prov</td>
</tr>
<tr class="even">
<td align="left">Сети с квантованием обучающего вектора (learned vector quantization)</td>
<td align="center">lvq</td>
<td align="center">class</td>
<td align="center">k</td>
</tr>
</tbody>
</table>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-4.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
