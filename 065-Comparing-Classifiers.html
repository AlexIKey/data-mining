<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Классификация, регрессия и другие алгоритмы Data Mining с использованием R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Реализация алгоритмов Data Mining с использованием R">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Классификация, регрессия и другие алгоритмы Data Mining с использованием R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://ranalytics.github.io/data-mining/" />
  
  <meta property="og:description" content="Реализация алгоритмов Data Mining с использованием R" />
  <meta name="github-repo" content="ranalytics/data-mining" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Классификация, регрессия и другие алгоритмы Data Mining с использованием R" />
  
  <meta name="twitter:description" content="Реализация алгоритмов Data Mining с использованием R" />
  

<meta name="author" content="Шитиков В. К., Мастицкий С. Э.">


<meta name="date" content="2017-04-06">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="064-Classification-Trees.html">
<link rel="next" href="071-Multiclass-Classification.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Аннотация</a></li>
<li class="chapter" data-level="1" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html"><i class="fa fa-check"></i><b>1</b> Реализация моделей Data Mining в среде R (вместо предисловия)</a><ul>
<li class="chapter" data-level="1.1" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#section_1_1"><i class="fa fa-check"></i><b>1.1</b> Data Mining как направление анализа данных</a><ul>
<li class="chapter" data-level="1.1.1" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_1"><i class="fa fa-check"></i><b>1.1.1</b> От статистического анализа разового эксперимента к Data Mining</a></li>
<li class="chapter" data-level="1.1.2" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_2"><i class="fa fa-check"></i><b>1.1.2</b> Принципиальная множественность моделей окружающего мира</a></li>
<li class="chapter" data-level="1.1.3" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_3"><i class="fa fa-check"></i><b>1.1.3</b> Нарастающая множественность алгоритмов построения моделей</a></li>
<li class="chapter" data-level="1.1.4" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_4"><i class="fa fa-check"></i><b>1.1.4</b> Типы и характеристики групп моделей Data Mining</a></li>
<li class="chapter" data-level="1.1.5" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_5"><i class="fa fa-check"></i><b>1.1.5</b> Природа многомерного отклика и его моделирование</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="012-R-Intro.html"><a href="012-R-Intro.html"><i class="fa fa-check"></i><b>1.2</b> Статистическая среда R и ее использование в Data Mining</a></li>
<li class="chapter" data-level="1.3" data-path="013-What-This-Book-Is-About.html"><a href="013-What-This-Book-Is-About.html"><i class="fa fa-check"></i><b>1.3</b> О чем эта книга и чего в ней нет</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="021-Model-Quality-Criteria.html"><a href="021-Model-Quality-Criteria.html"><i class="fa fa-check"></i><b>2</b> Статистические модели: критерии и методы оценивания их качества</a><ul>
<li class="chapter" data-level="2.1" data-path="021-Model-Quality-Criteria.html"><a href="021-Model-Quality-Criteria.html#sec_2_1"><i class="fa fa-check"></i><b>2.1</b> Основные шаги построения и верификации моделей</a></li>
<li class="chapter" data-level="2.2" data-path="022-Resampling-Techniques.html"><a href="022-Resampling-Techniques.html"><i class="fa fa-check"></i><b>2.2</b> Использование алгоритмов ресэмплинга для тестирования моделей и оптимизации их параметров</a></li>
<li class="chapter" data-level="2.3" data-path="023-Models-for-Class-Prediction.html"><a href="023-Models-for-Class-Prediction.html"><i class="fa fa-check"></i><b>2.3</b> Модели для предсказания класса объектов</a></li>
<li class="chapter" data-level="2.4" data-path="024-Projecting-Data-onto-a-Plane.html"><a href="024-Projecting-Data-onto-a-Plane.html"><i class="fa fa-check"></i><b>2.4</b> Проецирование многомерных данных на плоскости</a></li>
<li class="chapter" data-level="2.5" data-path="025-MV-analysis.html"><a href="025-MV-analysis.html"><i class="fa fa-check"></i><b>2.5</b> Многомерный статистический анализ данных</a></li>
<li class="chapter" data-level="2.6" data-path="026-Clustering-Methods.html"><a href="026-Clustering-Methods.html"><i class="fa fa-check"></i><b>2.6</b> Методы кластеризации</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="031-Intro-to-Caret.html"><a href="031-Intro-to-Caret.html"><i class="fa fa-check"></i><b>3</b> Пакет <code>caret</code> - инструмент построения статистических моделей в R</a><ul>
<li class="chapter" data-level="3.1" data-path="031-Intro-to-Caret.html"><a href="031-Intro-to-Caret.html#---------caret"><i class="fa fa-check"></i><b>3.1</b> Универсальный интерфейс доступа к функциям машинного обучения в пакете <code id="sec_3_1">caret</code></a></li>
<li class="chapter" data-level="3.2" data-path="032-Removing-Predictors.html"><a href="032-Removing-Predictors.html"><i class="fa fa-check"></i><b>3.2</b> Обнаружение и удаление “ненужных” предикторов</a></li>
<li class="chapter" data-level="3.3" data-path="033-Preprocessing.html"><a href="033-Preprocessing.html"><i class="fa fa-check"></i><b>3.3</b> Предварительная обработка: преобразование и групповая трансформация переменных</a></li>
<li class="chapter" data-level="3.4" data-path="034-Handling-Missing-Values.html"><a href="034-Handling-Missing-Values.html"><i class="fa fa-check"></i><b>3.4</b> Заполнение пропущенных значений в данных</a></li>
<li class="chapter" data-level="3.5" data-path="035-The-train-Functions.html"><a href="035-The-train-Functions.html"><i class="fa fa-check"></i><b>3.5</b> Функция <code>train()</code> из пакета <code id="sec_3_5">caret</code></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html"><i class="fa fa-check"></i><b>4</b> Построение регрессионных моделей различного типа</a><ul>
<li class="chapter" data-level="4.1" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1"><i class="fa fa-check"></i><b>4.1</b> Селекция оптимального набора предикторов линейной модели</a><ul>
<li class="chapter" data-level="4.1.1" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_1"><i class="fa fa-check"></i><b>4.1.1</b> Полная регрессионная модель и пошаговая процедура</a></li>
<li class="chapter" data-level="4.1.2" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_2"><i class="fa fa-check"></i><b>4.1.2</b> Рекурсивное исключение переменных</a></li>
<li class="chapter" data-level="4.1.3" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_3"><i class="fa fa-check"></i><b>4.1.3</b> Генетический алгоритм</a></li>
<li class="chapter" data-level="4.1.4" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_4"><i class="fa fa-check"></i><b>4.1.4</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="042-Regularization.html"><a href="042-Regularization.html"><i class="fa fa-check"></i><b>4.2</b> Регуляризация, частные наименьшие квадраты и kNN-регрессия</a><ul>
<li class="chapter" data-level="4.2.1" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_1"><i class="fa fa-check"></i><b>4.2.1</b> Регрессия по методу “лассо”</a></li>
<li class="chapter" data-level="4.2.2" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_2"><i class="fa fa-check"></i><b>4.2.2</b> Метод частных наименьших квадратов (PLS)</a></li>
<li class="chapter" data-level="4.2.3" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_3"><i class="fa fa-check"></i><b>4.2.3</b> Регрессия по методу <em>k</em> ближайших соседей</a></li>
<li class="chapter" data-level="4.2.4" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_4"><i class="fa fa-check"></i><b>4.2.4</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html"><i class="fa fa-check"></i><b>4.3</b> Построение деревьев регрессии</a><ul>
<li class="chapter" data-level="4.3.1" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html#sec_4_3_1"><i class="fa fa-check"></i><b>4.3.1</b> Построение деревьев на основе рекурсивного разбиения</a></li>
<li class="chapter" data-level="4.3.2" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html#sec_4_3_2"><i class="fa fa-check"></i><b>4.3.2</b> Построение деревьев с использованием алгортма условного вывода</a></li>
<li class="chapter" data-level="4.3.3" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html#sec_4_3_3"><i class="fa fa-check"></i><b>4.3.3</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="044-Ensembles.html"><a href="044-Ensembles.html"><i class="fa fa-check"></i><b>4.4</b> Ансамбли моделей: бэггинг, случайные леса, бустинг</a><ul>
<li class="chapter" data-level="4.4.1" data-path="044-Ensembles.html"><a href="044-Ensembles.html#sec_4_4_1"><i class="fa fa-check"></i><b>4.4.1</b> Бэггинг и случайные леса</a></li>
<li class="chapter" data-level="4.4.2" data-path="044-Ensembles.html"><a href="044-Ensembles.html#sec_4_4_2"><i class="fa fa-check"></i><b>4.4.2</b> Бустинг</a></li>
<li class="chapter" data-level="4.4.3" data-path="044-Ensembles.html"><a href="044-Ensembles.html#sec_4_4_3"><i class="fa fa-check"></i><b>4.4.3</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="045-Comparing-Trees.html"><a href="045-Comparing-Trees.html"><i class="fa fa-check"></i><b>4.5</b> Сравнение построенных моделей и оценка информативности предикторов</a></li>
<li class="chapter" data-level="4.6" data-path="046-MV-Trees.html"><a href="046-MV-Trees.html"><i class="fa fa-check"></i><b>4.6</b> Деревья регрессии с многомерным откликом</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="051-Association-Rules.html"><a href="051-Association-Rules.html"><i class="fa fa-check"></i><b>5</b> Бинарные матрицы и ассоциативные правила</a><ul>
<li class="chapter" data-level="5.1" data-path="051-Association-Rules.html"><a href="051-Association-Rules.html#sec_5_1"><i class="fa fa-check"></i><b>5.1</b> Классификация в бинарных пространствах с использованием классических моделей</a></li>
<li class="chapter" data-level="5.2" data-path="052-Binary-Decision-Trees.html"><a href="052-Binary-Decision-Trees.html"><i class="fa fa-check"></i><b>5.2</b> Бинарные деревья решений</a></li>
<li class="chapter" data-level="5.3" data-path="053-Logic-Rules.html"><a href="053-Logic-Rules.html"><i class="fa fa-check"></i><b>5.3</b> Поиск логических закономерностей в данных</a></li>
<li class="chapter" data-level="5.4" data-path="054-Association-Rules-Algos.html"><a href="054-Association-Rules-Algos.html"><i class="fa fa-check"></i><b>5.4</b> Алгоритмы выделения ассоциативных правил</a></li>
<li class="chapter" data-level="5.5" data-path="055-Traminer.html"><a href="055-Traminer.html"><i class="fa fa-check"></i><b>5.5</b> Анализ последовательностей знаков или событий</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="061-Binary-Classifiers.html"><a href="061-Binary-Classifiers.html"><i class="fa fa-check"></i><b>6</b> Бинарные классификаторы с различными разделяющими поверхностями</a><ul>
<li class="chapter" data-level="6.1" data-path="061-Binary-Classifiers.html"><a href="061-Binary-Classifiers.html#sec_6_1"><i class="fa fa-check"></i><b>6.1</b> Дискриминантный анализ</a></li>
<li class="chapter" data-level="6.2" data-path="062-SVM.html"><a href="062-SVM.html"><i class="fa fa-check"></i><b>6.2</b> Дискриминантный анализ</a></li>
<li class="chapter" data-level="6.3" data-path="063-Nonlinear-Borders.html"><a href="063-Nonlinear-Borders.html"><i class="fa fa-check"></i><b>6.3</b> Ядерные функции машины опорных векторов</a></li>
<li class="chapter" data-level="6.4" data-path="064-Classification-Trees.html"><a href="064-Classification-Trees.html"><i class="fa fa-check"></i><b>6.4</b> Деревья классификации, случайный лес и логистическая регрессия</a></li>
<li class="chapter" data-level="6.5" data-path="065-Comparing-Classifiers.html"><a href="065-Comparing-Classifiers.html"><i class="fa fa-check"></i><b>6.5</b> Процедуры сравнения эффективности моделей классификации</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="071-Multiclass-Classification.html"><a href="071-Multiclass-Classification.html"><i class="fa fa-check"></i><b>7</b> Модели классификации для нескольких классов</a><ul>
<li class="chapter" data-level="7.1" data-path="071-Multiclass-Classification.html"><a href="071-Multiclass-Classification.html#sec_7_1"><i class="fa fa-check"></i><b>7.1</b> Ирисы Фишера и метод <em>k</em> ближайших соседей</a></li>
<li class="chapter" data-level="7.2" data-path="072-NBC.html"><a href="072-NBC.html"><i class="fa fa-check"></i><b>7.2</b> Наивный байесовский классификатор</a></li>
<li class="chapter" data-level="7.3" data-path="073-In-Discriminant-Space.html"><a href="073-In-Discriminant-Space.html"><i class="fa fa-check"></i><b>7.3</b> Классификация в линейном дискриминантном пространстве</a></li>
<li class="chapter" data-level="7.4" data-path="074-Nonlinear-Classifiers.html"><a href="074-Nonlinear-Classifiers.html"><i class="fa fa-check"></i><b>7.4</b> Нелинейные классификаторы в R</a></li>
<li class="chapter" data-level="7.5" data-path="075-Multinomial-Logit.html"><a href="075-Multinomial-Logit.html"><i class="fa fa-check"></i><b>7.5</b> Модель мультиномиального логита</a></li>
<li class="chapter" data-level="7.6" data-path="076-NN.html"><a href="076-NN.html"><i class="fa fa-check"></i><b>7.6</b> Классификаторы на основе искусственных нейронных сетей</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="081-Logit-for-Count.html"><a href="081-Logit-for-Count.html"><i class="fa fa-check"></i><b>8</b> Моделирование порядковых и счетных переменных</a><ul>
<li class="chapter" data-level="8.1" data-path="081-Logit-for-Count.html"><a href="081-Logit-for-Count.html#sec_8_1"><i class="fa fa-check"></i><b>8.1</b> Модель логита для порядковой переменной</a></li>
<li class="chapter" data-level="8.2" data-path="082-NN-with-Caret.html"><a href="082-NN-with-Caret.html"><i class="fa fa-check"></i><b>8.2</b> Настройка параметров нейронных сетей средствами пакета <code id="sec_8_2">caret</code></a></li>
<li class="chapter" data-level="8.3" data-path="083-Model-Complexes.html"><a href="083-Model-Complexes.html"><i class="fa fa-check"></i><b>8.3</b> Методы комплексации модельных прогнозов</a></li>
<li class="chapter" data-level="8.4" data-path="084-GLM-for-Counts.html"><a href="084-GLM-for-Counts.html"><i class="fa fa-check"></i><b>8.4</b> Обобщенные линейные модели для счетных данных</a></li>
<li class="chapter" data-level="8.5" data-path="085-ZIP-for-Counts.html"><a href="085-ZIP-for-Counts.html"><i class="fa fa-check"></i><b>8.5</b> ZIP- и барьерные модели счетных данных</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="091-Data-Transformation.html"><a href="091-Data-Transformation.html"><i class="fa fa-check"></i><b>9</b> Методы многомерной ординации</a><ul>
<li class="chapter" data-level="9.1" data-path="091-Data-Transformation.html"><a href="091-Data-Transformation.html#sec_9_1"><i class="fa fa-check"></i><b>9.1</b> Преобразование данных и вычисление матрицы расстояний</a></li>
<li class="chapter" data-level="9.2" data-path="092-Distance-ANOVA.html"><a href="092-Distance-ANOVA.html"><i class="fa fa-check"></i><b>9.2</b> Непараметрический дисперсионный анализ матриц дистанций</a></li>
<li class="chapter" data-level="9.3" data-path="093-Comparing-Diagrams.html"><a href="093-Comparing-Diagrams.html"><i class="fa fa-check"></i><b>9.3</b> Методы ординации объектов и переменных: построение и сравнение диаграмм</a></li>
<li class="chapter" data-level="9.4" data-path="094-Ordination-Factors.html"><a href="094-Ordination-Factors.html"><i class="fa fa-check"></i><b>9.4</b> Оценка связи ординации с внешними факторами</a></li>
<li class="chapter" data-level="9.5" data-path="095-NMDS.html"><a href="095-NMDS.html"><i class="fa fa-check"></i><b>9.5</b> Неметрическое многомерное шкалирование и построение распределения чувствительности видов</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="101-Partitioning-Algos.html"><a href="101-Partitioning-Algos.html"><i class="fa fa-check"></i><b>10</b> Кластерный анализ</a><ul>
<li class="chapter" data-level="10.1" data-path="101-Partitioning-Algos.html"><a href="101-Partitioning-Algos.html#sec_10_1"><i class="fa fa-check"></i><b>10.1</b> Алгоритмы кластеризации, основанные на разделении</a></li>
<li class="chapter" data-level="10.2" data-path="102-H-Clustering.html"><a href="102-H-Clustering.html"><i class="fa fa-check"></i><b>10.2</b> Иерархическая кластеризация</a></li>
<li class="chapter" data-level="10.3" data-path="103-Clustering-Quality.html"><a href="103-Clustering-Quality.html"><i class="fa fa-check"></i><b>10.3</b> Оценка качества кластеризации</a></li>
<li class="chapter" data-level="10.4" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html"><i class="fa fa-check"></i><b>10.4</b> Другие алгоритмы кластеризации</a><ul>
<li class="chapter" data-level="10.4.1" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html#sec_10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Иерархическая кластеризация на главные компоненты</a></li>
<li class="chapter" data-level="10.4.2" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html#sec_10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Метод нечетких <em>k</em> средних (fuzzy analysis clustering)</a></li>
<li class="chapter" data-level="10.4.3" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html#sec_10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Статистическая модель кластеризации</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="105-Cohonen-Maps.html"><a href="105-Cohonen-Maps.html"><i class="fa fa-check"></i><b>10.5</b> Самоорганизующиеся карты Кохонена</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="111-Rattle-Intro.html"><a href="111-Rattle-Intro.html"><i class="fa fa-check"></i><b>11</b> <code>rattle</code>: графический интерфейс R для реализации алгоритмов Data Mining</a><ul>
<li class="chapter" data-level="11.1" data-path="111-Rattle-Intro.html"><a href="111-Rattle-Intro.html#----rattle"><i class="fa fa-check"></i><b>11.1</b> Начало работы с пакетом <code id="sec_11_1">rattle</code></a></li>
<li class="chapter" data-level="11.2" data-path="112-Descriptive-Stats.html"><a href="112-Descriptive-Stats.html"><i class="fa fa-check"></i><b>11.2</b> Описательная статистика и визуализация данных</a></li>
<li class="chapter" data-level="11.3" data-path="113-Model-Building.html"><a href="113-Model-Building.html"><i class="fa fa-check"></i><b>11.3</b> Построение и тестирование моделей классификации</a></li>
<li class="chapter" data-level="11.4" data-path="114-Descriptive-Models.html"><a href="114-Descriptive-Models.html"><i class="fa fa-check"></i><b>11.4</b> Дескриптивные модели (обучение без учителя)</a><ul>
<li class="chapter" data-level="11.4.1" data-path="114-Descriptive-Models.html"><a href="114-Descriptive-Models.html#sec_11_4_1"><i class="fa fa-check"></i><b>11.4.1</b> Кластерный анализ</a></li>
<li class="chapter" data-level="11.4.2" data-path="114-Descriptive-Models.html"><a href="114-Descriptive-Models.html#sec_11_4_2"><i class="fa fa-check"></i><b>11.4.2</b> Ассоциативные правила</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="120-References.html"><a href="120-References.html"><i class="fa fa-check"></i><b>12</b> Список рекомендуемой литературы</a></li>
<li class="chapter" data-level="" data-path="130-Appendix.html"><a href="130-Appendix.html"><i class="fa fa-check"></i>Приложение: cправочная карта по Data Mining с использованием R</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Классификация, регрессия и другие алгоритмы Data Mining с использованием R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec_6_5" class="section level2">
<h2><span class="header-section-number">6.5</span> Процедуры сравнения эффективности моделей классификации</h2>
<p>Итак, мы использовали шесть методов для построения моделей бинарной классификации. Осуществим их сравнение для данного примера с полным сохранением всех канонов тестирования моделей прогнозирования (Kuhn, Johnson, 2013) и в соответствии с методикой, представленной в сообщении <a href="http://machinelearningmastery.com/compare-the-performance-of-machine-learning-algorithms-in-r/">http://machinelearningmastery.com</a>:</p>
<ol style="list-style-type: decimal">
<li>Проводим разделение имеющейся выборки на обучающую и проверочную (использование фактора в качестве аргумента функции <code>createDataPartition()</code> обеспечивает необходимый баланс его уровней при разделении). Для обучения выделим 70% (115 элементов) исходной выборки:</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">7</span>)
train &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">createDataPartition</span>(DGlass$FAC, <span class="dt">p =</span> <span class="fl">0.7</span>))
<span class="co"># определение схемы тестирования</span>
control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, 
                        <span class="dt">number =</span> <span class="dv">10</span>, <span class="dt">repeats =</span> <span class="dv">3</span>, <span class="dt">classProbs =</span> <span class="ot">TRUE</span>)</code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>Выполняем обучение шести моделей:</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># LDA - линейный дискриминантный анализ</span>
<span class="kw">set.seed</span>(<span class="dv">7</span>)
fit.lda &lt;-<span class="st"> </span><span class="kw">train</span>(DGlass[train, <span class="dv">3</span>:<span class="dv">4</span>], DGlass$FAC[train],
                 <span class="dt">method =</span> <span class="st">&quot;lda&quot;</span>, <span class="dt">trControl =</span> control)
<span class="co"># SVML - метод опорных векторов с линейным ядром</span>
<span class="kw">set.seed</span>(<span class="dv">7</span>)
fit.svL &lt;-<span class="st"> </span><span class="kw">train</span>(DGlass[train, <span class="dv">1</span>:<span class="dv">9</span>], DGlass$FAC[train],
                 <span class="dt">method =</span> <span class="st">&quot;svmLinear&quot;</span>, <span class="dt">trControl =</span> control)
<span class="co"># SVMR  - метод опорных векторов с радиальным ядром</span>
<span class="kw">set.seed</span>(<span class="dv">7</span>)
fit.svR &lt;-<span class="st"> </span><span class="kw">train</span>(DGlass[train, <span class="dv">1</span>:<span class="dv">9</span>], DGlass$FAC[train],
                 <span class="dt">method =</span> <span class="st">&quot;svmRadial&quot;</span>, <span class="dt">trControl =</span> control,
                 <span class="dt">tuneGrid =</span> <span class="kw">expand.grid</span>(<span class="dt">sigma =</span> <span class="fl">0.4</span>, <span class="dt">C =</span> <span class="dv">2</span>))
<span class="co"># CART - дерево классификации</span>
<span class="kw">set.seed</span>(<span class="dv">7</span>)
fit.cart &lt;-<span class="st"> </span><span class="kw">train</span>(DGlass[train, <span class="dv">1</span>:<span class="dv">9</span>], DGlass$FAC[train],
                  <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>, <span class="dt">trControl =</span> control)
<span class="co"># RF - случайный лес</span>
<span class="kw">set.seed</span>(<span class="dv">7</span>)
fit.rf &lt;-<span class="st"> </span><span class="kw">train</span>(DGlass[train, <span class="dv">1</span>:<span class="dv">9</span>], DGlass$FAC[train],
                <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>, <span class="dt">trControl =</span> control)
<span class="co"># GLM - Логистическая регрессия</span>
<span class="kw">set.seed</span>(<span class="dv">7</span>)
fit.glm &lt;-<span class="st"> </span><span class="kw">train</span>(DGlass[train, -<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">10</span>, <span class="dv">11</span>)],DGlass$FAC[train],
                 <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>, <span class="dt">family =</span> binomial, <span class="dt">trControl =</span> control)</code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>Осуществляем ресэмплинг полученных моделей, выполняем статистический анализ двух сформированных метрик качества моделей (<code>Accuracy</code> и индекс <code>Kappa</code> Дж. Коэна) на обучающей выборке, ранжируем модели и оцениваем доверительные интервалы использованных критериев:</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">caret.models &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">LDA =</span> fit.lda, <span class="dt">SVML =</span> fit.svL,
                     <span class="dt">SVMR =</span> fit.svR, <span class="dt">CART =</span> fit.cart, 
                     <span class="dt">RF =</span> fit.rf, <span class="dt">GLM =</span> fit.glm)
<span class="co"># ресэмплинг коллекции моделей</span>
results &lt;-<span class="st"> </span><span class="kw">resamples</span>(caret.models)
<span class="co"># обобщение различий между моделями</span>
<span class="kw">summary</span>(results)</code></pre></div>
<pre><code>## 
## Call:
## summary.resamples(object = results)
## 
## Models: LDA, SVML, SVMR, CART, RF, GLM 
## Number of resamples: 30 
## 
## Accuracy 
##        Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA&#39;s
## LDA  0.5455  0.6667 0.7937 0.7787  0.9091 1.0000    0
## SVML 0.5000  0.6364 0.7273 0.7135  0.8011 0.9167    0
## SVMR 0.6364  0.7273 0.8182 0.8081  0.8934 1.0000    0
## CART 0.5455  0.7330 0.8333 0.8195  0.9091 1.0000    0
## RF   0.6364  0.8182 0.8776 0.8728  1.0000 1.0000    0
## GLM  0.5000  0.6667 0.7273 0.7251  0.8182 0.9167    0
## 
## Kappa 
##        Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA&#39;s
## LDA  0.0678  0.3333 0.5754 0.5502  0.8181 1.0000    0
## SVML 0.0000  0.2477 0.4391 0.4181  0.5905 0.8333    0
## SVMR 0.2667  0.4590 0.6270 0.6141  0.7828 1.0000    0
## CART 0.0678  0.4821 0.6667 0.6366  0.8197 1.0000    0
## RF   0.2667  0.6239 0.7482 0.7443  1.0000 1.0000    0
## GLM  0.0000  0.3333 0.4590 0.4459  0.6333 0.8333    0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#  Оценка доверительных интервалов и построение графика</span>
scales &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">x =</span> <span class="kw">list</span>(<span class="dt">relation =</span> <span class="st">&quot;free&quot;</span>),
               <span class="dt">y =</span> <span class="kw">list</span>(<span class="dt">relation =</span> <span class="st">&quot;free&quot;</span>))
<span class="kw">dotplot.resamples</span>(results)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-6-8"></span>
<img src="figures/resamps.png" alt="Сравнительный статистический анализ двух метрик эффективности шести моделей прогнозирования типа стекла" width="480px" />
<p class="caption">
Рисунок 6.8: Сравнительный статистический анализ двух метрик эффективности шести моделей прогнозирования типа стекла
</p>
</div>
<ol start="4" style="list-style-type: decimal">
<li>Наконец, можно вычислить статистическую значимость различий между моделями на основе распределения оценок критериев качества. Для этого воспользуемся функцией <code>diff()</code>:</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">diffs &lt;-<span class="st"> </span><span class="kw">diff</span>(results)
<span class="co"># summarize p-values for pair-wise comparisons</span>
<span class="kw">summary</span>(diffs)</code></pre></div>
<pre><code>## 
## Call:
## summary.diff.resamples(object = diffs)
## 
## p-value adjustment: bonferroni 
## Upper diagonal: estimates of the difference
## Lower diagonal: p-value for H0: difference = 0
## 
## Accuracy 
##      LDA       SVML      SVMR      CART      RF        GLM     
## LDA             0.06519  -0.02937  -0.04073  -0.09406   0.05361
## SVML 0.0058827           -0.09456  -0.10592  -0.15925  -0.01158
## SVMR 1.0000000 0.0013835           -0.01136  -0.06469   0.08298
## CART 0.4810803 2.101e-05 1.0000000           -0.05332   0.09435
## RF   5.290e-05 1.169e-09 0.0708208 0.0891429            0.14767
## GLM  0.1042965 1.0000000 0.0032517 0.0007246 1.971e-09         
## 
## Kappa 
##      LDA       SVML      SVMR      CART      RF        GLM     
## LDA             0.13215  -0.06383  -0.08632  -0.19409   0.10437
## SVML 0.0070527           -0.19599  -0.21847  -0.32624  -0.02778
## SVMR 1.0000000 0.0007739           -0.02248  -0.13025   0.16821
## CART 0.3928054 1.583e-05 1.0000000           -0.10777   0.19069
## RF   4.549e-05 5.371e-10 0.0654607 0.0951760            0.29846
## GLM  0.1556033 1.0000000 0.0029324 0.0008886 1.784e-09</code></pre>
<p>Результаты выше главной диагонали таблицы статистической значимости содержат разности между центрами распределения. Эти значения показывают, как модели соотносятся друг с другом по абсолютному значению точности. Данные ниже диагонали таблицы представляют собой <span class="math inline">\(p\)</span>-значения для нулевой гипотезы, которая утверждает, что параметры распределения одинаковы. Например, очевидно, что модель Random Forrest статистически значимо точнее всех остальных.</p>
<ol start="5" style="list-style-type: decimal">
<li>Составим таблицу прогноза всеми шестью моделями для 48 проверочных наблюдений:</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred.fm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
    <span class="dt">LDA =</span> <span class="kw">predict</span>(fit.lda, DGlass[-train, <span class="dv">3</span>:<span class="dv">4</span>]),
    <span class="dt">SVML =</span> <span class="kw">predict</span>(fit.svL, DGlass[-train, <span class="dv">1</span>:<span class="dv">9</span>]),
    <span class="dt">SVMR =</span> <span class="kw">predict</span>(fit.svR, DGlass[-train, <span class="dv">1</span>:<span class="dv">9</span>]),
    <span class="dt">CART =</span> <span class="kw">predict</span>(fit.cart, DGlass[-train, <span class="dv">1</span>:<span class="dv">9</span>]),
    <span class="dt">RF =</span> <span class="kw">predict</span>(fit.rf, DGlass[-train, <span class="dv">1</span>:<span class="dv">9</span>]),
    <span class="dt">GLM =</span> <span class="kw">predict</span>(fit.glm, DGlass[-train,-<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">10</span>, <span class="dv">11</span>)])
)</code></pre></div>
<p>Резерв повышения надежности моделей прогнозирования многими исследователями видится в объединении отдельных моделей в “ансамбль” (ensemble) и построении системы коллективного распознавания. Простейшим способом комбинирования результатов бинарной классификации является выбор класса, за который “проголосовало” большинство предсказывающих моделей. Добавим в таблицу прогнозов столбец с результатами голосования (методу Random Forrest, как наиболее эффективному, отдано два голоса):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">CombyPred &lt;-<span class="st"> </span><span class="kw">apply</span>(pred.fm, <span class="dv">1</span>, function (voice) {
    voice2 &lt;-<span class="st"> </span><span class="kw">c</span>(voice, voice[<span class="dv">5</span>]) <span class="co"># У RF двойной голос</span>
    <span class="kw">ifelse</span>(<span class="kw">sum</span>(voice2 ==<span class="st"> &quot;Flash&quot;</span>) &gt;<span class="st"> </span><span class="dv">3</span>, <span class="st">&quot;Flash&quot;</span>, <span class="st">&quot;No&quot;</span>) }
)
pred.fm &lt;-<span class="st"> </span><span class="kw">cbind</span>(pred.fm, <span class="dt">COMB =</span> CombyPred)
<span class="kw">head</span>(pred.fm)</code></pre></div>
<pre><code>##     LDA  SVML  SVMR  CART    RF   GLM  COMB
## 1 Flash    No    No Flash    No    No    No
## 2    No    No    No    No    No    No    No
## 3 Flash    No Flash Flash Flash    No Flash
## 4 Flash Flash Flash Flash Flash Flash Flash
## 5 Flash Flash Flash Flash Flash Flash Flash
## 6 Flash Flash Flash Flash Flash Flash Flash</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>Определимся со списком критериев, которые следует использовать для тестирования. Кроме традиционных оценок точности <code>AC</code>, чувствительности <code>SE</code> и специфичности <code>SP</code>, воспользуемся дополнительными метриками, рекомендованными в обзоре <a href="http://datareview.info/article/luchshaya-metrika-dlya-ocenki-tochnosti-klassifikacionnyx-modelej/">http://datareview.info</a> и позволяющими получить несмещенную оценку, особенно в случае несбалансированных классов: <code>F</code>-мера (<span class="math inline">\(F\)</span>-score) <code>= 2 * AC * SE / (AC + SE)</code> и коэффициент корреляции Мэтьюса (Matthews сorrelation сoefficient, MCC):</li>
</ol>
<p><span class="math display">\[MCC = \frac{(TP \times TN) - (FP \times FN)}{\sqrt{(TP+FP)\times(TP+FN)\times(TN+FP)\times(TN+FN)}}\]</span></p>
<p>(условные обозначения см. в разделе <a href="023-Models-for-Class-Prediction.html#sec_2_3">2.3</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Функция формирования строки критериев</span>
ModCrit &lt;-<span class="st"> </span>function(fact, pred) {
    cM &lt;-<span class="st"> </span><span class="kw">table</span>(fact, pred)
    <span class="kw">c</span>( Accur &lt;-<span class="st"> </span>(cM[<span class="dv">1</span>, <span class="dv">1</span>] +<span class="st"> </span>cM[<span class="dv">2</span>, <span class="dv">2</span>])/<span class="kw">sum</span>(cM),
       Sens &lt;-<span class="st"> </span>cM[<span class="dv">1</span>, <span class="dv">1</span>]/(cM[<span class="dv">1</span>, <span class="dv">1</span>] +<span class="st"> </span>cM[<span class="dv">2</span>, <span class="dv">1</span>]),
       Spec &lt;-<span class="st"> </span>cM[<span class="dv">2</span>, <span class="dv">2</span>]/(cM[<span class="dv">2</span>, <span class="dv">2</span>] +<span class="st"> </span>cM[<span class="dv">1</span>, <span class="dv">2</span>]),
       F.score &lt;-<span class="st">  </span><span class="dv">2</span> *<span class="st"> </span>Accur *<span class="st"> </span>Sens /<span class="st"> </span>(Accur +<span class="st"> </span>Sens),
       MCC &lt;-<span class="st"> </span>((cM[<span class="dv">1</span>, <span class="dv">1</span>]*cM[<span class="dv">2</span>, <span class="dv">2</span>]) -<span class="st"> </span>(cM[<span class="dv">1</span>, <span class="dv">2</span>]*cM[<span class="dv">2</span>, <span class="dv">1</span>])) /<span class="st"> </span>
<span class="st">           </span><span class="kw">sqrt</span>((cM[<span class="dv">1</span>, <span class="dv">1</span>] +<span class="st"> </span>cM[<span class="dv">1</span>, <span class="dv">2</span>])*(cM[<span class="dv">1</span>, <span class="dv">1</span>] +<span class="st"> </span>cM[<span class="dv">2</span>, <span class="dv">1</span>]) *
<span class="st">                    </span>(cM[<span class="dv">2</span>, <span class="dv">2</span>] +<span class="st"> </span>cM[<span class="dv">1</span>, <span class="dv">2</span>])*(cM[<span class="dv">2</span>, <span class="dv">2</span>] +<span class="st"> </span>cM[<span class="dv">2</span>, <span class="dv">1</span>])) )
}
Result &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(pred.fm, <span class="dv">2</span>, function(x) <span class="kw">ModCrit</span>(DGlass$FAC[-train], x)))
<span class="kw">colnames</span>(Result) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Точность&quot;</span>, <span class="st">&quot;Чувствит.&quot;</span>, <span class="st">&quot;Специфичн.&quot;</span>, <span class="st">&quot;F-мера&quot;</span>, <span class="st">&quot;КК Мэтьюса&quot;</span>)
<span class="kw">round</span>(Result, <span class="dv">3</span>)</code></pre></div>
<pre><code>##      Точность Чувствит. Специфичн. F-мера КК Мэтьюса
## LDA     0.688     0.704      0.667  0.696      0.369
## SVML    0.562     0.600      0.522  0.581      0.122
## SVMR    0.792     0.786      0.800  0.789      0.580
## CART    0.708     0.714      0.700  0.711      0.410
## RF      0.750     0.769      0.727  0.759      0.497
## GLM     0.542     0.571      0.500  0.556      0.071
## COMB    0.688     0.690      0.684  0.689      0.367</code></pre>
<p>По существу проведенных расчетов можно сделать два вывода:</p>
<ul>
<li>при тестировании на “свежих” данных модель Random Forrest уступает по эффективности модели опорных векторов с радиальным ядром;</li>
<li>прогноз “голосующего коллектива” COMB имеет умеренные показатели точности предсказания на фоне индивидуальных моделей.</li>
</ul>
<p>В разделе <a href="023-Models-for-Class-Prediction.html#sec_2_3">2.3</a> было показано, что универсальным способом сравнения точности классификаторов является ROC-анализ. Воспользовавшись тем, что при обучении мы сохранили вероятности принадлежности к классам (<code>classProbs = TRUE</code>), построим ROC-кривые для трех используемых методов распознавания (LDA, SVMR и RF) по полной таблице исходных данных, включая обучающую и тестовую последовательности. Будем анализировать вероятности отнесения к классу <code>Flash</code>, т.е. 1-й столбец таблицы вероятностей, возвращаемой функцией <code>predict()</code> при значении параметра <code>type = &quot;prob&quot;</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Извлекаем вероятности классов по всей выборке</span>
pred.lda.roc &lt;-<span class="st"> </span><span class="kw">predict</span>(fit.lda, DGlass[, <span class="dv">3</span>:<span class="dv">4</span>], <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)
pred.svmR.roc &lt;-<span class="st"> </span><span class="kw">predict</span>(fit.svR, DGlass[, <span class="dv">1</span>:<span class="dv">9</span>], <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)
pred.rf.roc &lt;-<span class="st"> </span><span class="kw">predict</span>(fit.rf, DGlass[, <span class="dv">1</span>:<span class="dv">9</span>], <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)

<span class="kw">library</span>(pROC)        <span class="co"># Строим три ROC-кривые</span>
m1.roc &lt;-<span class="st"> </span><span class="kw">roc</span>(DGlass$FAC, pred.lda.roc[, <span class="dv">1</span>])
m2.roc &lt;-<span class="st"> </span><span class="kw">roc</span>(DGlass$FAC, pred.svmR.roc[, <span class="dv">1</span>])
m3.roc &lt;-<span class="st"> </span><span class="kw">roc</span>(DGlass$FAC, pred.rf.roc[, <span class="dv">1</span>])</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(m1.roc, <span class="dt">grid.col =</span> <span class="kw">c</span>(<span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="dt">grid =</span> <span class="kw">c</span>(<span class="fl">0.1</span>, <span class="fl">0.2</span>),
     <span class="dt">print.auc =</span> <span class="ot">TRUE</span>, <span class="dt">print.thres =</span> <span class="ot">TRUE</span>)
<span class="kw">plot</span>(m2.roc , <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">col =</span> <span class="st">&quot;green&quot;</span>, <span class="dt">print.auc =</span> <span class="ot">TRUE</span>,
     <span class="dt">print.auc.y =</span> <span class="fl">0.45</span>, <span class="dt">print.thres =</span> <span class="ot">TRUE</span>)
<span class="kw">plot</span>(m3.roc , <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">print.auc =</span> <span class="ot">TRUE</span>,
     <span class="dt">print.auc.y =</span> <span class="fl">0.40</span>,<span class="dt">print.thres =</span> <span class="ot">TRUE</span>)
<span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;LDA&quot;</span>,<span class="st">&quot;SVM&quot;</span>,<span class="st">&quot;RF&quot;</span>,<span class="st">&quot;COMBY&quot;</span>), <span class="dt">lwd =</span> <span class="dv">2</span>,
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>))</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-6-9"></span>
<img src="figures/rocs.png" alt="ROC-кривые для четырех моделей классификации" width="500px" />
<p class="caption">
Рисунок 6.9: ROC-кривые для четырех моделей классификации
</p>
</div>
<p>Нетрудно заметить, что каждая модель, в силу характерных для нее стратегий распознавания, имеет различную эффективность классификации на каждом из диапазонов значений апостериорных вероятностей <span class="math inline">\(p\)</span> класса <code>Flash</code>. Например, классификатор SVM на опорных векторах имеет определенное преимущество при предсказании объектов в критической зоне разделения (между <span class="math inline">\(p = 0.4\)</span> и <span class="math inline">\(p = 0.6\)</span>) и при пороговом значении <span class="math inline">\(p = 0.53\)</span> он достигает максимальной точности (чувствительность <code>SE = 93.1</code>% и специфичность <code>SP = 88.2</code>%). В других диапазонах <span class="math inline">\(p\)</span> эффективность модели SVM несколько уступает остальным классификаторам, что имеет свои объяснения, если вспомнить механизм распознавания на основе опорных векторов.</p>
<p>Максимум значения AUC на рис. <a href="065-Comparing-Classifiers.html#fig:fig-6-9">6.9</a> принадлежит модели случайного леса, но использование функций <code>ci.auc()</code> и <code>roc.test()</code> даст нам доверительные интервалы для AUC и позволит сделать вывод о значимости различий между классификаторами по этому показателю.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Доверительные интервалы для параметров ROC-анализа:</span>
<span class="kw">ci.auc</span>(m2.roc)</code></pre></div>
<pre><code>## 95% CI: 0.8949-0.9814 (DeLong)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ci.auc</span>(m3.roc)</code></pre></div>
<pre><code>## 95% CI: 0.9344-0.9919 (DeLong)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">roc.test</span>(m2.roc, m3.roc)</code></pre></div>
<pre><code>## 
##  DeLong&#39;s test for two correlated ROC curves
## 
## data:  m2.roc and m3.roc
## Z = 1.5878, p-value = 0.1123
## alternative hypothesis: true difference in AUC is not equal to 0
## sample estimates:
## AUC of roc1 AUC of roc2 
##   0.9381428   0.9631730</code></pre>
<p>Результаты свидетельствуют, что по величине AUC метод случайного леса (<code>m3.roc</code>) на полной выборке значимо превосходит метод опорных векторов (<code>m2.roc</code>). Аналогичные функции <code>ci.se()</code> и <code>ci.sp()</code> дадут возможность сделать такой же вывод относительно чувствительности и специфичности.</p>
<p>Вернемся к нашим попыткам осуществить коллективный прогноз. Кроме голосования моделей, вторым простым способом комбинирования результатов бинарной классификации является расчет взвешенной средней апостериорной вероятности отнесения объекта к тому или иному классу <span class="math inline">\(p_c = \sum_j w_j p_j\)</span>, где <span class="math inline">\(p_j\)</span> - частные вероятности, полученные g различными классификаторами, <span class="math inline">\(w_j\)</span> - веса, нормирующие “компетентность” отдельных моделей. В качестве весов могут использоваться оценки экспертов или любые другие статистики (например, AUC, предварительно возведенную в квадрат для увеличения “контраста” и нормированную на их сумму по трем выбранным методам):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">weight &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">as.numeric</span>(m1.roc$auc)^<span class="dv">2</span>,
            <span class="kw">as.numeric</span>(m2.roc$auc)^<span class="dv">2</span>,
            <span class="kw">as.numeric</span>(m3.roc$auc)^<span class="dv">2</span>)
weight &lt;-<span class="st"> </span>weight/<span class="kw">sum</span>(weight)
Pred =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">lda =</span> pred.lda.roc[, <span class="dv">1</span>], 
                  <span class="dt">svm =</span> pred.svmR.roc[, <span class="dv">1</span>],
                  <span class="dt">rf =</span> pred.rf.roc[, <span class="dv">1</span>])
CombyPred &lt;-<span class="st"> </span><span class="kw">apply</span>(Pred, <span class="dv">1</span>, function(x) <span class="kw">sum</span>(x*weight))</code></pre></div>
<p>Построим ROC-кривую для комбинированного прогноза по этому методу и добавим ее на рис. <a href="065-Comparing-Classifiers.html#fig:fig-6-9">6.9</a> - <code>COMBY</code>. С помощью функции <code>coord()</code> можно вывести полную информацию об оптимальной пороговой точке (<code>threshold</code>) полученного классификатора.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m.Comby &lt;-<span class="st"> </span><span class="kw">roc</span>(DGlass$FAC,  CombyPred)
<span class="kw">plot</span>(m.Comby, <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>,
     <span class="dt">print.auc =</span> <span class="ot">TRUE</span>, <span class="dt">print.auc.y =</span> <span class="fl">0.35</span>)
<span class="kw">coords</span>(m.Comby, <span class="st">&quot;best&quot;</span>, <span class="dt">ret =</span> <span class="kw">c</span>(<span class="st">&quot;threshold&quot;</span>, <span class="st">&quot;specificity&quot;</span>,
                                <span class="st">&quot;sensitivity&quot;</span>, <span class="st">&quot;accuracy&quot;</span>))</code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="064-Classification-Trees.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="071-Multiclass-Classification.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["_main.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
