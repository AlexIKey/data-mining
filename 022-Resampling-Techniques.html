<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Классификация, регрессия и другие алгоритмы Data Mining с использованием R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Реализация алгоритмов Data Mining с использованием R">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Классификация, регрессия и другие алгоритмы Data Mining с использованием R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://ranalytics.github.io/data-mining/" />
  
  <meta property="og:description" content="Реализация алгоритмов Data Mining с использованием R" />
  <meta name="github-repo" content="ranalytics/data-mining" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Классификация, регрессия и другие алгоритмы Data Mining с использованием R" />
  
  <meta name="twitter:description" content="Реализация алгоритмов Data Mining с использованием R" />
  

<meta name="author" content="Шитиков В. К., Мастицкий С. Э.">


<meta name="date" content="2017-04-06">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="021-Model-Quality-Criteria.html">
<link rel="next" href="023-Models-for-Class-Prediction.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Аннотация</a></li>
<li class="chapter" data-level="1" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html"><i class="fa fa-check"></i><b>1</b> Реализация моделей Data Mining в среде R (вместо предисловия)</a><ul>
<li class="chapter" data-level="1.1" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#section_1_1"><i class="fa fa-check"></i><b>1.1</b> Data Mining как направление анализа данных</a><ul>
<li class="chapter" data-level="1.1.1" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_1"><i class="fa fa-check"></i><b>1.1.1</b> От статистического анализа разового эксперимента к Data Mining</a></li>
<li class="chapter" data-level="1.1.2" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_2"><i class="fa fa-check"></i><b>1.1.2</b> Принципиальная множественность моделей окружающего мира</a></li>
<li class="chapter" data-level="1.1.3" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_3"><i class="fa fa-check"></i><b>1.1.3</b> Нарастающая множественность алгоритмов построения моделей</a></li>
<li class="chapter" data-level="1.1.4" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_4"><i class="fa fa-check"></i><b>1.1.4</b> Типы и характеристики групп моделей Data Mining</a></li>
<li class="chapter" data-level="1.1.5" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_5"><i class="fa fa-check"></i><b>1.1.5</b> Природа многомерного отклика и его моделирование</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="012-R-Intro.html"><a href="012-R-Intro.html"><i class="fa fa-check"></i><b>1.2</b> Статистическая среда R и ее использование в Data Mining</a></li>
<li class="chapter" data-level="1.3" data-path="013-What-This-Book-Is-About.html"><a href="013-What-This-Book-Is-About.html"><i class="fa fa-check"></i><b>1.3</b> О чем эта книга и чего в ней нет</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="021-Model-Quality-Criteria.html"><a href="021-Model-Quality-Criteria.html"><i class="fa fa-check"></i><b>2</b> Статистические модели: критерии и методы оценивания их качества</a><ul>
<li class="chapter" data-level="2.1" data-path="021-Model-Quality-Criteria.html"><a href="021-Model-Quality-Criteria.html#sec_2_1"><i class="fa fa-check"></i><b>2.1</b> Основные шаги построения и верификации моделей</a></li>
<li class="chapter" data-level="2.2" data-path="022-Resampling-Techniques.html"><a href="022-Resampling-Techniques.html"><i class="fa fa-check"></i><b>2.2</b> Использование алгоритмов ресэмплинга для тестирования моделей и оптимизации их параметров</a></li>
<li class="chapter" data-level="2.3" data-path="023-Models-for-Class-Prediction.html"><a href="023-Models-for-Class-Prediction.html"><i class="fa fa-check"></i><b>2.3</b> Модели для предсказания класса объектов</a></li>
<li class="chapter" data-level="2.4" data-path="024-Projecting-Data-onto-a-Plane.html"><a href="024-Projecting-Data-onto-a-Plane.html"><i class="fa fa-check"></i><b>2.4</b> Проецирование многомерных данных на плоскости</a></li>
<li class="chapter" data-level="2.5" data-path="025-MV-analysis.html"><a href="025-MV-analysis.html"><i class="fa fa-check"></i><b>2.5</b> Многомерный статистический анализ данных</a></li>
<li class="chapter" data-level="2.6" data-path="026-Clustering-Methods.html"><a href="026-Clustering-Methods.html"><i class="fa fa-check"></i><b>2.6</b> Методы кластеризации</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="031-Intro-to-Caret.html"><a href="031-Intro-to-Caret.html"><i class="fa fa-check"></i><b>3</b> Пакет <code>caret</code> - инструмент построения статистических моделей в R</a><ul>
<li class="chapter" data-level="3.1" data-path="031-Intro-to-Caret.html"><a href="031-Intro-to-Caret.html#---------caret"><i class="fa fa-check"></i><b>3.1</b> Универсальный интерфейс доступа к функциям машинного обучения в пакете <code id="sec_3_1">caret</code></a></li>
<li class="chapter" data-level="3.2" data-path="032-Removing-Predictors.html"><a href="032-Removing-Predictors.html"><i class="fa fa-check"></i><b>3.2</b> Обнаружение и удаление “ненужных” предикторов</a></li>
<li class="chapter" data-level="3.3" data-path="033-Preprocessing.html"><a href="033-Preprocessing.html"><i class="fa fa-check"></i><b>3.3</b> Предварительная обработка: преобразование и групповая трансформация переменных</a></li>
<li class="chapter" data-level="3.4" data-path="034-Handling-Missing-Values.html"><a href="034-Handling-Missing-Values.html"><i class="fa fa-check"></i><b>3.4</b> Заполнение пропущенных значений в данных</a></li>
<li class="chapter" data-level="3.5" data-path="035-The-train-Functions.html"><a href="035-The-train-Functions.html"><i class="fa fa-check"></i><b>3.5</b> Функция <code>train()</code> из пакета <code id="sec_3_5">caret</code></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html"><i class="fa fa-check"></i><b>4</b> Построение регрессионных моделей различного типа</a><ul>
<li class="chapter" data-level="4.1" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1"><i class="fa fa-check"></i><b>4.1</b> Селекция оптимального набора предикторов линейной модели</a><ul>
<li class="chapter" data-level="4.1.1" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_1"><i class="fa fa-check"></i><b>4.1.1</b> Полная регрессионная модель и пошаговая процедура</a></li>
<li class="chapter" data-level="4.1.2" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_2"><i class="fa fa-check"></i><b>4.1.2</b> Рекурсивное исключение переменных</a></li>
<li class="chapter" data-level="4.1.3" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_3"><i class="fa fa-check"></i><b>4.1.3</b> Генетический алгоритм</a></li>
<li class="chapter" data-level="4.1.4" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_4"><i class="fa fa-check"></i><b>4.1.4</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="042-Regularization.html"><a href="042-Regularization.html"><i class="fa fa-check"></i><b>4.2</b> Регуляризация, частные наименьшие квадраты и kNN-регрессия</a><ul>
<li class="chapter" data-level="4.2.1" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_1"><i class="fa fa-check"></i><b>4.2.1</b> Регрессия по методу “лассо”</a></li>
<li class="chapter" data-level="4.2.2" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_2"><i class="fa fa-check"></i><b>4.2.2</b> Метод частных наименьших квадратов (PLS)</a></li>
<li class="chapter" data-level="4.2.3" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_3"><i class="fa fa-check"></i><b>4.2.3</b> Регрессия по методу <em>k</em> ближайших соседей</a></li>
<li class="chapter" data-level="4.2.4" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_4"><i class="fa fa-check"></i><b>4.2.4</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html"><i class="fa fa-check"></i><b>4.3</b> Построение деревьев регрессии</a><ul>
<li class="chapter" data-level="4.3.1" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html#sec_4_3_1"><i class="fa fa-check"></i><b>4.3.1</b> Построение деревьев на основе рекурсивного разбиения</a></li>
<li class="chapter" data-level="4.3.2" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html#sec_4_3_2"><i class="fa fa-check"></i><b>4.3.2</b> Построение деревьев с использованием алгортма условного вывода</a></li>
<li class="chapter" data-level="4.3.3" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html#sec_4_3_3"><i class="fa fa-check"></i><b>4.3.3</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="044-Ensembles.html"><a href="044-Ensembles.html"><i class="fa fa-check"></i><b>4.4</b> Ансамбли моделей: бэггинг, случайные леса, бустинг</a><ul>
<li class="chapter" data-level="4.4.1" data-path="044-Ensembles.html"><a href="044-Ensembles.html#sec_4_4_1"><i class="fa fa-check"></i><b>4.4.1</b> Бэггинг и случайные леса</a></li>
<li class="chapter" data-level="4.4.2" data-path="044-Ensembles.html"><a href="044-Ensembles.html#sec_4_4_2"><i class="fa fa-check"></i><b>4.4.2</b> Бустинг</a></li>
<li class="chapter" data-level="4.4.3" data-path="044-Ensembles.html"><a href="044-Ensembles.html#sec_4_4_3"><i class="fa fa-check"></i><b>4.4.3</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="045-Comparing-Trees.html"><a href="045-Comparing-Trees.html"><i class="fa fa-check"></i><b>4.5</b> Сравнение построенных моделей и оценка информативности предикторов</a></li>
<li class="chapter" data-level="4.6" data-path="046-MV-Trees.html"><a href="046-MV-Trees.html"><i class="fa fa-check"></i><b>4.6</b> Деревья регрессии с многомерным откликом</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="051-Association-Rules.html"><a href="051-Association-Rules.html"><i class="fa fa-check"></i><b>5</b> Бинарные матрицы и ассоциативные правила</a><ul>
<li class="chapter" data-level="5.1" data-path="051-Association-Rules.html"><a href="051-Association-Rules.html#sec_5_1"><i class="fa fa-check"></i><b>5.1</b> Классификация в бинарных пространствах с использованием классических моделей</a></li>
<li class="chapter" data-level="5.2" data-path="052-Binary-Decision-Trees.html"><a href="052-Binary-Decision-Trees.html"><i class="fa fa-check"></i><b>5.2</b> Бинарные деревья решений</a></li>
<li class="chapter" data-level="5.3" data-path="053-Logic-Rules.html"><a href="053-Logic-Rules.html"><i class="fa fa-check"></i><b>5.3</b> Поиск логических закономерностей в данных</a></li>
<li class="chapter" data-level="5.4" data-path="054-Association-Rules-Algos.html"><a href="054-Association-Rules-Algos.html"><i class="fa fa-check"></i><b>5.4</b> Алгоритмы выделения ассоциативных правил</a></li>
<li class="chapter" data-level="5.5" data-path="055-Traminer.html"><a href="055-Traminer.html"><i class="fa fa-check"></i><b>5.5</b> Анализ последовательностей знаков или событий</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="061-Binary-Classifiers.html"><a href="061-Binary-Classifiers.html"><i class="fa fa-check"></i><b>6</b> Бинарные классификаторы с различными разделяющими поверхностями</a><ul>
<li class="chapter" data-level="6.1" data-path="061-Binary-Classifiers.html"><a href="061-Binary-Classifiers.html#sec_6_1"><i class="fa fa-check"></i><b>6.1</b> Дискриминантный анализ</a></li>
<li class="chapter" data-level="6.2" data-path="062-SVM.html"><a href="062-SVM.html"><i class="fa fa-check"></i><b>6.2</b> Дискриминантный анализ</a></li>
<li class="chapter" data-level="6.3" data-path="063-Nonlinear-Borders.html"><a href="063-Nonlinear-Borders.html"><i class="fa fa-check"></i><b>6.3</b> Ядерные функции машины опорных векторов</a></li>
<li class="chapter" data-level="6.4" data-path="064-Classification-Trees.html"><a href="064-Classification-Trees.html"><i class="fa fa-check"></i><b>6.4</b> Деревья классификации, случайный лес и логистическая регрессия</a></li>
<li class="chapter" data-level="6.5" data-path="065-Comparing-Classifiers.html"><a href="065-Comparing-Classifiers.html"><i class="fa fa-check"></i><b>6.5</b> Процедуры сравнения эффективности моделей классификации</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="071-Multiclass-Classification.html"><a href="071-Multiclass-Classification.html"><i class="fa fa-check"></i><b>7</b> Модели классификации для нескольких классов</a><ul>
<li class="chapter" data-level="7.1" data-path="071-Multiclass-Classification.html"><a href="071-Multiclass-Classification.html#sec_7_1"><i class="fa fa-check"></i><b>7.1</b> Ирисы Фишера и метод <em>k</em> ближайших соседей</a></li>
<li class="chapter" data-level="7.2" data-path="072-NBC.html"><a href="072-NBC.html"><i class="fa fa-check"></i><b>7.2</b> Наивный байесовский классификатор</a></li>
<li class="chapter" data-level="7.3" data-path="073-In-Discriminant-Space.html"><a href="073-In-Discriminant-Space.html"><i class="fa fa-check"></i><b>7.3</b> Классификация в линейном дискриминантном пространстве</a></li>
<li class="chapter" data-level="7.4" data-path="074-Nonlinear-Classifiers.html"><a href="074-Nonlinear-Classifiers.html"><i class="fa fa-check"></i><b>7.4</b> Нелинейные классификаторы в R</a></li>
<li class="chapter" data-level="7.5" data-path="075-Multinomial-Logit.html"><a href="075-Multinomial-Logit.html"><i class="fa fa-check"></i><b>7.5</b> Модель мультиномиального логита</a></li>
<li class="chapter" data-level="7.6" data-path="076-NN.html"><a href="076-NN.html"><i class="fa fa-check"></i><b>7.6</b> Классификаторы на основе искусственных нейронных сетей</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="081-Logit-for-Count.html"><a href="081-Logit-for-Count.html"><i class="fa fa-check"></i><b>8</b> Моделирование порядковых и счетных переменных</a><ul>
<li class="chapter" data-level="8.1" data-path="081-Logit-for-Count.html"><a href="081-Logit-for-Count.html#sec_8_1"><i class="fa fa-check"></i><b>8.1</b> Модель логита для порядковой переменной</a></li>
<li class="chapter" data-level="8.2" data-path="082-NN-with-Caret.html"><a href="082-NN-with-Caret.html"><i class="fa fa-check"></i><b>8.2</b> Настройка параметров нейронных сетей средствами пакета <code id="sec_8_2">caret</code></a></li>
<li class="chapter" data-level="8.3" data-path="083-Model-Complexes.html"><a href="083-Model-Complexes.html"><i class="fa fa-check"></i><b>8.3</b> Методы комплексации модельных прогнозов</a></li>
<li class="chapter" data-level="8.4" data-path="084-GLM-for-Counts.html"><a href="084-GLM-for-Counts.html"><i class="fa fa-check"></i><b>8.4</b> Обобщенные линейные модели для счетных данных</a></li>
<li class="chapter" data-level="8.5" data-path="085-ZIP-for-Counts.html"><a href="085-ZIP-for-Counts.html"><i class="fa fa-check"></i><b>8.5</b> ZIP- и барьерные модели счетных данных</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="091-Data-Transformation.html"><a href="091-Data-Transformation.html"><i class="fa fa-check"></i><b>9</b> Методы многомерной ординации</a><ul>
<li class="chapter" data-level="9.1" data-path="091-Data-Transformation.html"><a href="091-Data-Transformation.html#sec_9_1"><i class="fa fa-check"></i><b>9.1</b> Преобразование данных и вычисление матрицы расстояний</a></li>
<li class="chapter" data-level="9.2" data-path="092-Distance-ANOVA.html"><a href="092-Distance-ANOVA.html"><i class="fa fa-check"></i><b>9.2</b> Непараметрический дисперсионный анализ матриц дистанций</a></li>
<li class="chapter" data-level="9.3" data-path="093-Comparing-Diagrams.html"><a href="093-Comparing-Diagrams.html"><i class="fa fa-check"></i><b>9.3</b> Методы ординации объектов и переменных: построение и сравнение диаграмм</a></li>
<li class="chapter" data-level="9.4" data-path="094-Ordination-Factors.html"><a href="094-Ordination-Factors.html"><i class="fa fa-check"></i><b>9.4</b> Оценка связи ординации с внешними факторами</a></li>
<li class="chapter" data-level="9.5" data-path="095-NMDS.html"><a href="095-NMDS.html"><i class="fa fa-check"></i><b>9.5</b> Неметрическое многомерное шкалирование и построение распределения чувствительности видов</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="101-Partitioning-Algos.html"><a href="101-Partitioning-Algos.html"><i class="fa fa-check"></i><b>10</b> Кластерный анализ</a><ul>
<li class="chapter" data-level="10.1" data-path="101-Partitioning-Algos.html"><a href="101-Partitioning-Algos.html#sec_10_1"><i class="fa fa-check"></i><b>10.1</b> Алгоритмы кластеризации, основанные на разделении</a></li>
<li class="chapter" data-level="10.2" data-path="102-H-Clustering.html"><a href="102-H-Clustering.html"><i class="fa fa-check"></i><b>10.2</b> Иерархическая кластеризация</a></li>
<li class="chapter" data-level="10.3" data-path="103-Clustering-Quality.html"><a href="103-Clustering-Quality.html"><i class="fa fa-check"></i><b>10.3</b> Оценка качества кластеризации</a></li>
<li class="chapter" data-level="10.4" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html"><i class="fa fa-check"></i><b>10.4</b> Другие алгоритмы кластеризации</a><ul>
<li class="chapter" data-level="10.4.1" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html#sec_10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Иерархическая кластеризация на главные компоненты</a></li>
<li class="chapter" data-level="10.4.2" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html#sec_10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Метод нечетких <em>k</em> средних (fuzzy analysis clustering)</a></li>
<li class="chapter" data-level="10.4.3" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html#sec_10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Статистическая модель кластеризации</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="105-Cohonen-Maps.html"><a href="105-Cohonen-Maps.html"><i class="fa fa-check"></i><b>10.5</b> Самоорганизующиеся карты Кохонена</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="111-Rattle-Intro.html"><a href="111-Rattle-Intro.html"><i class="fa fa-check"></i><b>11</b> <code>rattle</code>: графический интерфейс R для реализации алгоритмов Data Mining</a><ul>
<li class="chapter" data-level="11.1" data-path="111-Rattle-Intro.html"><a href="111-Rattle-Intro.html#----rattle"><i class="fa fa-check"></i><b>11.1</b> Начало работы с пакетом <code id="sec_11_1">rattle</code></a></li>
<li class="chapter" data-level="11.2" data-path="112-Descriptive-Stats.html"><a href="112-Descriptive-Stats.html"><i class="fa fa-check"></i><b>11.2</b> Описательная статистика и визуализация данных</a></li>
<li class="chapter" data-level="11.3" data-path="113-Model-Building.html"><a href="113-Model-Building.html"><i class="fa fa-check"></i><b>11.3</b> Построение и тестирование моделей классификации</a></li>
<li class="chapter" data-level="11.4" data-path="114-Descriptive-Models.html"><a href="114-Descriptive-Models.html"><i class="fa fa-check"></i><b>11.4</b> Дескриптивные модели (обучение без учителя)</a><ul>
<li class="chapter" data-level="11.4.1" data-path="114-Descriptive-Models.html"><a href="114-Descriptive-Models.html#sec_11_4_1"><i class="fa fa-check"></i><b>11.4.1</b> Кластерный анализ</a></li>
<li class="chapter" data-level="11.4.2" data-path="114-Descriptive-Models.html"><a href="114-Descriptive-Models.html#sec_11_4_2"><i class="fa fa-check"></i><b>11.4.2</b> Ассоциативные правила</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="120-References.html"><a href="120-References.html"><i class="fa fa-check"></i><b>12</b> Список рекомендуемой литературы</a></li>
<li class="chapter" data-level="" data-path="130-Appendix.html"><a href="130-Appendix.html"><i class="fa fa-check"></i>Приложение: cправочная карта по Data Mining с использованием R</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Классификация, регрессия и другие алгоритмы Data Mining с использованием R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec_2_2" class="section level2">
<h2><span class="header-section-number">2.2</span> Использование алгоритмов ресэмплинга для тестирования моделей и оптимизации их параметров</h2>
<p>Фундаментальным для статистики является рассуждение о соотношении свойств случайных выборок и генеральной совокупности, из которых они были извлечены. Любой эксперимент в принципе ограничен некоторым (чаще всего, небольшим) множеством наблюдений, причем никакие сверхинтеллектуальные методы обработки не являются панацеей от влияния неучтенных факторов или систематических погрешностей. Один из возможных путей надежного оценивания свойств данных заключается в использовании компьютерно-интенсивных (computer-intensive) технологий, объединенных общим термином “численный ресэмплинг” (англ. resampling) или, как их иногда называют в русскоязычной литературе, “методов генерации повторных выборок” (Эфрон, 1988; Edgington, 1995; Davison, Hinkley, 2006). Ключевая особенность этой технологии заключается в том, что повторные выборки при классическом сценарии извлекаются из генеральной совокупности, а псевдоповторные выборки при выполнении ресэмплинга - из самой эмпирической выборки (хорошо известна фраза <em>“The population is to the sample as the sample is to the bootstrap samples”</em> из работы Fox, 2002).</p>
<p>Ресэмплинг объединяет четыре разных подхода к обработке данных, отличающихся по алгоритму, но близких по сути: перекрестная проверка (cross-validation, CV), бутстреп (bootstrap), рандомизация, или перестановочный тест (permutation), и метод “складного ножа” (jackknife). Ниже рассматриваются два из них: перекрестная проверка для оптимизации параметров модели и бутстреп для оценивания доверительных интервалов критериев эффективности моделей.</p>
<p>Минимизация ошибок на обучающем множестве, которое не бывает ни идеальным, ни бесконечно большим, неизбежно приводит к моделям, смещенным относительно истинной функции процесса, порождающего наблюдаемые данные. Преодолеть это смещение можно только с использованием “принципа внешнего дополнения”, т.е. блоков “свежих” данных, не участвовавших в построении модели.</p>
<p>При отсутствии дополнительных блоков данных, специально предназначенных для тестирования, вполне естественно выглядит идея провести случайное разбиение исходной выборки на обучающее (train sample) и проверочное (test sample) подмножества, после чего оценить параметры модели только на обучающей выборке, тогда как оценку погрешности прогноза отклика <span class="math inline">\(\hat{y}_i\)</span> осуществить для значений из проверочной совокупности. Если подобные шаги выполняются многократно и каждое из наблюдений используется поочередно то для обучения, то для контроля, то такая процедура эмпирического оценивания моделей, построенных по прецедентам, называется <em>перекрестной проверкой</em>, или “кросс-проверкой”.</p>
<p>Стандартной считается методика <span class="math inline">\(r \times k\)</span>-кратной кросс-проверки (<span class="math inline">\(r \times k\)</span>-fold cross-validation), когда исходная выборка случайным образом <span class="math inline">\(r\)</span> раз разбивается на <span class="math inline">\(k\)</span> блоков (folds) равной (или почти равной) длины. При реализации каждой повторности <span class="math inline">\(r\)</span> (replication) один из блоков по очереди становится проверочной совокупностью, а все остальные блоки - одной большой обучающей выборкой (рис. <a href="022-Resampling-Techniques.html#fig:fig-2-5">2.5</a>):</p>
<div class="figure" style="text-align: center"><span id="fig:fig-2-5"></span>
<img src="figures/r_k_cv.png" alt="Пример 1x3-кратной кросс-проверки" width="600px" />
<p class="caption">
Рисунок 2.5: Пример 1x3-кратной кросс-проверки
</p>
</div>
<p>При этом генерируется <span class="math inline">\(r \times k\)</span> значений отклика <span class="math inline">\(\hat{y}\)</span>, и ошибкой перекрестной проверки при использовании модели <span class="math inline">\(М\)</span> на исходной выборке <span class="math inline">\(X^n\)</span> называется средняя по всем <span class="math inline">\(k\)</span> разбиениям величина ошибки на контрольных подмножествах: <span class="math display">\[S_{CV} = \sqrt{\frac{1}{r \times n} \sum_{i=1}^{r \times n} (y_i - \hat{y}_i)^2}.\]</span></p>
<p>Выполнить разбиение исходной выборки <code>y</code> на <code>k</code> блоков можно с использованием различных функций R, например, из пакета <code>caret</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">createDataPartition</span>(y, <span class="dt">p =</span> <span class="fl">0.5</span>) 
<span class="kw">createFolds</span>(y, <span class="dt">k =</span> <span class="dv">10</span>)
<span class="kw">createMultiFolds</span>(y, <span class="dt">k =</span> <span class="dv">10</span>, <span class="dt">times =</span> r)</code></pre></div>
<p>Частным случаем является “скользящий контроль”, или перекрестная проверка с последовательным исключением одного наблюдения (leave-one-out CV, LOOCV), т.е. <span class="math inline">\(k = n\)</span>. При этом строится <span class="math inline">\(n\)</span> моделей по <span class="math inline">\((n – 1)\)</span> выборочным значениям, а исключенное наблюдение каждый раз используется для расчета ошибки прогноза. В. Н. Вапник (1984) теоретически обосновал применение скользящего контроля и показал, что если исходные выборки независимы, то средняя ошибка перекрестной проверки даёт несмещённую оценку ошибки модели. Это выгодно отличает её от средней ошибки на обучающей выборке, которая при переусложнении модели может оказаться оптимистично заниженной.</p>
<p>В разделе <a href="#section_2_1">2.1</a> мы рассматривали пример выбора оптимального числа параметров полиномиальной регрессии с использованием информационных критериев. Попробуем достичь той же цели, выполнив перекрестную проверку моделей в режиме “leave-one-out” с использованием самостоятельно оформленной функции:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Функция скользящего контроля для модели y~poly(x, degree):</span>
crossvalidate &lt;-<span class="st"> </span>function(x, y, degree) {
    preds &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="kw">length</span>(x))
    for (i in <span class="dv">1</span>:<span class="kw">length</span>(x)) {
        x.in &lt;-<span class="st"> </span>x[-i]; x.out &lt;-<span class="st"> </span>x[i]
        y.in &lt;-<span class="st"> </span>y[-i]; y.out &lt;-<span class="st"> </span>x[i]
        m &lt;-<span class="st"> </span><span class="kw">lm</span>(y.in ~<span class="st"> </span><span class="kw">poly</span>(x.in, <span class="dt">degree =</span> degree) )
        new &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x.in =</span> <span class="kw">seq</span>(-<span class="dv">3</span>, <span class="dv">3</span>, <span class="dt">by =</span> <span class="fl">0.1</span>))
        preds[i] &lt;-<span class="st"> </span><span class="kw">predict</span>(m, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x.in =</span> x.out))
    }
    <span class="co"># Тестовая статистика - сумма квадратов отклонений:</span>
    <span class="kw">return</span>(<span class="kw">sum</span>((y -<span class="st"> </span>preds)^<span class="dv">2</span>))
} 

<span class="co"># Заполнение таблицы результатами кросс-проверки </span>
<span class="co"># и сохранение квадрата ошибки в таблице &quot;a&quot;</span>
a &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">cross =</span> <span class="kw">numeric</span>(max.poly))
for (i in <span class="dv">1</span>:max.poly) {
    a[i, <span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">crossvalidate</span>(x, y, <span class="dt">degree =</span> i)
}

<span class="co"># График суммы квадратов ошибки при кросс-проверке:</span>
<span class="kw">qplot</span>(<span class="dv">1</span>:max.poly, cross, <span class="dt">data =</span> a, <span class="dt">geom =</span> <span class="kw">c</span>(<span class="st">&quot;line&quot;</span>)) +
<span class="st">    </span><span class="kw">xlab</span>(<span class="st">&quot;Степень полинома &quot;</span>) +<span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Квадратичная ошибка&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-2-6"></span>
<img src="022-Resampling-Techniques_files/figure-html/fig-2-6-1.png" alt="Поиск степени функции полиномиальной регрессии с использованием перекрестной проверки" width="672" />
<p class="caption">
Рисунок 2.6: Поиск степени функции полиномиальной регрессии с использованием перекрестной проверки
</p>
</div>
<p>Как видно из графика на рис. <a href="022-Resampling-Techniques.html#fig:fig-2-6">2.6</a>, по мере усложнения модели от линейной регрессии с одним предиктором до полинома 7-й степени ошибка предсказаний на проверочном множестве (test error) сначала снижается, а затем после достижения некоторого минимума начинает возрастать. Такая <span class="math inline">\(U\)</span>-образная форма поведения ошибки является универсальной и справедлива практически для любых алгоритмов и методов создания предсказательных моделей. Повторный рост ошибки на проверочных данных является сигналом о том, что модель стала переобученной. Результаты, представленные на рис. <a href="021-Model-Quality-Criteria.html#fig:fig-2-4">2.4</a> и <a href="022-Resampling-Techniques.html#fig:fig-2-6">2.6</a>, говорят о том, что в нашем конкретном случае оба метода одинаково выбирают в качестве наилучшей модели полином 4-й степени.</p>
<p>Проверим, является ли статистически значимым превышение качества полиномиальной модели над моделью с одним предиктором:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(M_poly &lt;-<span class="st"> </span><span class="kw">glm</span>(y ~<span class="st"> </span><span class="kw">poly</span>(x, <span class="dv">4</span>)))</code></pre></div>
<pre><code>## 
## Call:  glm(formula = y ~ poly(x, 4))
## 
## Coefficients:
## (Intercept)  poly(x, 4)1  poly(x, 4)2  poly(x, 4)3  poly(x, 4)4  
##       4.360      -16.750        4.751        3.946       -3.371  
## 
## Degrees of Freedom: 127 Total (i.e. Null);  123 Residual
## Null Deviance:       439.3 
## Residual Deviance: 109.2     AIC: 354.9</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(M_glm, M_poly, <span class="dt">test =</span> <span class="st">&quot;Chisq&quot;</span>)</code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: y ~ x
## Model 2: y ~ poly(x, 4)
##   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
## 1       126     158.71                          
## 2       123     109.21  3   49.501 4.743e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Имеет место статистически значимое снижение ошибки модели.</p>
<p>В среде R перекрестную проверку модели можно выполнить не только на основе самостоятельно оформленных процедур, но и с использованием нескольких общедоступных функций из известных пакетов:</p>
<ul>
<li><code>CVlm(df, form.lm, m = 3)</code> из пакета <code>DAAG</code>, где <code>df</code> - исходная таблица с данными, <code>form.lm</code> - формула линейной модели, <code>m</code> - число блоков (сюда подставляется значение <span class="math inline">\(k\)</span>);</li>
<li><code>cv.glm(df, glmfit, K = n)</code> из пакета <code>boot</code>, где <code>df</code> - исходная таблица с данными, <code>glmfit</code> - объект обобщенной линейной модели типа <code>glm</code>, <code>K</code> - число блоков (т.е. по умолчанию выполняется скользящий контроль);</li>
<li><code>cvLm(lmfit, folds = cvFolds(n, K, R), cost)</code> из пакета <code>cvTools</code>, где <code>lmfit</code> - объект обобщенной линейной модели <code>lm</code>, <code>K</code> - число блоков, <code>R</code> - число повторностей, <code>cost</code> - наименование одного из пяти разновидностей используемых критериев качества;</li>
<li><code>train(form, df, method, trainControl, ...)</code> из пакета caret, где <code>df</code> - исходная таблица с данными, <code>form</code>, <code>method</code> - формула и метод построения модели, <code>trainControl</code> - объект, в котором прописываются все необходимые параметры перекрестной проверки.</li>
</ul>
<p>Бутстреп-процедура состоит в том, чтобы случайным образом многократно извлекать повторные выборки из эмпирического распределения. Конкретно, если мы имеем исходную выборку из <span class="math inline">\(n\)</span> членов <span class="math inline">\(x_1, x_2, \dots, x_{n - 1}, x_n\)</span>, то с помощью датчика случайных чисел, равномерно распределенных на интервале <span class="math inline">\([1, n]\)</span>, можем вытянуть из нее произвольный элемент <span class="math inline">\(x_k\)</span>, который снова вернем в исходную выборку для возможного повторного извлечения. Такая процедура повторяется <span class="math inline">\(n\)</span> раз и образуется бутстреп-выборка, в которой одни элементы могут повторяться два или более раз, тогда как другие элементы - отсутствовать. Например, при <span class="math inline">\(n = 6\)</span> одна из таких бутстреп-комбинаций имеет вид <span class="math inline">\(x_4, x_2, x_2, x_1, x_4, x_5\)</span>.</p>
<p>В R бутстреп легко реализуется с помощью функции <code>sample(..., replace = TRUE)</code>, генерирующей любые случайные выборки с “возвращением”. Вероятность того, что конкретное наблюдение не войдет в бутстреп-выборку размера <span class="math inline">\(n\)</span>, равна <span class="math inline">\((1 - 1/n)n\)</span> и стремится к <span class="math inline">\(1/e = 0.368\)</span> при <span class="math inline">\(n \rightarrow \infty\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Nboot =<span class="st"> </span><span class="dv">1000</span>; n =<span class="st"> </span><span class="dv">100</span>
<span class="kw">mean</span>(<span class="kw">replicate</span>(Nboot, <span class="kw">length</span>(<span class="kw">unique</span>(<span class="kw">sample</span>(n, <span class="dt">replace =</span> <span class="ot">TRUE</span>)))))</code></pre></div>
<pre><code>## [1] 63.432</code></pre>
<p>Таким способом можно сформировать любое, сколь угодно большое число бутстреп-выборок (обычно 500-1000), каждая из которых содержит около 2/3 уникальных значений эмпирической совокупности.</p>
<p>В результате легкой модификации частотного распределения реализаций исходных данных можно ожидать, что каждая следующая генерируемая псевдовыборка будет обладать значением оцениваемого параметра, немного отличающемся от вычисленного для первоначальной совокупности. На основе разброса значений анализируемого показателя, полученного в ходе такого процесса имитации, можно построить, например, доверительные интервалы оцениваемого параметра.</p>
<p>В предыдущем разделе мы рассматривали использование информационных критериев для выбора оптимальной степени полинома. Осуществим оценку квантильных интервалов байесовского критерия <code>BIC</code> для каждой из модели-претендента:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">123</span>)
Nboot =<span class="st"> </span><span class="dv">1000</span>
BootMat &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span>:max.poly, function(k) 
    <span class="kw">replicate</span>(Nboot, {
        ind &lt;-<span class="st"> </span><span class="kw">sample</span>(n, <span class="dt">replace=</span>T)
        <span class="kw">BIC</span>(<span class="kw">glm</span>(y[ind]~<span class="kw">poly</span>(x[ind],k)))
    } )
)
<span class="kw">apply</span>(BootMat, <span class="dv">2</span>, mean)</code></pre></div>
<pre><code>## [1] 311.5186 316.0324 305.9600 303.1222 304.7347 308.1959 305.3907</code></pre>
<p>Искомые интервалы покажем на графике (рис. <a href="022-Resampling-Techniques.html#fig:fig-2-7">2.7</a>), подключив функцию <code>stat_summary()</code> из пакета <code>ggplot2</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(reshape) <span class="co"># для функции melt()</span>
BootMat.df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">melt</span>(BootMat))
<span class="kw">ggplot</span>(<span class="dt">data =</span> BootMat.df, <span class="kw">aes</span>(X2, value)) +
<span class="st">    </span><span class="kw">geom_jitter</span>(<span class="dt">position =</span> <span class="kw">position_jitter</span>(<span class="dt">width =</span> <span class="fl">0.1</span>), 
                <span class="dt">alpha =</span> <span class="fl">0.2</span>) +<span class="st"> </span>
<span class="st">    </span><span class="co"># Добавляем средние значения в виде точек красного цвета:</span>
<span class="st">    </span><span class="kw">stat_summary</span>(<span class="dt">fun.y =</span> mean, <span class="dt">geom =</span> <span class="st">&quot;point&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>,
                 <span class="dt">size =</span> <span class="dv">5</span>)  +<span class="st"> </span>
<span class="st">    </span><span class="co"># Добавляем отрезки, символизирующие 0.25 и 0.75 квантили:</span>
<span class="st">    </span><span class="kw">stat_summary</span>(<span class="dt">fun.y =</span> mean,
                 <span class="dt">fun.ymin =</span> function(x){<span class="kw">quantile</span>(x, <span class="dt">p =</span> <span class="fl">0.25</span>)},
                 <span class="dt">fun.ymax =</span> function(x){<span class="kw">quantile</span>(x, <span class="dt">p =</span> <span class="fl">0.75</span>)},
                 <span class="dt">geom =</span> <span class="st">&quot;errorbar&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;magenta&quot;</span>, <span class="dt">width =</span> <span class="fl">0.5</span>,
                 <span class="dt">size =</span><span class="fl">1.5</span>) +
<span class="st">    </span><span class="kw">xlab</span>(<span class="st">&quot;Степень полинома&quot;</span>) +<span class="st"> </span>
<span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;Информационный критерий Байеса&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-2-7"></span>
<img src="022-Resampling-Techniques_files/figure-html/fig-2-7-1.png" alt="Доверительные интервалы байесовского информационного критерия для разных степеней полинома" width="672" />
<p class="caption">
Рисунок 2.7: Доверительные интервалы байесовского информационного критерия для разных степеней полинома
</p>
</div>
<p>Для оценки 95%-х доверительных интервалов методом процентилей достаточно установить значения <code>p = c(0.025, 0.975)</code> (мы это не сделали исключительно из эстетических соображений). Из результатов бутстрепа можно сделать содержательные выводы, например: если доверительные интервалы смежных степеней полинома будут пересекаться, то уменьшение BIC статистически незначимо и вполне можно ограничиться более простой моделью. Подробнее о различных возможностях ресэмплинга см. книгу В. Шитикова и Г. Розенберга (2014).</p>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="021-Model-Quality-Criteria.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="023-Models-for-Class-Prediction.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["_main.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
