<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Классификация, регрессия и другие алгоритмы Data Mining с использованием R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Реализация алгоритмов Data Mining с использованием R">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Классификация, регрессия и другие алгоритмы Data Mining с использованием R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://ranalytics.github.io/data-mining/" />
  
  <meta property="og:description" content="Реализация алгоритмов Data Mining с использованием R" />
  <meta name="github-repo" content="ranalytics/data-mining" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Классификация, регрессия и другие алгоритмы Data Mining с использованием R" />
  
  <meta name="twitter:description" content="Реализация алгоритмов Data Mining с использованием R" />
  

<meta name="author" content="Шитиков В. К., Мастицкий С. Э.">


<meta name="date" content="2017-04-06">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="114-Descriptive-Models.html">
<link rel="next" href="130-Appendix.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Аннотация</a></li>
<li class="chapter" data-level="1" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html"><i class="fa fa-check"></i><b>1</b> Реализация моделей Data Mining в среде R (вместо предисловия)</a><ul>
<li class="chapter" data-level="1.1" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#section_1_1"><i class="fa fa-check"></i><b>1.1</b> Data Mining как направление анализа данных</a><ul>
<li class="chapter" data-level="1.1.1" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_1"><i class="fa fa-check"></i><b>1.1.1</b> От статистического анализа разового эксперимента к Data Mining</a></li>
<li class="chapter" data-level="1.1.2" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_2"><i class="fa fa-check"></i><b>1.1.2</b> Принципиальная множественность моделей окружающего мира</a></li>
<li class="chapter" data-level="1.1.3" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_3"><i class="fa fa-check"></i><b>1.1.3</b> Нарастающая множественность алгоритмов построения моделей</a></li>
<li class="chapter" data-level="1.1.4" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_4"><i class="fa fa-check"></i><b>1.1.4</b> Типы и характеристики групп моделей Data Mining</a></li>
<li class="chapter" data-level="1.1.5" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_5"><i class="fa fa-check"></i><b>1.1.5</b> Природа многомерного отклика и его моделирование</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="012-R-Intro.html"><a href="012-R-Intro.html"><i class="fa fa-check"></i><b>1.2</b> Статистическая среда R и ее использование в Data Mining</a></li>
<li class="chapter" data-level="1.3" data-path="013-What-This-Book-Is-About.html"><a href="013-What-This-Book-Is-About.html"><i class="fa fa-check"></i><b>1.3</b> О чем эта книга и чего в ней нет</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="021-Model-Quality-Criteria.html"><a href="021-Model-Quality-Criteria.html"><i class="fa fa-check"></i><b>2</b> Статистические модели: критерии и методы оценивания их качества</a><ul>
<li class="chapter" data-level="2.1" data-path="021-Model-Quality-Criteria.html"><a href="021-Model-Quality-Criteria.html#sec_2_1"><i class="fa fa-check"></i><b>2.1</b> Основные шаги построения и верификации моделей</a></li>
<li class="chapter" data-level="2.2" data-path="022-Resampling-Techniques.html"><a href="022-Resampling-Techniques.html"><i class="fa fa-check"></i><b>2.2</b> Использование алгоритмов ресэмплинга для тестирования моделей и оптимизации их параметров</a></li>
<li class="chapter" data-level="2.3" data-path="023-Models-for-Class-Prediction.html"><a href="023-Models-for-Class-Prediction.html"><i class="fa fa-check"></i><b>2.3</b> Модели для предсказания класса объектов</a></li>
<li class="chapter" data-level="2.4" data-path="024-Projecting-Data-onto-a-Plane.html"><a href="024-Projecting-Data-onto-a-Plane.html"><i class="fa fa-check"></i><b>2.4</b> Проецирование многомерных данных на плоскости</a></li>
<li class="chapter" data-level="2.5" data-path="025-MV-analysis.html"><a href="025-MV-analysis.html"><i class="fa fa-check"></i><b>2.5</b> Многомерный статистический анализ данных</a></li>
<li class="chapter" data-level="2.6" data-path="026-Clustering-Methods.html"><a href="026-Clustering-Methods.html"><i class="fa fa-check"></i><b>2.6</b> Методы кластеризации</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="031-Intro-to-Caret.html"><a href="031-Intro-to-Caret.html"><i class="fa fa-check"></i><b>3</b> Пакет <code>caret</code> - инструмент построения статистических моделей в R</a><ul>
<li class="chapter" data-level="3.1" data-path="031-Intro-to-Caret.html"><a href="031-Intro-to-Caret.html#---------caret"><i class="fa fa-check"></i><b>3.1</b> Универсальный интерфейс доступа к функциям машинного обучения в пакете <code id="sec_3_1">caret</code></a></li>
<li class="chapter" data-level="3.2" data-path="032-Removing-Predictors.html"><a href="032-Removing-Predictors.html"><i class="fa fa-check"></i><b>3.2</b> Обнаружение и удаление “ненужных” предикторов</a></li>
<li class="chapter" data-level="3.3" data-path="033-Preprocessing.html"><a href="033-Preprocessing.html"><i class="fa fa-check"></i><b>3.3</b> Предварительная обработка: преобразование и групповая трансформация переменных</a></li>
<li class="chapter" data-level="3.4" data-path="034-Handling-Missing-Values.html"><a href="034-Handling-Missing-Values.html"><i class="fa fa-check"></i><b>3.4</b> Заполнение пропущенных значений в данных</a></li>
<li class="chapter" data-level="3.5" data-path="035-The-train-Functions.html"><a href="035-The-train-Functions.html"><i class="fa fa-check"></i><b>3.5</b> Функция <code>train()</code> из пакета <code id="sec_3_5">caret</code></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html"><i class="fa fa-check"></i><b>4</b> Построение регрессионных моделей различного типа</a><ul>
<li class="chapter" data-level="4.1" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1"><i class="fa fa-check"></i><b>4.1</b> Селекция оптимального набора предикторов линейной модели</a><ul>
<li class="chapter" data-level="4.1.1" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_1"><i class="fa fa-check"></i><b>4.1.1</b> Полная регрессионная модель и пошаговая процедура</a></li>
<li class="chapter" data-level="4.1.2" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_2"><i class="fa fa-check"></i><b>4.1.2</b> Рекурсивное исключение переменных</a></li>
<li class="chapter" data-level="4.1.3" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_3"><i class="fa fa-check"></i><b>4.1.3</b> Генетический алгоритм</a></li>
<li class="chapter" data-level="4.1.4" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_4"><i class="fa fa-check"></i><b>4.1.4</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="042-Regularization.html"><a href="042-Regularization.html"><i class="fa fa-check"></i><b>4.2</b> Регуляризация, частные наименьшие квадраты и kNN-регрессия</a><ul>
<li class="chapter" data-level="4.2.1" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_1"><i class="fa fa-check"></i><b>4.2.1</b> Регрессия по методу “лассо”</a></li>
<li class="chapter" data-level="4.2.2" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_2"><i class="fa fa-check"></i><b>4.2.2</b> Метод частных наименьших квадратов (PLS)</a></li>
<li class="chapter" data-level="4.2.3" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_3"><i class="fa fa-check"></i><b>4.2.3</b> Регрессия по методу <em>k</em> ближайших соседей</a></li>
<li class="chapter" data-level="4.2.4" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_4"><i class="fa fa-check"></i><b>4.2.4</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html"><i class="fa fa-check"></i><b>4.3</b> Построение деревьев регрессии</a><ul>
<li class="chapter" data-level="4.3.1" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html#sec_4_3_1"><i class="fa fa-check"></i><b>4.3.1</b> Построение деревьев на основе рекурсивного разбиения</a></li>
<li class="chapter" data-level="4.3.2" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html#sec_4_3_2"><i class="fa fa-check"></i><b>4.3.2</b> Построение деревьев с использованием алгортма условного вывода</a></li>
<li class="chapter" data-level="4.3.3" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html#sec_4_3_3"><i class="fa fa-check"></i><b>4.3.3</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="044-Ensembles.html"><a href="044-Ensembles.html"><i class="fa fa-check"></i><b>4.4</b> Ансамбли моделей: бэггинг, случайные леса, бустинг</a><ul>
<li class="chapter" data-level="4.4.1" data-path="044-Ensembles.html"><a href="044-Ensembles.html#sec_4_4_1"><i class="fa fa-check"></i><b>4.4.1</b> Бэггинг и случайные леса</a></li>
<li class="chapter" data-level="4.4.2" data-path="044-Ensembles.html"><a href="044-Ensembles.html#sec_4_4_2"><i class="fa fa-check"></i><b>4.4.2</b> Бустинг</a></li>
<li class="chapter" data-level="4.4.3" data-path="044-Ensembles.html"><a href="044-Ensembles.html#sec_4_4_3"><i class="fa fa-check"></i><b>4.4.3</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="045-Comparing-Trees.html"><a href="045-Comparing-Trees.html"><i class="fa fa-check"></i><b>4.5</b> Сравнение построенных моделей и оценка информативности предикторов</a></li>
<li class="chapter" data-level="4.6" data-path="046-MV-Trees.html"><a href="046-MV-Trees.html"><i class="fa fa-check"></i><b>4.6</b> Деревья регрессии с многомерным откликом</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="051-Association-Rules.html"><a href="051-Association-Rules.html"><i class="fa fa-check"></i><b>5</b> Бинарные матрицы и ассоциативные правила</a><ul>
<li class="chapter" data-level="5.1" data-path="051-Association-Rules.html"><a href="051-Association-Rules.html#sec_5_1"><i class="fa fa-check"></i><b>5.1</b> Классификация в бинарных пространствах с использованием классических моделей</a></li>
<li class="chapter" data-level="5.2" data-path="052-Binary-Decision-Trees.html"><a href="052-Binary-Decision-Trees.html"><i class="fa fa-check"></i><b>5.2</b> Бинарные деревья решений</a></li>
<li class="chapter" data-level="5.3" data-path="053-Logic-Rules.html"><a href="053-Logic-Rules.html"><i class="fa fa-check"></i><b>5.3</b> Поиск логических закономерностей в данных</a></li>
<li class="chapter" data-level="5.4" data-path="054-Association-Rules-Algos.html"><a href="054-Association-Rules-Algos.html"><i class="fa fa-check"></i><b>5.4</b> Алгоритмы выделения ассоциативных правил</a></li>
<li class="chapter" data-level="5.5" data-path="055-Traminer.html"><a href="055-Traminer.html"><i class="fa fa-check"></i><b>5.5</b> Анализ последовательностей знаков или событий</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="061-Binary-Classifiers.html"><a href="061-Binary-Classifiers.html"><i class="fa fa-check"></i><b>6</b> Бинарные классификаторы с различными разделяющими поверхностями</a><ul>
<li class="chapter" data-level="6.1" data-path="061-Binary-Classifiers.html"><a href="061-Binary-Classifiers.html#sec_6_1"><i class="fa fa-check"></i><b>6.1</b> Дискриминантный анализ</a></li>
<li class="chapter" data-level="6.2" data-path="062-SVM.html"><a href="062-SVM.html"><i class="fa fa-check"></i><b>6.2</b> Дискриминантный анализ</a></li>
<li class="chapter" data-level="6.3" data-path="063-Nonlinear-Borders.html"><a href="063-Nonlinear-Borders.html"><i class="fa fa-check"></i><b>6.3</b> Ядерные функции машины опорных векторов</a></li>
<li class="chapter" data-level="6.4" data-path="064-Classification-Trees.html"><a href="064-Classification-Trees.html"><i class="fa fa-check"></i><b>6.4</b> Деревья классификации, случайный лес и логистическая регрессия</a></li>
<li class="chapter" data-level="6.5" data-path="065-Comparing-Classifiers.html"><a href="065-Comparing-Classifiers.html"><i class="fa fa-check"></i><b>6.5</b> Процедуры сравнения эффективности моделей классификации</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="071-Multiclass-Classification.html"><a href="071-Multiclass-Classification.html"><i class="fa fa-check"></i><b>7</b> Модели классификации для нескольких классов</a><ul>
<li class="chapter" data-level="7.1" data-path="071-Multiclass-Classification.html"><a href="071-Multiclass-Classification.html#sec_7_1"><i class="fa fa-check"></i><b>7.1</b> Ирисы Фишера и метод <em>k</em> ближайших соседей</a></li>
<li class="chapter" data-level="7.2" data-path="072-NBC.html"><a href="072-NBC.html"><i class="fa fa-check"></i><b>7.2</b> Наивный байесовский классификатор</a></li>
<li class="chapter" data-level="7.3" data-path="073-In-Discriminant-Space.html"><a href="073-In-Discriminant-Space.html"><i class="fa fa-check"></i><b>7.3</b> Классификация в линейном дискриминантном пространстве</a></li>
<li class="chapter" data-level="7.4" data-path="074-Nonlinear-Classifiers.html"><a href="074-Nonlinear-Classifiers.html"><i class="fa fa-check"></i><b>7.4</b> Нелинейные классификаторы в R</a></li>
<li class="chapter" data-level="7.5" data-path="075-Multinomial-Logit.html"><a href="075-Multinomial-Logit.html"><i class="fa fa-check"></i><b>7.5</b> Модель мультиномиального логита</a></li>
<li class="chapter" data-level="7.6" data-path="076-NN.html"><a href="076-NN.html"><i class="fa fa-check"></i><b>7.6</b> Классификаторы на основе искусственных нейронных сетей</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="081-Logit-for-Count.html"><a href="081-Logit-for-Count.html"><i class="fa fa-check"></i><b>8</b> Моделирование порядковых и счетных переменных</a><ul>
<li class="chapter" data-level="8.1" data-path="081-Logit-for-Count.html"><a href="081-Logit-for-Count.html#sec_8_1"><i class="fa fa-check"></i><b>8.1</b> Модель логита для порядковой переменной</a></li>
<li class="chapter" data-level="8.2" data-path="082-NN-with-Caret.html"><a href="082-NN-with-Caret.html"><i class="fa fa-check"></i><b>8.2</b> Настройка параметров нейронных сетей средствами пакета <code id="sec_8_2">caret</code></a></li>
<li class="chapter" data-level="8.3" data-path="083-Model-Complexes.html"><a href="083-Model-Complexes.html"><i class="fa fa-check"></i><b>8.3</b> Методы комплексации модельных прогнозов</a></li>
<li class="chapter" data-level="8.4" data-path="084-GLM-for-Counts.html"><a href="084-GLM-for-Counts.html"><i class="fa fa-check"></i><b>8.4</b> Обобщенные линейные модели для счетных данных</a></li>
<li class="chapter" data-level="8.5" data-path="085-ZIP-for-Counts.html"><a href="085-ZIP-for-Counts.html"><i class="fa fa-check"></i><b>8.5</b> ZIP- и барьерные модели счетных данных</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="091-Data-Transformation.html"><a href="091-Data-Transformation.html"><i class="fa fa-check"></i><b>9</b> Методы многомерной ординации</a><ul>
<li class="chapter" data-level="9.1" data-path="091-Data-Transformation.html"><a href="091-Data-Transformation.html#sec_9_1"><i class="fa fa-check"></i><b>9.1</b> Преобразование данных и вычисление матрицы расстояний</a></li>
<li class="chapter" data-level="9.2" data-path="092-Distance-ANOVA.html"><a href="092-Distance-ANOVA.html"><i class="fa fa-check"></i><b>9.2</b> Непараметрический дисперсионный анализ матриц дистанций</a></li>
<li class="chapter" data-level="9.3" data-path="093-Comparing-Diagrams.html"><a href="093-Comparing-Diagrams.html"><i class="fa fa-check"></i><b>9.3</b> Методы ординации объектов и переменных: построение и сравнение диаграмм</a></li>
<li class="chapter" data-level="9.4" data-path="094-Ordination-Factors.html"><a href="094-Ordination-Factors.html"><i class="fa fa-check"></i><b>9.4</b> Оценка связи ординации с внешними факторами</a></li>
<li class="chapter" data-level="9.5" data-path="095-NMDS.html"><a href="095-NMDS.html"><i class="fa fa-check"></i><b>9.5</b> Неметрическое многомерное шкалирование и построение распределения чувствительности видов</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="101-Partitioning-Algos.html"><a href="101-Partitioning-Algos.html"><i class="fa fa-check"></i><b>10</b> Кластерный анализ</a><ul>
<li class="chapter" data-level="10.1" data-path="101-Partitioning-Algos.html"><a href="101-Partitioning-Algos.html#sec_10_1"><i class="fa fa-check"></i><b>10.1</b> Алгоритмы кластеризации, основанные на разделении</a></li>
<li class="chapter" data-level="10.2" data-path="102-H-Clustering.html"><a href="102-H-Clustering.html"><i class="fa fa-check"></i><b>10.2</b> Иерархическая кластеризация</a></li>
<li class="chapter" data-level="10.3" data-path="103-Clustering-Quality.html"><a href="103-Clustering-Quality.html"><i class="fa fa-check"></i><b>10.3</b> Оценка качества кластеризации</a></li>
<li class="chapter" data-level="10.4" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html"><i class="fa fa-check"></i><b>10.4</b> Другие алгоритмы кластеризации</a><ul>
<li class="chapter" data-level="10.4.1" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html#sec_10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Иерархическая кластеризация на главные компоненты</a></li>
<li class="chapter" data-level="10.4.2" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html#sec_10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Метод нечетких <em>k</em> средних (fuzzy analysis clustering)</a></li>
<li class="chapter" data-level="10.4.3" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html#sec_10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Статистическая модель кластеризации</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="105-Cohonen-Maps.html"><a href="105-Cohonen-Maps.html"><i class="fa fa-check"></i><b>10.5</b> Самоорганизующиеся карты Кохонена</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="111-Rattle-Intro.html"><a href="111-Rattle-Intro.html"><i class="fa fa-check"></i><b>11</b> <code>rattle</code>: графический интерфейс R для реализации алгоритмов Data Mining</a><ul>
<li class="chapter" data-level="11.1" data-path="111-Rattle-Intro.html"><a href="111-Rattle-Intro.html#----rattle"><i class="fa fa-check"></i><b>11.1</b> Начало работы с пакетом <code id="sec_11_1">rattle</code></a></li>
<li class="chapter" data-level="11.2" data-path="112-Descriptive-Stats.html"><a href="112-Descriptive-Stats.html"><i class="fa fa-check"></i><b>11.2</b> Описательная статистика и визуализация данных</a></li>
<li class="chapter" data-level="11.3" data-path="113-Model-Building.html"><a href="113-Model-Building.html"><i class="fa fa-check"></i><b>11.3</b> Построение и тестирование моделей классификации</a></li>
<li class="chapter" data-level="11.4" data-path="114-Descriptive-Models.html"><a href="114-Descriptive-Models.html"><i class="fa fa-check"></i><b>11.4</b> Дескриптивные модели (обучение без учителя)</a><ul>
<li class="chapter" data-level="11.4.1" data-path="114-Descriptive-Models.html"><a href="114-Descriptive-Models.html#sec_11_4_1"><i class="fa fa-check"></i><b>11.4.1</b> Кластерный анализ</a></li>
<li class="chapter" data-level="11.4.2" data-path="114-Descriptive-Models.html"><a href="114-Descriptive-Models.html#sec_11_4_2"><i class="fa fa-check"></i><b>11.4.2</b> Ассоциативные правила</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="120-References.html"><a href="120-References.html"><i class="fa fa-check"></i><b>12</b> Список рекомендуемой литературы</a></li>
<li class="chapter" data-level="" data-path="130-Appendix.html"><a href="130-Appendix.html"><i class="fa fa-check"></i>Приложение: cправочная карта по Data Mining с использованием R</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Классификация, регрессия и другие алгоритмы Data Mining с использованием R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch_12" class="section level1">
<h1><span class="header-section-number">ГЛАВА 12</span> Список рекомендуемой литературы</h1>
<ul>
<li>Айвазян С.А., Бухштабер В.М., Енюков И.С., Мешалкин Л.Д. Прикладная статистика: Классификация и снижение размерности М.: Финансы и статистика, 1989. 607 с.</li>
<li>Айвазян С.А., Мхитарян В.С. Прикладная статистика и основы эконометрии. М.: ЮНИТИ, 1998. 1022 с.</li>
<li>Арапов М.В., Ефимова Е.Н., Шрейдер Ю.А. О смысле ранговых распределений // Науч.-техн. информ. 1975. Сер. 2. №1. С. 9–20.</li>
<li>Афифи А., Эйзен С. Статистический анализ: Подход с использованием ЭВМ. М.: Мир, 1982. 488 с.</li>
<li>Барсегян А.А., Куприянов М.С., Холод И.И. и др. Анализ данных и процессов. СПб.: БХВ-Петербург, 2009. 512 с.</li>
<li>Бирюкова С. Анализ последовательностей в R: TraMineR. НИУ ВШЭ. 2014. URL: <a href="https://www.hse.ru" class="uri">https://www.hse.ru</a></li>
<li>Бокс Дж., Дженкинс Г. Анализ временных рядов. Прогноз и управление. М.: Мир, 1974. Вып. 1. 406 с.</li>
<li>Бонгард М. М. Проблема узнавания. М.: Наука, 1967. 320 с.</li>
<li>Вайнцвайг М. Н. Алгоритм обучения распознаванию образов “Кора”. // Алгоритмы обучения распознаванию образов. Ред. В. Н. Вапник. М.: Сов. радио, 1973. С. 110-116.</li>
<li>Вапник В.Н. (ред.). Алгоритмы и программы восстановления зависимостей. М.: Наука, 1984. 816 с.</li>
<li>Вапник В.Н., Червоненкис А.Я. Теория распознавания образов. М.: Наука, 1974. 487 с.</li>
<li>Венэбльз У.Н., Смит Д.М. Введение в R. Заметки по R: среда программирования для анализа данных и графики. Вер. 3.1.0. Москва, 2014. 109 с.</li>
<li>Воробейчик Е.Л. О некоторых индексах ширины и перекрывания экологических ниш // Журн. общ. биологии. 1993. Т. 54, № 6. С. 706-712.</li>
<li>Горбань А.Н., Дунин-Барковский В.Л., Миркес Е.М. и др. Нейроинформатика. Новосибирск: Наука. Сиб. предприятие РАН, 1998. 296 с.</li>
<li>Горбач А.Н., Цейтлин Н.А. Покупательское поведение: анализ спонтанных последовательностей и регрессионных моделей в маркетинговых исследованиях. Киев: Освiта УкраЇны, 2011. 220 с.</li>
<li>Джеймс Г., Уиттон Д., Хасти Т., Тибширани Р. Введение в статистическое обучение с примерами на языке R. Пер. С.Э. Мастицкого. М.: ДМК Пресс, 2016. 450 с.</li>
<li>Джонгман Р.Г.Г., тер Браак С.Дж.Ф., ван Тонгерен О.Ф.Р. Анализ данных в экологии сообществ и ландшафтов. М.: РАСХН, 1999. 306 с.</li>
<li>Дрейпер Н., Смит Г. Прикладной регрессионный анализ. М.: Финансы и статистика. Кн. 1, 1986. 366 с. Кн. 2, 1987. 352 с.</li>
<li>Дэйвисон М. Многомерное шкалирование. Методы наглядного представления данных. М.: Финансы и статистика, 1988. 348с.</li>
<li>Дюк В.А., Самойленко А.П. Data Mining: учебный курс. СПб.: Питер, 2001. 368 с. Загоруйко Н.Г. Прикладные методы анализа данных и знаний. Новосибирск: ИМ СО РАН, 1999. 270 с.</li>
<li>Зайцев К.С. Применение методов Data Mining для поддержки процессов управления IT-услугами. М.: МИФИ, 2009. 96 с.</li>
<li>Зарядов И.С. Введение в статистический пакет R: типы переменных, структуры данных, чтение и запись информации, графика. Москва: Изд-во РУДНБ, 2010а. 207 с.</li>
<li>Зарядов И.С. Статистический пакет R: теория вероятностей и математическая статистика. Москва: Изд-во РУДНБ, 2010б. 141 с.<br />
</li>
<li>Зиновьев А.Ю. Визуализация многомерных данных. Красноярск: КГТУ, 2000. 168 с.</li>
<li>Кабаков Р.И. R в действии: Анализ и визуализация данных в программе. М.: ДМК Пресс, 2014. 580 с.</li>
<li>Кендалл М., Стьюарт А. Многомерный статистический анализ и временные ряды. М.: Наука, 1976. 736 с.</li>
<li>Кендалл М., Стьюарт А. Статистические выводы и связи. М.: Наука, 1973. 899 с.</li>
<li>Ким Дж.-О., Мюллер Ч.У., Клекка У.Р. и др. Факторный, дискриминантный и кластерный анализ. М.: Финансы и статистика, 1989. 215 с.</li>
<li>Классификация и кластер. / Под ред. Дж. Вэн-Райзина. М.: Мир, 1980. 390 с.</li>
<li>Кобзарь А.И. Прикладная математическая статистика. Для инженеров и научных работников. М.: Физматлит, 2006. 816 с.</li>
<li>Краскэл Дж.Б. Многомерное шкалирование и другие методы поиска структуры // Статистические методы для ЭВМ / Под ред. К. Энслейна, Э. Рэлстона, Г. С. Уилфа. М.: Наука, 1986. С. 301-347.</li>
<li>Кудрин Б.И. Математика ценозов: видовое, ранговидовое, ранговое по параметру гиперболические Н-распределения и законы Лотки, Ципфа, Парето, Мандельброта // Математический аппарат структурного описания ценозов и гиперболические Н-ограничения. Ценологические исследования. М.: Центр системн. исслед., 2002. Вып. 19. С. 357-412.</li>
<li>Кшнясев И.А. Анализ обилия организмов: мультимодельный вывод как альтернатива проверки нуль-гипотезы. // Биологические системы: устойчивость, принципы и механизмы функционирования. Сб. материалов III Всеросс. Науч.-практ. Конф. Нижний Тагил, 2010. С. 348-353. URL: <a href="https://www.ipae.uran.ru/user/69" class="uri">https://www.ipae.uran.ru/user/69</a>.</li>
<li>Лбов Г.С. Методы обработки разнотипных экспериментальных данных. Новосибирск: Наука, 1981. 160 с.</li>
<li>Ллойд Э., Ледерман У. (ред). Справочник по прикладной статистике. В 2-х т. М.: Финансы и статистика. Т. 1. 1989. 510 с., Т. 2. 1990. 526 с.</li>
<li>Люк Д. Анализ сетей (графов) в среде R. Руководство пользователя. М.: ДМК Пресс, 2016.</li>
<li>Мастицкий С.Э. Визуализация данных с помощью ggplot2. М.: ДМК Пресс, 2016.</li>
<li>Мастицкий С.Э., Шитиков В.К. Статистический анализ и визуализация данных с помощью R. М.: ДМК Пресс, 2015. 496 с. URL: <a href="http://ievbras.ru/ecostat" class="uri">http://ievbras.ru/ecostat</a></li>
<li>Налимов В.В. Теория эксперимента. М.: Наука, 1971. 207 с.</li>
<li>Огнева Д. Пакет “arules” системы R. МГУ им. Ломоносова. 2012. URL: &lt;www.machinelearning.ru&gt;</li>
<li>Розенберг Г.С. Введение в теоретическую экологию. / В 2-х т.; Тольятти: Кассандра, 2013. Т. 1. 565 с. URL: <a href="http://ievbras.ru/ecostat" class="uri">http://ievbras.ru/ecostat</a></li>
<li>Розенберг Г.С., Шитиков В.К., Брусиловский П.М. Экологическое прогнозирование (Функциональные предикторы временных рядов). Тольятти: ИЭВБ РАН, 1994. 185 c. URL: <a href="http://ievbras.ru/ecostat" class="uri">http://ievbras.ru/ecostat</a></li>
<li>Самарский А.А. Что такое вычислительный эксперимент? // Наука и жизнь. 1979. № 3. С. 27-33.</li>
<li>Флах П. Машинное обучение. Наука и искусство построения алгоритмов, которые извлекают знания из данных. М.: ДМК Пресс, 2015. 400 с.</li>
<li>Форрестер Дж. Антиинтуитивное поведение сложных систем // Современные проблемы кибернетики. М.: Знание, 1977. С. 9-25.</li>
<li>Хокинс Д., Блейксли С. Об интеллекте. М.: ООО «И.Д.Вильямс», 2007. 204 с.</li>
<li>Храмов Д.А. Сбор данных в Интернете на языке R. . М.: ДМК Пресс, 2016.</li>
<li>Хромов-Борисов Н.Н. Синдром статистической снисходительности или значение и назначение p-значения // Телеконференция по медицине, биологии и экологии, 2011. №4. URL: <a href="http://tele-conf.ru" class="uri">http://tele-conf.ru</a> .</li>
<li>Чубукова И.А. Data Mining. Курс лекций интернет-университета INTUIT, 2006. 328 с.</li>
<li>Шипунов А.Б. и др. Наглядная статистика. Используем R! М.: ДМК Пресс, 2014. 298 с.</li>
<li>Шитиков В.К. Экотоксикология и статистическое моделирование эффекта с использованием R. Тольятти: ИЭВБ РАН , 2016. 149 с. URL: <a href="http://ievbras.ru/ecostat" class="uri">http://ievbras.ru/ecostat</a></li>
<li>Шитиков В.К., Зинченко Т.Д., Розенберг Г.С. Макроэкология речных сообществ: концепции, методы, модели. Тольятти: СамНЦ РАН, Кассандра, 2012. 257 с. URL: <a href="http://ievbras.ru/ecostat" class="uri">http://ievbras.ru/ecostat</a></li>
<li>Шитиков В.К., Розенберг Г.С. Рандомизация и бутстреп: статистический анализ в биологии и экологии с использованием R. Тольятти : Кассандра, 2014. 314 с. URL: URL: <a href="http://ievbras.ru/ecostat" class="uri">http://ievbras.ru/ecostat</a></li>
<li>Эфрон Б. Нетрадиционные методы многомерного статистического анализа. М.: Финансы и статистика, 1988. 263 с.</li>
<li>Abdi H., Williams L. Principal component analysis // WIREs Computational Statistics. 2010. V. 2 (4). P. 433-459. URL: <a href="http://onlinelibrary.wiley.com/doi/10.1002/wics.101/abstract">http://onlinelibrary.wiley.com</a></li>
<li>Agrawal R., Srikant R. Fast Discovery of Association Rules. // Proc. of the 20th Intern. Conf. on VLDB. Santiago, Chile, 1994.</li>
<li>Agresti A. An introduction to categorical data analysis. Wiley. 2007. 372 р. Anderson D. R. Model-based inference in life sciences: A primer on evidence. Springer, 2008. 184 p.</li>
<li>Banfield J., Raftery A. Model-Based Gaussian and Non-Gaussian Clustering. // Biometrics. V. 49. P. 803-821.</li>
<li>Bates D.M., Watts D.G. Nonlinear Regression Analysis and Its Applications. New-York: John Wiley, 2007. 392 p.</li>
<li>Bezdek J.C. Pattern Recognition with Fuzzy Objective Function Algorithms. New York: Plenum Press, 1981.</li>
<li>Borcard D., Gillet F., Legendre P. Numerical Ecology with R. N.Y.: Springer, 2011. 306 p.</li>
<li>Box G., Cox D.R. An analysis of Transformation // Journal of Royal Statistical Society B. 1964. V. 26. P. 211-243.</li>
<li>Boyce R.L, Ellison P.C Choosing the best similarity index when performing fuzzy set ordination on binary data // Journal of Vegetation Science. 2001. V. 12. P. 711-720.</li>
<li>Breiman L. Random forests // Machine Learning. 2001. V. 45 (1). P. 5-32.</li>
<li>Breiman L., Friedman J.H., Olshen R.A. et al. Classiﬁcation and Regression Trees. Belmont (CA): Wadsworth Int. Group, 1984. 368 p.</li>
<li>Burnham K.P., Anderson D.R. Model selection and multimodel inference: a practical information-theoretic approach. N.Y.: Springer-Verlag, 2002. 496 p.</li>
<li>Cameron A.C, Trivedi P.K. Regression Analysis of Count Data. Cambridge University Press, Cambridge. 2013.</li>
<li>Chiu YW. Machine learning with R. Cookbook. Packt Publishing, 2015.</li>
<li>Cristianini N., Shawe-Taylor J. An Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press, 2000.</li>
<li>Davison A.C., Hinkley D.V. Bootstrap methods and their application. Cambridge: Cambridge University Press, 2006. 592 p.</li>
<li>De’Ath G. Multivariate regression trees: a new technique for modeling species environment relationships // Ecology. 2002. V.83. P. 1105-1117.</li>
<li>Delgado M.F., Cernadas E., Barro S., Amorim D. Do we Need Hundreds of Classifiers to Solve Real World Classification Problems? // Journal of Machine Learning Research. 2014. V. 15. P. 3133-3181.</li>
<li>Edgington E.S. Randomization tests. N.Y.:Marcel Dekker, 1995. 341 p.</li>
<li>Everitt B.S., Howell D.C. Encyclopedia of Statistics in Behavioral Science. Chichester: Wiley &amp; Sons Ltd., 2005. 2132 p.</li>
<li>Faraway J.J. Extending the Linear Model with R: Generalized Linear, Mixed Eﬀects and Nonparametric Regression Models. Chapman, Hall/CRC, 2006. 345 p.</li>
<li>Fox J. Applied Regression Analysis and Generalized Linear Models, 2nd ed. Sage Publications, 2008.</li>
<li>Gabadinho A., Ritschard G., Muller N.S., Studer M. Analyzing and Visualizing State Sequences in R with TraMineR. // Journal of Statistical Software. 2011. V.40 (4). p. 1-37. URL: <a href="http://www.jstatsoft.org/v40/i04/" class="uri">http://www.jstatsoft.org/v40/i04/</a>.</li>
<li>Gabadinho A., Ritschard G., Muller N.S., Studer M. Mining sequence data in R with the TraMineR package: A user’s guide. University of Geneva, Switzerland. 2011а. URL: <a href="http://mephisto.unige.ch/traminer/" class="uri">http://mephisto.unige.ch/traminer/</a></li>
<li>Gibb S., Strimmer K. Differential protein expression and peak selection in mass spectrometry data by binary discriminant analysis. // Bioinformatics. 2015. 31(19), 3156–3162. URL: <a href="http://strimmerlab.org/software/binda" class="uri">http://strimmerlab.org/software/binda</a></li>
<li>Glur C. data.tree sample applications. 2016. URL: <a href="http://ipub.com/" class="uri">http://ipub.com/</a></li>
<li>Goddard M.J., Hinberg I. Receiver operator characteristic (ROC) curves and non-normal data: An empirical study. // Statistics in Medicine. 1989. Vol. 9 (3). P. 325–337.</li>
<li>Hahsler M., Grun B, Hornik K., Buchta C. Introduction to arules - A computational environment for mining association rules and frequent item sets. URL: <a href="https://cran.r-project.org" class="uri">https://cran.r-project.org</a></li>
<li>Hand D. Classifier Technology and the Illusion of Progress // Statistical Science. 2006. V. 21. P. 1-14.</li>
<li>Hastie T., Tibshirani R., Friedman J. The Elements of Statistical Learning: Data Mining, Inference and Prediction. N.Y.: Springer-Verlag, 2009. 763 p.</li>
<li>Hunt E.B., Marin J., Stone P.J. Experiments in Induction. New York: Academic Press, 1966.</li>
<li>Kassambara A. Practical Guide to Cluster Analysis in R: Unsupervised Machine Learning. Statistical tools for high-throughput data analysis STHDA, 2017. URL: <a href="http://www.sthda.com/english/wiki/cluster-analysis-in-r-unsupervised-machine-learning">http://www.sthda.com/</a></li>
<li>Kaufman L., Rousseeuw P.J. Finding Groups in Data. An Introduction to Cluster Analysis. Hoboken (NJ): Wiley &amp; Sons Inc., 2005. P. 342.</li>
<li>Kohonen T. Self-organized formation of topologically correct feature maps // Biological Cybernetics. 1982. № 43. P. 59-69.</li>
<li>Kruskal J.B. Multidimensional Scaling by Optimizing Goodness-of-Fit to a Nonmetric Hypothesis. // Psychometrika. 1964. V. 29. P. 1–28.</li>
<li>Kuhn M. Building Predictive Models in R Using the caret Package // Journal of Statistical Software. 2008. V. 5. P. 113-142. DOI 10.18637/jss.v028.i05</li>
<li>Kuhn M. Predictive Modeling with R and the caret Package. 2013. URL: <a href="https://www.r-project.org/nosvn/conferences/useR-2013/Tutorials/Kuhn.html">https://www.r-project.org/</a></li>
<li>Kuhn M. Variable Selection Using The caret Package. 2012. URL: <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.168.1655&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu</a></li>
<li>Kuhn M., Johnson K. Applied Predictive Modeling. Springer-Verlag New-York, 2013. 574 p.</li>
<li>Kursa M., Rudnicki W. Feature Selection with the Boruta Package // Journal of Statistical Software. 2010. V.36 (11). P. 2-12.</li>
<li>Le S., Josse J, Husson F. FactoMineR: An R Package for Multivariate Analysis // Journal of Statistical Software. 2008. V. 25 (1).</li>
<li>Legendre P, Legendre L. Numerical Ecology. Amsterdam: Elsevier Sci. BV, 2012.</li>
<li>Legendre P., Gallagher E. Ecologically meaningful transformations for ordination of species data // Oecologia. 2001. V. 129. P. 271-280.</li>
<li>Leskovec J., Rajaraman A., Ullman J. The Mining of Massive Datasets. Cambridge University Press. 2014. URL: <a href="http://www.mmds.org/#book" class="uri">http://www.mmds.org/#book</a></li>
<li>Loh W.-Y, Shih Y.-S. Split selection methods for classification trees. // Statistica Sinica1997. V. 7. Р. 815-840.</li>
<li>MacQueen J. Some methods for classification and analysis of multivariate observations. // The Fifth Berkeley Symposium on Mathematical Statistics and Probability. eds L.M. Le Cam, J. Neyman. Berkeley, CA: University of California Press. 1967. P. 281–297.</li>
<li>Maindonald J., Braun W.J. Data Analysis and Graphics Using R. Cambridge: University Press, 2010. 525 p.</li>
<li>McArdle B.H., Anderson M.J. Fitting multivariate models to community data: a comment on distance-based redundancy analysis // Ecology. 2001. V. 82, № 1. P. 290-297.</li>
<li>McCune B., Grace J.B., Urban D.L. Analysis of Ecological Communities. Gleneden Beach (OR): MjM Software, 2002. 285 p.</li>
<li>Mitchell M. An Introduction to Genetic Algorithms. Cambridge, MA: MIT Press. 1996.</li>
<li>Mount J., Zumel N. Exploring Data Science. Manning Publications Co., 2016. 184 p.</li>
<li>Mount J., Zumel N. Practical Data Science with R. Manning Publications Co., 2014. 416 p.</li>
<li>Myers W.L., Patil G.P. Multivariate Methods of Representing Relations in R for Prioritization Purposes: Selective Scaling, Comparative Clustering, Collective Criteria, and Sequenced Sets. N.-Y.: Springer, 2012. 297 р.</li>
<li>Oksanen J., Blanchet F.G., Kindt R. et al. vegan: Community Ecology Package. R package version 2.0-2. 2011. URL: <a href="https://cran.r-project.org/web/packages/vegan">https://cran.r-project.org</a>.</li>
<li>Quinlan J.R. Induction of Decision Trees. // Mach. Learn. 1986. V. 1 (1). P. 81-106</li>
<li>Ripley B.D. Pattern Recognition and Neural Networks. Cambridge University Press, 1996.</li>
<li>Sakurai S. Theory and Applications for Advanced Text Mining. InTech Chapters published, 2012. URL: <a href="http://www.intechopen.com/books/theory-and-applications-for-advanced-text-mining">http://www.intechopen.com</a></li>
<li>Serneels S., Van Espen P., De Nolf E. Spatial sign preprocessing: a simple way to impart moderate robustness to multivariate estimators. // J. Chem. Inf. Model. 2006. V. 46 (3). P. 1402-1409</li>
<li>Shimodaira H. An approximately unbiased test of phylogenetic tree selection // Syst. Biol. 2002. V. 51. P. 492-508.</li>
<li>Stanton JM. Introduction To Data Science. Syracuse University, 2013.</li>
<li>Torgo L. Data mining with R : learning with case studies. Chapman &amp; Hall/CRC, 2011. 272 p.</li>
<li>Vapnik V.N. The Nature of Statistical Learning Theory. Springer, 1995</li>
<li>Wang W., Yang J. Mining Sequential Patterns from Large Data Sets. N.-Y.: Springer, 2005. 162 р.</li>
<li>Williams G. Data Mining with Rattle and R: The Art of Excavating Data for Knowledge Discovery. N.-Y.: Springer, 2011. 365 р.</li>
<li>Williams G. Rattle: A Data Mining GUI for R // Journal of Statistical Software. 2009. V. 2(1). P. 45-55. URL: <a href="http://journal.r-project.org/archive/2009-2/RJournal_2009-2_Williams.pdf">http://journal.r-project.org</a></li>
<li>Wood S.N. Generalized Additive Models: An Introduction with R. Chapman, Hall/CRC, 2006. 410 p.</li>
<li>Chiu Y-W. (Chiu D.). Machine Learning with R Cookbook. Packt Publising, 2015. 405 p.</li>
<li>Zacharski R. A Programmer’s Guide to Data Mining. 2015. URL: <a href="http://guidetodatamining.com/" class="uri">http://guidetodatamining.com/</a></li>
<li>Zafarani R., Abbasi M., Liu H. Social Media Mining. An Introduction. Cambridge University Press, 2014. URL: <a href="http://dmml.asu.edu/smm/" class="uri">http://dmml.asu.edu/smm/</a></li>
<li>Zaki M., Meira W. Data Mining and Analysis. Fundamental Concepts and Algorithms. Cambridge University Press, 2014. URL: <a href="http://www.dataminingbook.info/" class="uri">http://www.dataminingbook.info/</a></li>
<li>Zeileis A., Kleiber C., Jackman S. Regression Models for Count Data in R. // Journal of Statistical Software. 2008. V. 27(8). URL: <a href="https://www.jstatsoft.org/article/view/v027i08">https://www.jstatsoft.org</a></li>
<li>Zhao Y. R and Data Mining: Examples and case studies. Academic Press, 2012. 256 p.</li>
<li>Zhao Y., Zhang C., Cao L. Post-Mining of Association Rules: Techniques for Effective Knowledge Extraction. Information Science Reference. 2009.</li>
<li>Zhao Y., Cen Y. Data Mining Applications with R. Academic Press, 2014. 470 p.</li>
<li>Zuur A. F., Ieno E.N., Walker N. et al. Mixed Effects, Models and Extensions in Ecology with R. Berlin: Springer Sci., 2009. 574 p. URL: <a href="http://www.highstat.com" class="uri">http://www.highstat.com</a></li>
</ul>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="114-Descriptive-Models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="130-Appendix.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["_main.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
