<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Классификация, регрессия и другие алгоритмы Data Mining с использованием R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Реализация алгоритмов Data Mining с использованием R">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Классификация, регрессия и другие алгоритмы Data Mining с использованием R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://ranalytics.github.io/data-mining/" />
  
  <meta property="og:description" content="Реализация алгоритмов Data Mining с использованием R" />
  <meta name="github-repo" content="ranalytics/data-mining" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Классификация, регрессия и другие алгоритмы Data Mining с использованием R" />
  
  <meta name="twitter:description" content="Реализация алгоритмов Data Mining с использованием R" />
  

<meta name="author" content="Шитиков В. К., Мастицкий С. Э.">


<meta name="date" content="2017-04-07">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="042-Regularization.html">
<link rel="next" href="044-Ensembles.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Аннотация</a></li>
<li class="chapter" data-level="1" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html"><i class="fa fa-check"></i><b>1</b> Реализация моделей Data Mining в среде R (вместо предисловия)</a><ul>
<li class="chapter" data-level="1.1" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#section_1_1"><i class="fa fa-check"></i><b>1.1</b> Data Mining как направление анализа данных</a><ul>
<li class="chapter" data-level="1.1.1" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_1"><i class="fa fa-check"></i><b>1.1.1</b> От статистического анализа разового эксперимента к Data Mining</a></li>
<li class="chapter" data-level="1.1.2" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_2"><i class="fa fa-check"></i><b>1.1.2</b> Принципиальная множественность моделей окружающего мира</a></li>
<li class="chapter" data-level="1.1.3" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_3"><i class="fa fa-check"></i><b>1.1.3</b> Нарастающая множественность алгоритмов построения моделей</a></li>
<li class="chapter" data-level="1.1.4" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_4"><i class="fa fa-check"></i><b>1.1.4</b> Типы и характеристики групп моделей Data Mining</a></li>
<li class="chapter" data-level="1.1.5" data-path="01-Data-Mining-Models-in-R.html"><a href="01-Data-Mining-Models-in-R.html#sec_1_1_5"><i class="fa fa-check"></i><b>1.1.5</b> Природа многомерного отклика и его моделирование</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="012-R-Intro.html"><a href="012-R-Intro.html"><i class="fa fa-check"></i><b>1.2</b> Статистическая среда R и ее использование в Data Mining</a></li>
<li class="chapter" data-level="1.3" data-path="013-What-This-Book-Is-About.html"><a href="013-What-This-Book-Is-About.html"><i class="fa fa-check"></i><b>1.3</b> О чем эта книга и чего в ней нет</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="021-Model-Quality-Criteria.html"><a href="021-Model-Quality-Criteria.html"><i class="fa fa-check"></i><b>2</b> Статистические модели: критерии и методы оценивания их качества</a><ul>
<li class="chapter" data-level="2.1" data-path="021-Model-Quality-Criteria.html"><a href="021-Model-Quality-Criteria.html#sec_2_1"><i class="fa fa-check"></i><b>2.1</b> Основные шаги построения и верификации моделей</a></li>
<li class="chapter" data-level="2.2" data-path="022-Resampling-Techniques.html"><a href="022-Resampling-Techniques.html"><i class="fa fa-check"></i><b>2.2</b> Использование алгоритмов ресэмплинга для тестирования моделей и оптимизации их параметров</a></li>
<li class="chapter" data-level="2.3" data-path="023-Models-for-Class-Prediction.html"><a href="023-Models-for-Class-Prediction.html"><i class="fa fa-check"></i><b>2.3</b> Модели для предсказания класса объектов</a></li>
<li class="chapter" data-level="2.4" data-path="024-Projecting-Data-onto-a-Plane.html"><a href="024-Projecting-Data-onto-a-Plane.html"><i class="fa fa-check"></i><b>2.4</b> Проецирование многомерных данных на плоскости</a></li>
<li class="chapter" data-level="2.5" data-path="025-MV-analysis.html"><a href="025-MV-analysis.html"><i class="fa fa-check"></i><b>2.5</b> Многомерный статистический анализ данных</a></li>
<li class="chapter" data-level="2.6" data-path="026-Clustering-Methods.html"><a href="026-Clustering-Methods.html"><i class="fa fa-check"></i><b>2.6</b> Методы кластеризации</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="031-Intro-to-Caret.html"><a href="031-Intro-to-Caret.html"><i class="fa fa-check"></i><b>3</b> Пакет <code>caret</code> - инструмент построения статистических моделей в R</a><ul>
<li class="chapter" data-level="3.1" data-path="031-Intro-to-Caret.html"><a href="031-Intro-to-Caret.html#---------caret"><i class="fa fa-check"></i><b>3.1</b> Универсальный интерфейс доступа к функциям машинного обучения в пакете <code id="sec_3_1">caret</code></a></li>
<li class="chapter" data-level="3.2" data-path="032-Removing-Predictors.html"><a href="032-Removing-Predictors.html"><i class="fa fa-check"></i><b>3.2</b> Обнаружение и удаление “ненужных” предикторов</a></li>
<li class="chapter" data-level="3.3" data-path="033-Preprocessing.html"><a href="033-Preprocessing.html"><i class="fa fa-check"></i><b>3.3</b> Предварительная обработка: преобразование и групповая трансформация переменных</a></li>
<li class="chapter" data-level="3.4" data-path="034-Handling-Missing-Values.html"><a href="034-Handling-Missing-Values.html"><i class="fa fa-check"></i><b>3.4</b> Заполнение пропущенных значений в данных</a></li>
<li class="chapter" data-level="3.5" data-path="035-The-train-Functions.html"><a href="035-The-train-Functions.html"><i class="fa fa-check"></i><b>3.5</b> Функция <code>train()</code> из пакета <code id="sec_3_5">caret</code></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html"><i class="fa fa-check"></i><b>4</b> Построение регрессионных моделей различного типа</a><ul>
<li class="chapter" data-level="4.1" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1"><i class="fa fa-check"></i><b>4.1</b> Селекция оптимального набора предикторов линейной модели</a><ul>
<li class="chapter" data-level="4.1.1" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_1"><i class="fa fa-check"></i><b>4.1.1</b> Полная регрессионная модель и пошаговая процедура</a></li>
<li class="chapter" data-level="4.1.2" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_2"><i class="fa fa-check"></i><b>4.1.2</b> Рекурсивное исключение переменных</a></li>
<li class="chapter" data-level="4.1.3" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_3"><i class="fa fa-check"></i><b>4.1.3</b> Генетический алгоритм</a></li>
<li class="chapter" data-level="4.1.4" data-path="041-Regression-Models.html"><a href="041-Regression-Models.html#sec_4_1_4"><i class="fa fa-check"></i><b>4.1.4</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="042-Regularization.html"><a href="042-Regularization.html"><i class="fa fa-check"></i><b>4.2</b> Регуляризация, частные наименьшие квадраты и kNN-регрессия</a><ul>
<li class="chapter" data-level="4.2.1" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_1"><i class="fa fa-check"></i><b>4.2.1</b> Регрессия по методу “лассо”</a></li>
<li class="chapter" data-level="4.2.2" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_2"><i class="fa fa-check"></i><b>4.2.2</b> Метод частных наименьших квадратов (PLS)</a></li>
<li class="chapter" data-level="4.2.3" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_3"><i class="fa fa-check"></i><b>4.2.3</b> Регрессия по методу <em>k</em> ближайших соседей</a></li>
<li class="chapter" data-level="4.2.4" data-path="042-Regularization.html"><a href="042-Regularization.html#sec_4_2_4"><i class="fa fa-check"></i><b>4.2.4</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html"><i class="fa fa-check"></i><b>4.3</b> Построение деревьев регрессии</a><ul>
<li class="chapter" data-level="4.3.1" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html#sec_4_3_1"><i class="fa fa-check"></i><b>4.3.1</b> Построение деревьев на основе рекурсивного разбиения</a></li>
<li class="chapter" data-level="4.3.2" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html#sec_4_3_2"><i class="fa fa-check"></i><b>4.3.2</b> Построение деревьев с использованием алгортма условного вывода</a></li>
<li class="chapter" data-level="4.3.3" data-path="043-Decision-Trees.html"><a href="043-Decision-Trees.html#sec_4_3_3"><i class="fa fa-check"></i><b>4.3.3</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="044-Ensembles.html"><a href="044-Ensembles.html"><i class="fa fa-check"></i><b>4.4</b> Ансамбли моделей: бэггинг, случайные леса, бустинг</a><ul>
<li class="chapter" data-level="4.4.1" data-path="044-Ensembles.html"><a href="044-Ensembles.html#sec_4_4_1"><i class="fa fa-check"></i><b>4.4.1</b> Бэггинг и случайные леса</a></li>
<li class="chapter" data-level="4.4.2" data-path="044-Ensembles.html"><a href="044-Ensembles.html#sec_4_4_2"><i class="fa fa-check"></i><b>4.4.2</b> Бустинг</a></li>
<li class="chapter" data-level="4.4.3" data-path="044-Ensembles.html"><a href="044-Ensembles.html#sec_4_4_3"><i class="fa fa-check"></i><b>4.4.3</b> Тестирование моделей с использованием дополнительного набора данных</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="045-Comparing-Trees.html"><a href="045-Comparing-Trees.html"><i class="fa fa-check"></i><b>4.5</b> Сравнение построенных моделей и оценка информативности предикторов</a></li>
<li class="chapter" data-level="4.6" data-path="046-MV-Trees.html"><a href="046-MV-Trees.html"><i class="fa fa-check"></i><b>4.6</b> Деревья регрессии с многомерным откликом</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="051-Association-Rules.html"><a href="051-Association-Rules.html"><i class="fa fa-check"></i><b>5</b> Бинарные матрицы и ассоциативные правила</a><ul>
<li class="chapter" data-level="5.1" data-path="051-Association-Rules.html"><a href="051-Association-Rules.html#sec_5_1"><i class="fa fa-check"></i><b>5.1</b> Классификация в бинарных пространствах с использованием классических моделей</a></li>
<li class="chapter" data-level="5.2" data-path="052-Binary-Decision-Trees.html"><a href="052-Binary-Decision-Trees.html"><i class="fa fa-check"></i><b>5.2</b> Бинарные деревья решений</a></li>
<li class="chapter" data-level="5.3" data-path="053-Logic-Rules.html"><a href="053-Logic-Rules.html"><i class="fa fa-check"></i><b>5.3</b> Поиск логических закономерностей в данных</a></li>
<li class="chapter" data-level="5.4" data-path="054-Association-Rules-Algos.html"><a href="054-Association-Rules-Algos.html"><i class="fa fa-check"></i><b>5.4</b> Алгоритмы выделения ассоциативных правил</a></li>
<li class="chapter" data-level="5.5" data-path="055-Traminer.html"><a href="055-Traminer.html"><i class="fa fa-check"></i><b>5.5</b> Анализ последовательностей знаков или событий</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="061-Binary-Classifiers.html"><a href="061-Binary-Classifiers.html"><i class="fa fa-check"></i><b>6</b> Бинарные классификаторы с различными разделяющими поверхностями</a><ul>
<li class="chapter" data-level="6.1" data-path="061-Binary-Classifiers.html"><a href="061-Binary-Classifiers.html#sec_6_1"><i class="fa fa-check"></i><b>6.1</b> Дискриминантный анализ</a></li>
<li class="chapter" data-level="6.2" data-path="062-SVM.html"><a href="062-SVM.html"><i class="fa fa-check"></i><b>6.2</b> Метод опорных векторов</a></li>
<li class="chapter" data-level="6.3" data-path="063-Nonlinear-Borders.html"><a href="063-Nonlinear-Borders.html"><i class="fa fa-check"></i><b>6.3</b> Ядерные функции машины опорных векторов</a></li>
<li class="chapter" data-level="6.4" data-path="064-Classification-Trees.html"><a href="064-Classification-Trees.html"><i class="fa fa-check"></i><b>6.4</b> Деревья классификации, случайный лес и логистическая регрессия</a></li>
<li class="chapter" data-level="6.5" data-path="065-Comparing-Classifiers.html"><a href="065-Comparing-Classifiers.html"><i class="fa fa-check"></i><b>6.5</b> Процедуры сравнения эффективности моделей классификации</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="071-Multiclass-Classification.html"><a href="071-Multiclass-Classification.html"><i class="fa fa-check"></i><b>7</b> Модели классификации для нескольких классов</a><ul>
<li class="chapter" data-level="7.1" data-path="071-Multiclass-Classification.html"><a href="071-Multiclass-Classification.html#sec_7_1"><i class="fa fa-check"></i><b>7.1</b> Ирисы Фишера и метод <em>k</em> ближайших соседей</a></li>
<li class="chapter" data-level="7.2" data-path="072-NBC.html"><a href="072-NBC.html"><i class="fa fa-check"></i><b>7.2</b> Наивный байесовский классификатор</a></li>
<li class="chapter" data-level="7.3" data-path="073-In-Discriminant-Space.html"><a href="073-In-Discriminant-Space.html"><i class="fa fa-check"></i><b>7.3</b> Классификация в линейном дискриминантном пространстве</a></li>
<li class="chapter" data-level="7.4" data-path="074-Nonlinear-Classifiers.html"><a href="074-Nonlinear-Classifiers.html"><i class="fa fa-check"></i><b>7.4</b> Нелинейные классификаторы в R</a></li>
<li class="chapter" data-level="7.5" data-path="075-Multinomial-Logit.html"><a href="075-Multinomial-Logit.html"><i class="fa fa-check"></i><b>7.5</b> Модель мультиномиального логита</a></li>
<li class="chapter" data-level="7.6" data-path="076-NN.html"><a href="076-NN.html"><i class="fa fa-check"></i><b>7.6</b> Классификаторы на основе искусственных нейронных сетей</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="081-Logit-for-Count.html"><a href="081-Logit-for-Count.html"><i class="fa fa-check"></i><b>8</b> Моделирование порядковых и счетных переменных</a><ul>
<li class="chapter" data-level="8.1" data-path="081-Logit-for-Count.html"><a href="081-Logit-for-Count.html#sec_8_1"><i class="fa fa-check"></i><b>8.1</b> Модель логита для порядковой переменной</a></li>
<li class="chapter" data-level="8.2" data-path="082-NN-with-Caret.html"><a href="082-NN-with-Caret.html"><i class="fa fa-check"></i><b>8.2</b> Настройка параметров нейронных сетей средствами пакета <code id="sec_8_2">caret</code></a></li>
<li class="chapter" data-level="8.3" data-path="083-Model-Complexes.html"><a href="083-Model-Complexes.html"><i class="fa fa-check"></i><b>8.3</b> Методы комплексации модельных прогнозов</a></li>
<li class="chapter" data-level="8.4" data-path="084-GLM-for-Counts.html"><a href="084-GLM-for-Counts.html"><i class="fa fa-check"></i><b>8.4</b> Обобщенные линейные модели для счетных данных</a></li>
<li class="chapter" data-level="8.5" data-path="085-ZIP-for-Counts.html"><a href="085-ZIP-for-Counts.html"><i class="fa fa-check"></i><b>8.5</b> ZIP- и барьерные модели счетных данных</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="091-Data-Transformation.html"><a href="091-Data-Transformation.html"><i class="fa fa-check"></i><b>9</b> Методы многомерной ординации</a><ul>
<li class="chapter" data-level="9.1" data-path="091-Data-Transformation.html"><a href="091-Data-Transformation.html#sec_9_1"><i class="fa fa-check"></i><b>9.1</b> Преобразование данных и вычисление матрицы расстояний</a></li>
<li class="chapter" data-level="9.2" data-path="092-Distance-ANOVA.html"><a href="092-Distance-ANOVA.html"><i class="fa fa-check"></i><b>9.2</b> Непараметрический дисперсионный анализ матриц дистанций</a></li>
<li class="chapter" data-level="9.3" data-path="093-Comparing-Diagrams.html"><a href="093-Comparing-Diagrams.html"><i class="fa fa-check"></i><b>9.3</b> Методы ординации объектов и переменных: построение и сравнение диаграмм</a></li>
<li class="chapter" data-level="9.4" data-path="094-Ordination-Factors.html"><a href="094-Ordination-Factors.html"><i class="fa fa-check"></i><b>9.4</b> Оценка связи ординации с внешними факторами</a></li>
<li class="chapter" data-level="9.5" data-path="095-NMDS.html"><a href="095-NMDS.html"><i class="fa fa-check"></i><b>9.5</b> Неметрическое многомерное шкалирование и построение распределения чувствительности видов</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="101-Partitioning-Algos.html"><a href="101-Partitioning-Algos.html"><i class="fa fa-check"></i><b>10</b> Кластерный анализ</a><ul>
<li class="chapter" data-level="10.1" data-path="101-Partitioning-Algos.html"><a href="101-Partitioning-Algos.html#sec_10_1"><i class="fa fa-check"></i><b>10.1</b> Алгоритмы кластеризации, основанные на разделении</a></li>
<li class="chapter" data-level="10.2" data-path="102-H-Clustering.html"><a href="102-H-Clustering.html"><i class="fa fa-check"></i><b>10.2</b> Иерархическая кластеризация</a></li>
<li class="chapter" data-level="10.3" data-path="103-Clustering-Quality.html"><a href="103-Clustering-Quality.html"><i class="fa fa-check"></i><b>10.3</b> Оценка качества кластеризации</a></li>
<li class="chapter" data-level="10.4" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html"><i class="fa fa-check"></i><b>10.4</b> Другие алгоритмы кластеризации</a><ul>
<li class="chapter" data-level="10.4.1" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html#sec_10_4_1"><i class="fa fa-check"></i><b>10.4.1</b> Иерархическая кластеризация на главные компоненты</a></li>
<li class="chapter" data-level="10.4.2" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html#sec_10_4_2"><i class="fa fa-check"></i><b>10.4.2</b> Метод нечетких <em>k</em> средних (fuzzy analysis clustering)</a></li>
<li class="chapter" data-level="10.4.3" data-path="104-Other-Clustering-Methods.html"><a href="104-Other-Clustering-Methods.html#sec_10_4_3"><i class="fa fa-check"></i><b>10.4.3</b> Статистическая модель кластеризации</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="105-Cohonen-Maps.html"><a href="105-Cohonen-Maps.html"><i class="fa fa-check"></i><b>10.5</b> Самоорганизующиеся карты Кохонена</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="111-Rattle-Intro.html"><a href="111-Rattle-Intro.html"><i class="fa fa-check"></i><b>11</b> <code>rattle</code>: графический интерфейс R для реализации алгоритмов Data Mining</a><ul>
<li class="chapter" data-level="11.1" data-path="111-Rattle-Intro.html"><a href="111-Rattle-Intro.html#----rattle"><i class="fa fa-check"></i><b>11.1</b> Начало работы с пакетом <code id="sec_11_1">rattle</code></a></li>
<li class="chapter" data-level="11.2" data-path="112-Descriptive-Stats.html"><a href="112-Descriptive-Stats.html"><i class="fa fa-check"></i><b>11.2</b> Описательная статистика и визуализация данных</a></li>
<li class="chapter" data-level="11.3" data-path="113-Model-Building.html"><a href="113-Model-Building.html"><i class="fa fa-check"></i><b>11.3</b> Построение и тестирование моделей классификации</a></li>
<li class="chapter" data-level="11.4" data-path="114-Descriptive-Models.html"><a href="114-Descriptive-Models.html"><i class="fa fa-check"></i><b>11.4</b> Дескриптивные модели (обучение без учителя)</a><ul>
<li class="chapter" data-level="11.4.1" data-path="114-Descriptive-Models.html"><a href="114-Descriptive-Models.html#sec_11_4_1"><i class="fa fa-check"></i><b>11.4.1</b> Кластерный анализ</a></li>
<li class="chapter" data-level="11.4.2" data-path="114-Descriptive-Models.html"><a href="114-Descriptive-Models.html#sec_11_4_2"><i class="fa fa-check"></i><b>11.4.2</b> Ассоциативные правила</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="120-References.html"><a href="120-References.html"><i class="fa fa-check"></i><b>12</b> Список рекомендуемой литературы</a></li>
<li class="chapter" data-level="" data-path="130-Appendix.html"><a href="130-Appendix.html"><i class="fa fa-check"></i>Приложение: cправочная карта по Data Mining с использованием R</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Классификация, регрессия и другие алгоритмы Data Mining с использованием R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec_4_3" class="section level2">
<h2><span class="header-section-number">4.3</span> Построение деревьев регрессии</h2>
<p>Деревья решений (Breiman at al., 1984; Quinlan, 1986) осуществляют разбиение пространства объектов в соответствии с некоторым набором <em>правил разбиения</em> (splitting rule). Эти правила являются логическими утверждениями в отношении той или иной переменной и могут быть истинными или ложными. Ключевыми здесь являются три обстоятельства: а) правила позволяют реализовать последовательную дихотомическую сегментацию данных, б) два объекта считаются похожими, если они оказываются в одном и том же сегменте разбиения, в) на каждом шаге разбиения увеличивается количество информации относительно исследуемой переменной (отклика).</p>
<p>Деревья классификации и регрессии являются одним из наиболее популярных методов решения многих практических задач, что обусловлено следующими причинами:</p>
<ol style="list-style-type: decimal">
<li>Деревья решений позволяют получать очень <em>легко интерпретируемые</em> модели, представляющие собой набор правил вида “если…, то…”. Интерпретация облегчается, в том числе, за счет возможности представить эти правила в виде наглядной <em>древовидной структуры</em>.</li>
<li>В силу своего устройства деревья решений позволяют работать с <em>переменными любого типа</em> без необходимости какой-либо предварительной подготовки этих переменных для ввода в модель (например, логарифмирование, преобразование категориальных переменных в индикаторные, и т.п.).</li>
<li>Исследователю <em>нет необходимости в явном виде задавать форму взаимосвязи</em> между откликом и предикторами, как это, например, происходит в случае с обычными регрессионными моделями. Это оказывается особенно полезным при работе с данными большого объема, о свойствах которых мало что известно.</li>
<li>Деревья решений, по сути, <em>автоматически выполняют отбор информативных предикторов</em> и учитывают возможные взаимодействия между ними. Это, в частности, делает деревья решений полезным инструментом разведочного анализа данных.</li>
<li>Деревья решений <em>можно эффективно применять к данным с пропущенными значениями</em>, что очень полезно при решении практических задач, где наличие пропущенных значений – это, скорее, правило, чем исключение.</li>
<li>Деревья решений одинаково <em>хорошо применимы как к количественным, так и к качественным зависимым переменным</em>.</li>
</ol>
<p>К недостаткам этого класса моделей иногда относят <em>нестабильность</em> и <em>невысокую точность предсказаний</em>, что, как будет показано ниже, не всегда подтверждается. По своей сути, деревья используют “наивный подход” (naive approach) в том смысле, что они исходят из предположения о взаимной независимости признаков. Поэтому модели регрессионных деревьев статистически наиболее работоспособны, когда комплекс анализируемых переменных является не слишком мультиколлинеарным или имеется регулярная внутренняя множественная альтернатива в исходной комбинации признаков.</p>
<p>Алгоритм CART (Classification and Regression Tree) рекурсивно делит исходный набор данных на подмножества, которые становятся все более и более гомогенными относительно определенных признаков, в результате чего формируется древовидная иерархическая структура. Деление осуществляется на основе традиционных логических правил в виде <code>ЕСЛИ (А) ТО (В)</code>, где <code>А</code> - некоторое логическое условие, а <code>В</code> - процедура деления подмножества на две части, для одной из которых условие <code>А</code> истинно, а для другой - ложно. Примеры условий: <code>Xi==F, Хi &lt;= V; Хi &gt;= V</code> и др., где <code>Хi</code> – один из предикторов исходной таблицы, <code>F</code> - выбранное значение категориальной переменной, <code>V</code> - специально подобранное опорное значение (порог).</p>
<p>На первой итерации корневой узел дерева связывается с наиболее оптимальным условным суждением, и все множество объектов делится на две группы. От каждого последующего узла-родителя к узлам-потомкам также может отходить по две ветви, в свою очередь связанные c граничными значениями других наиболее подходящих переменных и определяющие правила дальнейшего разделения (splitting criterion). Конечными узлами дерева являются “листья”, соответствующие найденным решениям и объединяющие все разделенные на группы объекты обучающей выборки. Общее правило выбора опорного значения для каждого узла построенного дерева можно сформулировать следующим образом: “выбранный признак должен разбить множество <span class="math inline">\(\mathbf{X}^*\)</span> так, чтобы получаемые в итоге подмножества <span class="math inline">\(\mathbf{X}_k^*, k = 1, 2, \dots, p\)</span>, состояли из объектов, принадлежащих к одному классу, или были максимально приближены к этому”.</p>
<p>Описанный процесс относится к так называемым “жадным” алгоритмам, стремящимся, не считаясь ни с чем, построить максимально “кустистое” дерево (также “глубокое дерево”, deep tree). Естественно, чем обширнее и кустистее дерево, тем лучше будут результаты его тестирования на обучающей выборке, но не столь успешными – на проверочной выборке. Поэтому построенная модель должна быть еще и оптимальной по размерам, т.е. содержать информацию, улучшающую качество распознавания, и игнорировать ту информацию, которая его не улучшает. Для этого обычно проводят “обрезание” дерева (tree pruning) – отсечение ветвей там, где эта процедура не приводит к серьезному возрастанию ошибки.</p>
<p>Невозможно подобрать объективный внутренний критерий, приводящий к хорошему компромиссу между безошибочностью и компактностью, поэтому стандартный механизм оптимизации деревьев основан на перекрестной проверке (Loh, Shih, 1997). Для этого обучающая выборка разделяется, например, на 10 равных частей: 9 частей используется для построения дерева, а оставшаяся часть играет роль проверочной совокупности. После многократного повторения этой процедуры из некоторого набора деревьев-претендентов, у которых имеется практически допустимый разброс критериев качества модели, выбирается дерево, показавшее наилучший результат при перекрестной проверке.</p>
<div id="sec_4_3_1" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Построение деревьев на основе рекурсивного разбиения</h3>
<p>В общем случае может быть использовано несколько алгоритмов построения деревьев на основе различных схем и критериев оптимизации. Функция <code>rpart()</code> из одноименного пакета выполняет рекурсивный выбор для каждого следующего узла таких разделяющих значений, которые приводят к минимальной сумме квадратов внутригрупповых отклонений Dt для всех t узлов дерева. Для оценки качества построенного дерева <span class="math inline">\(\mathbf{T}\)</span> в ходе его оптимизации используется следующая совокупность критериев:</p>
<ul>
<li>штраф за сложность модели (cost complexity), включающий штрафной множитель за каждую неотсечённую ветвь <span class="math inline">\(СС(\mathbf{T} = \sum_t D_t + \lambda t)\)</span>;</li>
<li>девианс <span class="math inline">\(D_0\)</span> для нулевого дерева (т.е. оценка изменчивости в исходных данных);</li>
<li>относительный параметр стоимости сложности <span class="math inline">\(Cp = \lambda / D_0\)</span>;</li>
<li>относительная ошибка обучения для дерева из <span class="math inline">\(t\)</span> узлов <span class="math inline">\(REL_{er} = \sum_t D_t /D_0\)</span>;</li>
<li>ошибка перекрестной проверки (<span class="math inline">\(CV_{er}\)</span>) с разбиением на 10 блоков, также отнесенная к девиансу нуль-дерева <span class="math inline">\(D_0\)</span>; <span class="math inline">\(CV_{er}\)</span>, как правило, больше, чем <span class="math inline">\(REL_{er}\)</span>;</li>
<li>стандартное отклонение (<span class="math inline">\(SE\)</span>) ошибки перекрестной проверки.</li>
</ul>
<p>Лучшим считается дерево, состоящее из такого количества ветвей <span class="math inline">\(t\)</span>, для которого сумма (<span class="math inline">\(CV_{er} + SE\)</span>) является минимальной.</p>
<p>В качестве примера рассмотрим построение дерева CART, прогнозирующего обилие водорослей группы <code>a1</code> в зависимости от гидрохимических показателей воды и условий отбора проб в различных водотоках (см. разделы <a href="034-Handling-Missing-Values.html#sec_3_4">3.4</a> и <a href="041-Regression-Models.html#sec_4_1">4.1</a>-<a href="042-Regularization.html#sec_4_2">4.2</a>). Используем сначала пакет <code>rpart</code>, для работы с которым обычно применяется двухшаговая процедура: функция <code>rpart()</code> устанавливает связи между зависимой и независимыми переменными и формирует бинарное дерево, а функция <code>prun()</code> выполняет обрезание лишних ветвей.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="dt">file =</span> <span class="st">&quot;algae.RData&quot;</span>) <span class="co"># Загрузка таблицы algae - раздел 4.1</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rpart)
(rt.a1 &lt;-<span class="st"> </span><span class="kw">rpart</span>(a1 ~<span class="st"> </span>., <span class="dt">data =</span> algae[, <span class="dv">1</span>:<span class="dv">12</span>]))</code></pre></div>
<pre><code>## n= 200 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 200 90694.880 16.923500  
##    2) PO4&gt;=43.818 148 31359.210  8.918919  
##      4) Cl&gt;=7.8065 141 21678.580  7.439716  
##        8) oPO4&gt;=51.118 85  3455.770  3.801176 *
##        9) oPO4&lt; 51.118 56 15389.430 12.962500  
##         18) mnO2&gt;=10.05 24  1248.673  6.716667 *
##         19) mnO2&lt; 10.05 32 12502.320 17.646870  
##           38) NO3&gt;=3.1875 9   257.080  7.866667 *
##           39) NO3&lt; 3.1875 23 11047.500 21.473910  
##             78) mnO2&lt; 8 13  2919.549 13.807690 *
##             79) mnO2&gt;=8 10  6370.704 31.440000 *
##      5) Cl&lt; 7.8065 7  3157.769 38.714290 *
##    3) PO4&lt; 43.818 52 22863.170 39.705770  
##      6) mxPH&lt; 7.87 28 11636.710 32.875000  
##       12) oPO4&gt;=3.1665 14  1408.304 23.978570 *
##       13) oPO4&lt; 3.1665 14  8012.309 41.771430 *
##      7) mxPH&gt;=7.87 24  8395.785 47.675000  
##       14) PO4&gt;=15.177 12  3047.517 38.183330 *
##       15) PO4&lt; 15.177 12  3186.067 57.166670 *</code></pre>
<p>Приведенной командой мы построили полное дерево без обрезания ветвей, состоящее из 9 узлов и 10 листьев, обозначенных в приведенном протоколе разбиения символом <code>*</code>. В каждой строке представлены по порядку: условие разделения, число наблюдений, соответствующих этому условию, девианс (в данном случае - это эквивалент суммы квадратов отклонений от группового среднего) и среднее значение отклика для выделенной ветви. Например, перед первой итерацией общее множество из 200 наблюдений имеет среднее значение <code>m = 16.92</code> при девиансе <code>D = 90694</code>. При <code>PO4&gt;=43.8</code> это множество делится на две части: <code>2)</code> 148 наблюдений (<code>m =8.92</code>, <code>D = 31359</code>) и <code>3)</code> 52 наблюдения с высоким уровнем обилия водорослей (<code>m =39.7</code>, <code>D = 22863</code>). Дальнейшие разбиения каждой из этих двух частей аналогичны.</p>
<p>Разумеется, лучший вариант – представить дерево графически. Популярны три варианта визуализации с использованием различных функций: <code>plot()</code>, <code>prettyTree()</code> из пакета <code>DMwR</code> и <code>prp()</code> из чрезвычайно продвинутого пакета <code>rpart.plot</code> (рис. <a href="043-Decision-Trees.html#fig:fig-4-7">4.7</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prettyTree</span>(rt.a1)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-4-7"></span>
<img src="043-Decision-Trees_files/figure-html/fig-4-7-1.png" alt="Дерево `rpart` без обрезания ветвей" width="768" />
<p class="caption">
Рисунок 4.7: Дерево <code>rpart</code> без обрезания ветвей
</p>
</div>
<p>Полезно также проследить изменение перечисленных выше статистических критериев по мере выращивания дерева:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">printcp</span>(rt.a1)</code></pre></div>
<pre><code>## 
## Regression tree:
## rpart(formula = a1 ~ ., data = algae[, 1:12])
## 
## Variables actually used in tree construction:
## [1] Cl   mnO2 mxPH NO3  oPO4 PO4 
## 
## Root node error: 90695/200 = 453.47
## 
## n= 200 
## 
##         CP nsplit rel error  xerror    xstd
## 1 0.402145      0   1.00000 1.00890 0.13049
## 2 0.071921      1   0.59785 0.71564 0.11458
## 3 0.031241      2   0.52593 0.68023 0.11225
## 4 0.031211      3   0.49469 0.71624 0.11875
## 5 0.024435      4   0.46348 0.70740 0.11725
## 6 0.023840      5   0.43905 0.69403 0.11063
## 7 0.018065      6   0.41521 0.70573 0.10933
## 8 0.016291      7   0.39714 0.74901 0.11225
## 9 0.010000      9   0.36456 0.78376 0.11720</code></pre>
<p>Функция <code>rpart()</code> и другие функции из пакета <code>rpart</code> имеют собственные возможности выполнить перекрестную проверку и оценить ее ошибку при различных значениях штрафа за сложность модели <code>cp</code> (рис. <a href="043-Decision-Trees.html#fig:fig-4-8">4.8</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">505</span>) <span class="co"># для воспроизводимости примера</span>
<span class="co">#   Снижаем порог штрафа за сложность с шагом .005</span>
rtp.a1 &lt;-<span class="st"> </span><span class="kw">rpart</span>(a1 ~<span class="st"> </span>., <span class="dt">data =</span> algae[, <span class="dv">1</span>:<span class="dv">12</span>], 
                <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">cp =</span> .<span class="dv">005</span>)) 
<span class="co">#  График зависимости относительных ошибок от числа узлов</span>
<span class="kw">plotcp</span>(rtp.a1) 
<span class="kw">with</span>(rtp.a1, {<span class="kw">lines</span>(cptable[, <span class="dv">2</span>] +<span class="st"> </span><span class="dv">1</span>, cptable[, <span class="dv">3</span>], <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)
    <span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;Ошибка обучения&quot;</span>,
                         <span class="st">&quot;Ошибка крос-проверки (CV)&quot;</span>, <span class="st">&quot;min(CV ошибка)+SE&quot;</span>),
           <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>), <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;black&quot;</span>), <span class="dt">bty =</span> <span class="st">&quot;n&quot;</span>) })</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-4-8"></span>
<img src="043-Decision-Trees_files/figure-html/fig-4-8-1.png" alt="Зависимость относительной ошибки перекрестной проверки от штрафа за сложность модели `cp`" width="768" />
<p class="caption">
Рисунок 4.8: Зависимость относительной ошибки перекрестной проверки от штрафа за сложность модели <code>cp</code>
</p>
</div>
<p>На рис. <a href="043-Decision-Trees.html#fig:fig-4-8">4.8</a> видно, что минимум относительной ошибки при перекрестной проверке приходится на значение <code>cp = 0.024</code>.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> Выполним обрезку дерева при этом значении (рис. <a href="043-Decision-Trees.html#fig:fig-4-9">4.9</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rtp.a1 &lt;-<span class="st"> </span><span class="kw">prune</span>(rtp.a1, <span class="dt">cp =</span> <span class="fl">0.024</span>)
<span class="kw">prettyTree</span>(rtp.a1) </code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-4-9"></span>
<img src="043-Decision-Trees_files/figure-html/fig-4-9-1.png" alt="Дерево `rpart` c обрезанием ветвей при `cp = 0.024`" width="768" />
<p class="caption">
Рисунок 4.9: Дерево <code>rpart</code> c обрезанием ветвей при <code>cp = 0.024</code>
</p>
</div>
<p>Выполним теперь дополнительную оптимизацию параметра <code>ср</code> с использованием функции <code>train()</code> из пакета <code>caret</code> (см раздел <a href="035-The-train-Functions.html#sec_3_5">3.5</a>). Будем тестировать деревья регрессии при 30 значениях критерия <code>ср</code>, для каждого из которых выполним 10-кратную перекрестную проверку с 3 повторностями (рис. <a href="043-Decision-Trees.html#fig:fig-4-10">4.10</a>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
cvCtrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, <span class="dt">repeats =</span> <span class="dv">3</span>)
<span class="kw">set.seed</span>(<span class="dv">202</span>) <span class="co"># для воспроизводимости примера</span>
rt.a1.train &lt;-<span class="st"> </span><span class="kw">train</span>(a1 ~<span class="st"> </span>., <span class="dt">data =</span> algae[, <span class="dv">1</span>:<span class="dv">12</span>], 
                     <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>, <span class="dt">tuneLength =</span> <span class="dv">30</span>, <span class="dt">trControl =</span> cvCtrl)
<span class="kw">plot</span>(rt.a1.train)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-4-10"></span>
<img src="043-Decision-Trees_files/figure-html/fig-4-10-1.png" alt="Оценка параметра `cp` с использованием функции `train()`" width="768" />
<p class="caption">
Рисунок 4.10: Оценка параметра <code>cp</code> с использованием функции <code>train()</code>
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rtt.a1 &lt;-<span class="st"> </span>rt.a1.train$finalModel
<span class="kw">prettyTree</span>(rtt.a1)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-4-11"></span>
<img src="043-Decision-Trees_files/figure-html/fig-4-11-1.png" alt="Дерево `rpart` c обрезанием ветвей при `cp = 0.0277`" width="768" />
<p class="caption">
Рисунок 4.11: Дерево <code>rpart</code> c обрезанием ветвей при <code>cp = 0.0277</code>
</p>
</div>
<p>При <code>cp = 0.0277</code> было получено существенно урезанное дерево, которое, правда, значительно потеряло в своей объясняющей ценности.</p>
</div>
<div id="sec_4_3_2" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Построение деревьев с использованием алгортма условного вывода</h3>
<p>Обратимся теперь к принципиально другим методам рекурсивного разделения при построении деревьев, представленным в пакете <code>party</code>. Стандартный механизм проверки статистического гипотез, который предотвращает переусложнение модели, реализован в функции <code>ctree()</code>, использующей метод построения деревьев на основе “условного вывода” (conditional inference). Алгоритм принимает во внимание характер распределения отдельных переменных и осуществляет на каждом шаге рекурсивного разделения данных несмещенный выбор влияющих ковариат, используя формальный тест на основе статистического критерия <span class="math inline">\(c(\boldsymbol{t}_j, \mu_j, \Sigma_j), j = 1, \dots, m\)</span>, где <span class="math inline">\(\mu, \Sigma\)</span> - соответственно среднее и ковариация (Hothorn et al., 2006). Оценка статистической значимости <span class="math inline">\(с\)</span>-критерия выполняется на основе перестановочного теста, в результате чего формируются компактные деревья, не требующие процедуры обрезания.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(party)  <span class="co"># Построение дерева методом &quot;условного вывода&quot;</span>
(ctree.a1 &lt;-<span class="st"> </span><span class="kw">ctree</span>(a1 ~<span class="st"> </span>., <span class="dt">data =</span> algae[, <span class="dv">1</span>:<span class="dv">12</span>]))</code></pre></div>
<pre><code>## 
##   Conditional inference tree with 4 terminal nodes
## 
## Response:  a1 
## Inputs:  season, size, speed, mxPH, mnO2, Cl, NO3, NH4, oPO4, PO4, Chla 
## Number of observations:  200 
## 
## 1) PO4 &lt;= 43.5; criterion = 1, statistic = 47.106
##   2)*  weights = 52 
## 1) PO4 &gt; 43.5
##   3) oPO4 &lt;= 51.111; criterion = 0.985, statistic = 10.234
##     4) size == {small}; criterion = 0.993, statistic = 14.65
##       5)*  weights = 14 
##     4) size == {large, medium}
##       6)*  weights = 49 
##   3) oPO4 &gt; 51.111
##     7)*  weights = 85</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(ctree.a1)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-4-12"></span>
<img src="043-Decision-Trees_files/figure-html/fig-4-12-1.png" alt="Дерево `cpart` без оптимизации параметра `mincriterion`" width="768" />
<p class="caption">
Рисунок 4.12: Дерево <code>cpart</code> без оптимизации параметра <code>mincriterion</code>
</p>
</div>
<p>Оптимизацию параметра <code>mincriterion</code> выполним с использованием функции <code>train()</code> при тех же условиях перекрестной проверки:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ctree.a1.train &lt;-<span class="st"> </span><span class="kw">train</span>(a1 ~<span class="st"> </span>., <span class="dt">data =</span> algae[, <span class="dv">1</span>:<span class="dv">12</span>], 
                        <span class="dt">method =</span> <span class="st">&quot;ctree&quot;</span>, <span class="dt">tuneLength =</span> <span class="dv">10</span>, <span class="dt">trControl =</span> cvCtrl)
ctreet.a1 &lt;-<span class="st"> </span>ctree.a1.train$finalModel
<span class="kw">plot</span>(ctreet.a1)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig-4-13"></span>
<img src="043-Decision-Trees_files/figure-html/fig-4-13-1.png" alt="Дерево `cpart` после оптимизации параметра `mincriterion`" width="768" />
<p class="caption">
Рисунок 4.13: Дерево <code>cpart</code> после оптимизации параметра <code>mincriterion</code>
</p>
</div>
<p>Здесь имел место обратный процесс: число узлов дерева было предложено увеличить с 7 до 11. Обратим также внимание на то, что в дереве появились категориальные переменные (размер и скорость течения реки), которые были проигнорированы <code>rpart</code>-деревьями.</p>
</div>
<div id="sec_4_3_3" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Тестирование моделей с использованием дополнительного набора данных</h3>
<p>Используем для прогноза набор данных (Torgo, 2011) из 140 наблюдений, который мы уже применяли в предыдущем разделе. Данные с восстановленными пропущенными значениями мы сохранили ранее в файле <code>algae_test.RData</code> (см раздел <a href="041-Regression-Models.html#sec_4_1">4.1</a>).</p>
<p>Оценим точность каждой модели на этом наборе данных по трем показателям: среднему абсолютному отклонению (<code>MAE</code>), корню из среднеквадратичного отклонения (<code>RSME</code>) и квадрату коэффициента детерминации <code>Rsq = 1 - NSME</code>, где <code>NSME</code> - относительная ошибка, равная отношению средних квадратов отклонений от модельных значений и от общего среднего:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="dt">file =</span> <span class="st">&quot;algae_test.RData&quot;</span>) <span class="co"># Загрузка таблиц Eval, Sols</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Функция, выводящая вектор критериев</span>
ModCrit &lt;-<span class="st"> </span>function(pred, fact) {
    mae &lt;-<span class="st"> </span><span class="kw">mean</span>(<span class="kw">abs</span>(pred -<span class="st"> </span>fact))
    rmse &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((pred -<span class="st"> </span>fact)^<span class="dv">2</span>))
    Rsq &lt;-<span class="st"> </span><span class="dv">1</span> -<span class="st"> </span><span class="kw">sum</span>((fact -<span class="st"> </span>pred)^<span class="dv">2</span>)/<span class="kw">sum</span>((<span class="kw">mean</span>(fact) -<span class="st"> </span>fact)^<span class="dv">2</span>)
    <span class="kw">c</span>(<span class="dt">MAE =</span> mae, <span class="dt">RSME =</span> rmse, <span class="dt">Rsq =</span> Rsq) 
} 
Result &lt;-<span class="st"> </span><span class="kw">rbind</span>(
    <span class="dt">rpart_prune =</span> <span class="kw">ModCrit</span>(<span class="kw">predict</span>(rtp.a1, Eval), Sols[, <span class="dv">1</span>]),
    <span class="dt">rpart_train =</span> <span class="kw">ModCrit</span>(<span class="kw">predict</span>(rt.a1.train, Eval), Sols[, <span class="dv">1</span>]),
    <span class="dt">ctree_party =</span> <span class="kw">ModCrit</span>(<span class="kw">predict</span>(ctree.a1, Eval), Sols[, <span class="dv">1</span>]),
    <span class="dt">ctree_train =</span> <span class="kw">ModCrit</span>(<span class="kw">predict</span>(ctree.a1.train, Eval), Sols[, <span class="dv">1</span>])
)
Result</code></pre></div>
<pre><code>##                  MAE     RSME       Rsq
## rpart_prune 10.99017 15.72931 0.4105428
## rpart_train 11.13198 16.04396 0.3867241
## ctree_party 11.31029 16.52534 0.3493711
## ctree_train 11.47503 16.63481 0.3407221</code></pre>
<p>Можно с разумной осторожностью сделать вывод о том, что деревья, построенные <code>rpart()</code>, немного точнее, чем деревья условного вывода <code>ctree()</code>. При этом все деревья решений оказались существенно эффективней для прогнозирования свежих данных, чем все ранее построенные модели.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="6">
<li id="fn6"><p>Обратите внимание: в связи с небольшим объемом рассматриваемого набора данных, оптимальные значения <code>cp</code>, найденные по результатам перекрестной проверки, будут существенно варьировать от раза к разу. Так, используя другое значение зерна в команде <code>set.seed()</code> вы, скорее всего, получите другое “оптимальное” значение <code>сp</code>.<a href="043-Decision-Trees.html#fnref6">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="042-Regularization.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="044-Ensembles.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["_main.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
